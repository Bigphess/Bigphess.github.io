<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CS231Nassignment3RNN]]></title>
    <url>%2F2019%2F05%2F10%2FCS231Nassignment3RNN%2F</url>
    <content type="text"><![CDATA[assignment3 targetIn this exercise you will implement a vanilla recurrent neural networks and use them it to train a model that can generate novel captions for images. Microsoft COCO 在这次的作业里用的是Microsoft的coco dataset，已经是一个很常用的给文字配上说明文（captioning）的dataset了，有80000个训练和40000个val，每个图片包含一个五个字的注释 在这个作业里已经preprocess了data，每个图片已经从VGG-16（ImageNet pretrain）layer 7提取了feature，存在了train2014_vgg16_fc7.h5和val2014_vgg16_fc7.h5 为了减少处理的时间和内存，feature的特征从4096降到了512 真实的图片太大了，所以把图片的url存在了txt里面，这样在vis的时候可以直接下载这些图片（必须联网） 直接处理string的效率太低了，所以在caption的一个encoded版本上面进行处理，这样可以把string表示成一串int。在dataset里面也有这两个之间转换的信息 -&gt; 在转换的时候也加了更多的tokens 事先看了一下图片和对应的语句]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习OpenCV第19章，投影和3D视觉]]></title>
    <url>%2F2019%2F05%2F08%2FOpenCV19%2F</url>
    <content type="text"><![CDATA[Chapter 19首先会讨论从3D得到2D信息，然后再讨论从2D推断3D信息如果没有多张图片是很难得到靠谱的3D信息的： 通过stereo vision 通过motion Projections 得到了物体在三维里面的位置之后，因为我们在18章已经calibration了相机，所以可以得到这个点在图片里面的位置 提供了一个函数projectPoints来投影一系列的点，针对刚体 a list of loca‐ tions in the object’s own body-centered coordinate system 加上了平移和旋转，相机intrinsic和distortion 最后输出在画面上的店 Affine and Perspective Transformationsaffine是针对一个list或者一整张图片进行的，可以把一个point从图片的一个location移动到另一个location，perspective trans更多的是针对一个矩形图片 -&gt; related to prspective transformation 总结不同的函数 Bird’s-eye-view trans（p699） 在robtic巡航的时间，经常把排到的画面变成从上往下看的bird-view 需要相机的intrinsic和distortion （把棋盘放在地上进行calibration） 步骤 首先读取相机的参数和distortion model 找到地面上已知的店（比如chessboard），找到至少四个点 cv::getPerspectiveTransform()计算地面上已知点的homography H cv::warpPerspective()形成bird-eye-view three-dim pose estimation物体的三维pose可以从 一个相机：必须先考虑情况惹 多个相机捕捉：从多个不同图片来推断，这样即使是不知道的东西都可以操作 single camera 如果我们知道一个object，我们需要知道这个东西在他自己坐标系里面关键点的坐标 如果现在给了一个新的view point，可以根据关键点的位置来推断 cv::solvePnP() 用来计算一个know object的位置 从图片里面提取特征点，然后计算不同点的位置，这个问题的解是应该是唯一的 PNP问题不是每次都有唯一的解 如果没有足够的关键点，为了保险起见应该有足够的店 或者当物体离得特别远（这时候光线接近于平行了，就不好判断了） 总的来说，单目视觉和人自己的眼睛（单只）看东西的感觉差不多，不能获得精确的大小，还会产生一些错觉（比如把大楼的窗户设计的小来显得楼更高） Stereo Imaging在电脑中，通过计算在两张图里面都出现的点的位置来计算，这样就可以计算这个点的三维位置。虽然这样计算的计算量很大，但是可以通过一些方法来压缩搜寻的范围，从而得到相应的结果。主要分为4步： 在数学上remove掉相机lens的辐射和平移distortion -&gt; undistortion 调整相机之间的角度和距离 -&gt; rectification。这一步输出之后的两张图片应该是row-aligned的（frontal parallel） 找到左右两张图相同的feature -&gt; correspondence。这一步的输出是一个disparity map，输出的是两个图中相同特征点的x坐标方向上面的disparity 最后可以把disparity转换成triangulation，这一步叫做reprojection，这样输出的就是depth map了 triangulation（找到disparity和depth的关系） 整体概念如上图所示，在这张图里我们假设系统已经完全undistort，aligned（两张图片的行和行对上了）了，两个相机的平面完全相同，焦距也相同，并且两个相机的cx已经被calibrated好了（相同） 这时，这个物体点P的depth和disparity是成正比的，求出来的disparity是：xl - xr（xl和xr都是根据各自的相机中心的坐标）: T - (xl - xr)/Z - f = T/Z 这个关系虽然是正比但是不是线性的 当disparity接近0的时候，小的disparity的差异会引发非常大的depth的差异 当disparity非常大的时候，disparity的改变不会对depth引起太多的影响 最终，stereo的系统只在比较接近相机的部分有比较高的depth resolution（如下图所示） 上面的例子是二维转一维的，实际在OpenCV的系统里面是三维转二维的 在实际的应用里面相机不是那么理想的共线的，要尽量确保共线，才不会引起太多的distortion。最终的目的是通过math的计算让他共线，而不是在物理上共线 除此之外，还需要保证相机的拍摄是同步的，避免在拍摄的时候会有东西移动 Epipolar Geometry（简化双目模型）stereo image system的模型： 组合了两个pinhole model 加入了新的points epipoles 主要点： 对于每个相机都会有一个投影的中心O，并且有两个和这个相关的投影平面 在现实中的物体P会在两个投影平面上分别有投影pl和pr el或者er，定义是另一个相机的中心在这个投影平面上的投影，el和pl可以形成一条epipolar line 得到要义 每个三维的点，都会在每个相机上面得到一个epipolar的，这个点和pr/pl的交点就是epipolar line 一个图片里面的feature，在另一个图片里面必须在相对应的epipolar line上面（epipolar constraint） 上面那个定义意味着：可以把在图片上寻找特征从二维（图片）降低到一维（线） 并且图片的order会保存，比如一条线在两张图里面都是水平的 The Essential and Fundamental Matrices E Mat：包括了两个相机的translation和rotation F Mat：包括了E的信息，以及相机的intrinsic（在pixel的层面上关联两个相机） 二者的区别 E只知道两个相机的关系，不知道任何关于图片的信息，只在物体的层面上关联了两个相机 F关联了两个照片在各自图片坐标系里面的关系 E math + F mat（p713，还没有怎么看） 在左边的相机里，观察到的点是pl，在右边的相机观察到的点是pr pr = R（pl - T） cv::findFundamentalMatComputing Epipolar Lines(计算上面模型里面的那条线) 有了F Mat之后希望可以计算上面的epipolar line。每一个图片里面的line都会在另一张图片里有一个对应的line line用一个三个点的vector来表示 cv::computeCorrespondEpilines Stereo Calibration上面已经说了很多的理论知识了所以我们现在就开始calibration吧！ Stereo calibration是在空间上面计算两个相机的位置。相反，后面要说的rectification才是来保证两张图片行是共线的 Stereo calibration主要依靠的是找两个相机之间的T和R矩阵，这两个都可以用cv::stereoCalibrate()来计算 和单目相机的calibration有些相似，但是单目的相机要寻找一系列相机和chessboard之间的R和T 双目的calibration在寻找唯一一个能让左右相机匹配上的R和T 可以得到三个等式求解 因为图片的noise或者rounding error，每组得到的结果可能会有轻微的不同，最后会取中位数 calibration会把右边的相机放在和左边的相机相同的plane上面，这样这两个相机得到的图片就是parallel的，但是这时候还不是row-aligned的！！！ 可以直接通过用这一个函数计算相机的intrinsic，extrinsic和Stereo的参数，不用先进行calibration Stereo Rectification 如果两个图片aligned了，那么根据上面计算出来的disparity就可以很轻易的得到depth map了。但是在实际中只有相机没有这么容易做到 目标：我们需要reproject两个image plane，让他们在完全相同的plane里面，可以得到完美的aligned 我们希望在rectification之后图片的row aliged，这样stereo correspondence（在两个图片里找相同的点）就会变得更可信而且容易计算 在另一张照片里只找match一个点的row 这样的结果会有无限个待选 我们再人为的加上限制 结果会有八个term，四个给左边的相机，四个给右边的相机（两种计算这些参数的算法） 每个相机都会有distCoffs和旋转矩阵R，修正和未修正的相机矩阵（4个） 用上面这些东西，得到map来确定原图要怎么修改cv::initUndistortRectifyMap() Hartley’s algorithm + Bouguet’s algorithm（p730）Rectification mapStereo Correspondence 在两个图片里面match三维的点，只能在两张图片交叠的地方找到 两种不同的算法 block matching：快，效率高，基于“sum of absolute difference” (SAD） 只会找到高度符合的点（highly textured）-&gt; 户外 semi-global block matching (SGBM) ：精确度更高 matching is done at subpixel level using the Birchfield-Tomasi metric enforce a global smoothness constraint on the computed depth information that it approximates by considering many one-dimensional smoothness constraints through the region of interest Block matching三个步骤 prefiltering，normal图片的亮度，增强纹理 用SAD的窗口，搜索水平的epipolar line 在rectificatin之后，每行都是一个epipolar line，所以左边的图片肯定在右边的同一行里面有一个对应的部分 disparity会在一定的pixel范围里进行搜索，不同范围里的disparity代表的是不同的depth。但是超过了最大值的话就找不到depth了 -&gt; Each disparity limit defines a plane at a fixed depth from the cameras Postfiltering，减少比较差的结果 Semi-global block matchingcode example （p752）Structure from Motion 从移动中得到构造信息。但是在静止的情况下，一个相机移动得到的信息和两个相机得到的信息没有本质的区别 但是如果特别大的时候，就需要通过计算frame之间的关系得到最后的结果（SLAM？） 在附录 FitLine（直线拟合） 在三维的分析之中比较常用，所以在这里介绍]]></content>
      <categories>
        <category>图像处理</category>
        <category>OpenCV</category>
        <category>Projection</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Projection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode笔记]]></title>
    <url>%2F2019%2F05%2F07%2FLeetcode%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[#1 twoSumGiven an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: Given nums = [2, 7, 11, 15], target = 9, Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. 1234567class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: for i, item in enumerate(nums): if target - item in nums and nums.index(target - item) != i: return [i, nums.index(target - item)] 总结： 刚开始用了直接for所有的元素的方法，忘记考虑当两个数字重复的时候需要怎么办，考虑了之后在非常大的数的情况下爆炸了 标准答案说到了hash表，但是其实在python实现里面本身就是个hash（不然怎么从索引得到结果），不需要考虑这个问题 然后考虑了把所有东西都放一个dict里面（毕竟hash？），但是遇到的问题是从value直接得到key会生一些问题。如果把数字作为key，索引作为value会发现数字有重复的，会覆盖key的值 这时候突然发现，如果用数字作为索引的话其实dict和list没有本质区别，在list里面操作就行了，而且list的.index()可以直接返回这个值得坐标（找到的是第一个值！！） 所以直接用enumerate把所有的index和item都列出来就可以解决了，神奇。 #27 remove elementGiven an array nums and a value val, remove all instances of that value in-place and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. The order of elements can be changed. It doesn’t matter what you leave beyond the new length. Example 1: Given nums = [3, 2, 2, 3], val = 3, Your function should return length = 2, with the first two elements of nums being 2. It doesn’t matter what you leave beyond the returned length.Example 2: Given nums = [0, 1, 2, 2, 3, 0, 4, 2], val = 2, Your function should return length = 5, with the first five elements of nums containing 0, 1, 3, 0, and 4. Note that the order of those five elements can be arbitrary. It doesn’t matter what values are set beyond the returned length. 12345678910111213class Solution: def removeElement(self, nums: List[int], val: int) -&gt; int: remove_nums = 0 ori_length = len(nums) for i in range(len(nums)): if nums[i] == val: remove_nums += 1 nums[i] = float('inf') nums.sort() nums = nums[:ori_length - remove_nums] return len(nums) 总结： 这道题的重点是需要in - place的处理，空间复杂度要求很高（然而我的空间结果很垃圾）。一个重点就是返回的list不需要按照原来的顺序排列 从不需要原来的顺序得到的思路是：我把需要删除的东西的位置改成了inf，然后对所有部分进行排序，得到排序之后的结果再进行切片（这里刚开始的思路是删掉这个地方的东西然后再insert，后来发现直接替换就好了） 其实也可以直接用交换位置的方法，不用切片，因为题目只需要前面的这些元素符合要求就可以了，没有说后面的怎么样。 看了一些discussion都是memory只比5 % 的人少。。。但是差距都不大应该没问题！ 看到了一个超级牛逼简要写法：1234while val in nums: nums.remove(val)return len(nums) #80Given a sorted array nums, remove the duplicates in-place such that duplicates appeared at most twice and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Example 1: Given nums = [1,1,1,2,2,3], Your function should return length = 5, with the first five elements of nums being 1, 1, 2, 2 and 3 respectively. It doesn’t matter what you leave beyond the returned length. 123456789101112131415161718192021222324class Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: i = 0 while(True): if i &gt;= len(nums) -1: if len(nums) &gt; 2 and nums[i] == nums[i - 2]: nums = nums[:nums.index(nums[i])+2] break else: break if nums[i] == nums[i+1]: i += 1 else: next_num = nums[i+1] start_index = nums.index(nums[i]) next_index = nums.index(next_num) if next_index - start_index &gt; 2: for l in range(start_index+2, next_index): nums[l] = float('inf') i = next_index while float('inf') in nums: nums.remove(float('inf')) return len(nums) 总结： 我深信我的方法虽然蠢但是没有问题，但是跑出来就是有问题，分明我return之前的数据还都是对的，但是return之后显示的东西就都有问题了 主要思路是这样的 因为in-place操作，所以就不能直接用remove去掉元素导致下标错乱 本来是想和上面的思路一样，换成inf，然后再把有inf的部分删除掉（参考了#27的简易解法） 怎么换成inf呢，我判断的方法是找到下一个值得index，然后计算这个index和上一个之间差多少个数，然后把富裕的数字都替换成inf 忽略的问题： 数数数错了很多问题 最开始没有考虑到什么停止 然后没有考虑到如果最后一个数字重复了两遍以上要怎么办的问题（这也是我用next_index的一个弊端） 然后看着大佬的代码哭出了声！！！12345678class Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: i = 0 for n in nums: if i &lt; 2 or n != nums[i-2]: nums[i] = n i += 1 return i 总结： 哇这个思路真的牛逼！ 中心思想就是让n来增加但是i不增加，这里已经说了不在意前面项之后list里面的内容，也就是说前n项之后的东西都不用管了。既然如此的话与其用inf来替换这个位置的数字，不如直接用后面的项填在相对应的位置上，只有填成功了才会增加i 这里需要先判断i的值是否小于2，然后再计算nums[i-2]，否则会out of range i跑的速度没有超过n跑的速度所以没有关系 合理利用题里面的条件限制真的很重要！！ #189 Rotate arrayGiven an array, rotate the array to the right by k steps, where k is non-negative. Example 1: Input: [1,2,3,4,5,6,7] and k = 3Output: [5,6,7,1,2,3,4]Explanation:rotate 1 steps to the right: [7,1,2,3,4,5,6]rotate 2 steps to the right: [6,7,1,2,3,4,5]rotate 3 steps to the right: [5,6,7,1,2,3,4] Example 2: Input: [-1,-100,3,99] and k = 2Output: [3,99,-1,-100]Explanation:rotate 1 steps to the right: [99,-1,-100,3]rotate 2 steps to the right: [3,99,-1,-100] Note: Try to come up as many solutions as you can, there are at least 3 different ways to solve this problem.Could you do it in-place with O(1) extra space? 思路 需不需要注意k = 0的时候 如果k的个数特别大需不需要简化一下 12345678910class Solution: def rotate(self, nums: List[int], k: int) -&gt; None: """ Do not return anything, modify nums in-place instead. """ steps = k % len(nums) unchange_nums = nums[:len(nums) - steps] change_nums = nums[len(nums) - steps : ] nums[:steps] = change_nums nums[steps:] = unchange_nums 总结： 居然第一种就这么写出来了，实际上就是把后面的数字移动到前面去 注意nums不能直接用change_nums + unchange_nums，大概是他认为这个不是in-place了吧 另一种方法：in-place123456789101112 k = k % len(nums) self.reverse_nums(nums,0,len(nums) - 1) self.reverse_nums(nums,0,k-1) self.reverse_nums(nums,k,len(nums) - 1)def reverse_nums(self,nums,start,end): while start &lt; end: temp = nums[start] nums[start] = nums[end] nums[end] = temp start += 1 end -= 1 实际上，rotate的另外一种方法是先把整个list反向，然后把前面的k个反向，然后再把后面的(n-k)个反向（这里我是没想到的） 把一个数组反向的算法就是从两头向中间逼近着交换（我该好好去看看基础的算法了。。） 最后，还有一种方法是跳着设置值，也就是说k个之后的值就应该是现在这个位置的值 #41 First Missing PositiveGiven an unsorted integer array, find the smallest missing positive integer. Example 1: Input: [1,2,0]Output: 3Example 2: Input: [3,4,-1,1]Output: 2Example 3: Input: [7,8,9,11,12]Output: 1Note: Your algorithm should run in O(n) time and uses constant extra space. 第一个思路：时间nlog(n)12345678class Solution: def firstMissingPositive(self, nums: List[int]) -&gt; int: nums.sort() target = 1 for n in nums: if n == target: target += 1 return target 这个思路整体建立在先排序的基础上，但是排序的时间复杂度本身就已经是nlog(n)了 排序 - 找到比0大的数字从这里开始 - 这个数字不符合的话找下一个 但是我在找比0大的数字的时候还想着把list切片，切片就又需要考虑0啊，1啊，缺多少个数字的问题，空的list。其实根本不用这么麻烦 本质上这个方法就是，找到miss的正数，那就从正数的第一个（1）开始找，如果找到了这个数就继续找下一个（target++），总是能找到的嘛，找到的就是缺的数字了 自己的方法12345678910111213141516class Solution: def firstMissingPositive(self, nums: List[int]) -&gt; int: if nums is None or len(nums) == 0: return 1 for i in range(len(nums)): target_num = i + 1 if nums[i] == target_num: if i == len(nums) - 1: return target_num + 1 else: continue if target_num in nums: temp = nums.index(target_num) nums[temp],nums[i] = nums[i], nums[temp] else: return target_num 桶排序：要把对应的数字放在对应的位置上 这道题里应该的样子就是nums[index] = index + 1 大佬的思路 -&gt; 首先判断边界条件！！(学到了学到了) 看过了上面的提示写出来的第二版 判断边界条件 判断这个数字是不是摆在了正确的位置 正确，判断是否是最后一个数字 是，输出的是最后一个数字+1 不是，这个位置的正确了，判断下一个位置 没有，判断nums里面还有没有应该摆在这个位置的数字 有，那就和这个位置交换 没有，那没有的数字就是缺少的数字了 因为每次都是把数字换到了正确的位置了，所以交换最多进行len(nums)次，时间复杂度是O(n) 123456789def firstMissingPositive(self, nums): for i in xrange(len(nums)): while 0 &lt;= nums[i]-1 &lt; len(nums) and nums[nums[i]-1] != nums[i]: tmp = nums[i]-1 nums[i], nums[tmp] = nums[tmp], nums[i] for i in xrange(len(nums)): if nums[i] != i+1: return i+1 return len(nums)+1 大佬的另一个方法，其实思路和上面的差不多，就是把数字换到正确的位置上，但是判断的条件和我的有一点不同，可能因为我的是基于python的功能 其中，换到正确位置的数字就是在1到len(nums)之间的数字。nums[i]-1是这个数字应该的坐标位置，如果应该的位置和现在的位置的数字不一样，那就交换这两个数字 注意这里需要用while换，要一直换到正确的位置才可以 这样的结果就是大家都按正确的填好了，最后不对的那个位置的index+1就是需要的结果 #299You are playing the following Bulls and Cows game with your friend: You write down a number and ask your friend to guess what the number is. Each time your friend makes a guess, you provide a hint that indicates how many digits in said guess match your secret number exactly in both digit and position (called “bulls”) and how many digits match the secret number but locate in the wrong position (called “cows”). Your friend will use successive guesses and hints to eventually derive the secret number. Write a function to return a hint according to the secret number and friend’s guess, use A to indicate the bulls and B to indicate the cows. Please note that both secret number and friend’s guess may contain duplicate digits. Example 1: Input: secret = “1807”, guess = “7810” Output: “1A3B” Explanation: 1 bull and 3 cows. The bull is 8, the cows are 0, 1 and 7.Example 2: Input: secret = “1123”, guess = “0111” Output: “1A1B” Explanation: The 1st 1 in friend’s guess is a bull, the 2nd or 3rd 1 is a cow.Note: You may assume that the secret number and your friend’s guess only contain digits, and their lengths are always equal. 1234567class Solution: def getHint(self, secret: str, guess: str) -&gt; str: bull = sum(a == b for a,b in zip(secret,guess)) cow = 0 for x in set(guess): cow += min(secret.count(x),guess.count(x)) return str(bull) + "A" + str(cow-bull) + "B" 这里自己想了一些比较蠢的想法之后直接参考别人的了 其一是比对他们两个位置和数字都相同的东西，想要转换成dict来比较，但是后来发现string就可以直接index了不用这么麻烦 想过能不能按位做减法，未果 其二是在得到了bull之后把bull的部分从原来的里面剔除出去然后再比较相似的数字 遇到了主要问题是重复的数字怎么办以及如何剔除出去bull 主要思路是这样的： 其实cow的数量就是bull-cow都是的数量减去bull的数量，也就相当于维恩图里面，只有A的量是A的量 - 同时AB的量。这里是bull就相当于AB都有，两个里面所有重复的数量就相当于A的量 这样可以做减法就解决了上面的从bull得到cow的问题！！ 所以说看问题还是要看本质 面对重复的数字，居然可以直接把string转换成set 这里复习一下set好吗！！！这个集合居然可以没有重复的元素，平常我忽视你了呀小可爱，转化成set就不会重复了哦，震惊！！ 这样问题就变成了： 求bull：用zip把两个东西一一对应的打包起来（居然还有你小可爱！）直接对比 求both：guess里面猜的次数就是总体的次数，secret里面的次数是真实的次数，对于每个在guess里面（set）的元素都看看分别在两个里面是多少个，然后小的那个就是both的大小 这里介绍.count()小可爱，居然还可以数数！ 最后both-bull就是结果了 #134 gas station居然自己搞出来了一个看起来很蠢的1234567891011121314151617181920212223242526class Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: if sum(gas) &lt; sum(cost): return -1 tank = 0 current = 0 counter = 0 while(True): tank = tank + gas[current] - cost[current] if tank &lt; 0: if current &lt; len(gas): current = current + 1 counter = 0 tank = 0 continue else: return -1 current += 1 current = current % len(gas) counter += 1 # print(current,counter) if counter == len(gas): return current % len(gas) ~时间超过了百分之48的人，感觉可能还可以吧~时间都是骗人的又跑了一次居然超过了百分之86的！！ 重点 一直按着顺序跑，不会跳着走 如果gas的总量从一开始就小于cost的总量，那绝对不可能 我的思路： 从第一个点开始试着跑，一直到试着从最后一个点开始跑，找到了就直接返回 增加一个计数的var，记一共跑了多远，因为是按着顺序跑的所以这个var等于gas的长度的时候就是跑完了 避免out of range问题，需要求余数 遇到问题： 当tank小于0，更新完条件之后记得continue继续循环呀 刚开始想用的判断条件是for或者while里面带条件，还想了一下要不要zip这两个数据，但是都是list实在是没有必要。但是感觉是想的实在是太多了 1234567891011class Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: if sum(gas) &lt; sum(cost): return -1 rest = start = 0 for i in range(len(gas)): rest += gas[i] - cost[i] if rest &lt; 0: start = i + 1 rest = 0 return start 居然有这么简要的写法！！ 所以只要不是sum(gas) &lt; sum(cost)就一定会有解诶，神奇。也就是说我上面有一个返回的-1是没有意义的 而且用for的话就不用再考虑counter的问题了 从哪里失败就从哪里的下一个爬起来 #118 Pascal’s TriangleExample: Input: 5Output:[ [1], [1,1], [1,2,1], [1,3,3,1], [1,4,6,4,1]] 1234567891011121314151617181920212223class Solution: def generate(self, numRows: int) -&gt; List[List[int]]: result = [] if numRows == 0: return [] for row in range(numRows): now_row = [] if row == 0: now_row = [1] elif row == 1: now_row = [1,1] else: now_row = [1] for member in range(1,row): now_row.append(result[row-1][member-1] + result[row-1][member]) now_row.append(1) result.append(now_row) return result 总算是自己写出来一个东西了 好简单，除了前两行是特定的，其他的可以归为一类 求一个简单的数学关系就行了，数数别数错了！！注意数0 唯一没有注意的点就是：事先不知道list的大小，所以初始化成空的之后需要用append添加元素 #119 杨辉三角形2Given a non-negative index k where k ≤ 33, return the kth index row of the Pascal’s triangle. Note that the row index starts from 0. Input: 3Output: [1,3,3,1] 1234567class Solution: def getRow(self, rowIndex: int) -&gt; List[int]: L = [1] while True: if len(L) == rowIndex + 1: return L L = [u+v for u,v in zip([0]+L,L+[0])] 没想到杨辉三角形的代码也有简要的解法，这个是用L记录了上一行的信息，然后再把这行扩充两个0，相当于这个三角形的本质是两行错位相加！！ 注意最后的L得到的是一个list，list要有list的样子 更加理解了一下zip和单行for的用法 index从0开始，结果开始没有注意到 while true 加上一个 if的效果等同于for的效果！！！越写越糊涂 #169 Majority ElementGiven an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. You may assume that the array is non-empty and the majority element always exist in the array. Example 1: Input: [3,2,3]Output: 3Example 2: Input: [2,2,1,1,1,2,2]Output: 2 1234class Solution: def majorityElement(self, nums: List[int]) -&gt; int: nums.sort() return nums[len(nums)//2] 思路：这回想到了很多历遍的方法，但是感觉太蠢了，终于开始想怎么才能更好的实现了 在写写画画的时候突然考虑到，如果有超过一半的数量都是这个数的话，把这个list排序之后最中间的那个数肯定是这个数 极限情况就是两个元素差1，这时候是多一点的那个数的边界上 其他的情况下就是在出现最多的那个数的中间 本来想要用floor的，但是发现需要math包，所以用了 // 来求除之后的整数 #229 Majority Element 2Given an integer array of size n, find all elements that appear more than ⌊ n/3 ⌋ times. Note: The algorithm should run in linear time and in O(1) space. Example 1: Input: [3,2,3]Output: [3]Example 2: Input: [1,1,1,3,3,2,2,2]Output: [1,2]12345678910111213141516171819202122class Solution: def majorityElement(self, nums: List[int]) -&gt; List[int]: if not nums: return [] major1,major2,count1,count2 = 0,1,0,0 for n in nums: if major1 == n: count1 += 1 elif major2 ==n: count2 += 1 elif count1 ==0: major1 = n count1 = 1 elif count2 == 0: major2 = n count2 =1 else: count1 -= 1 count2 -= 1 return [n for n in (major1,major2) if nums.count(n) &gt; len(nums) // 3] 注意这道题说的是出现次数大于1/3的数字，所以结果只有只能是没有，1个或者两个，不存在结果是三个的情况！ 这个想了半天不会做，查了一下用的是Boyer-Moore Majority Vote algorithm 这个算法的主要意思是如果两拨人打架，打架一对一抵消，然后看看剩下的部分哪个比较多 记录剩下的东西的方法就是增加了一个额外的部分，包括major和count两部分，major记录的是有剩余的数是什么，count记录还有多少个 如果count没有了，那么就从现在遇到的新的数开始记 如果现在的数不是需要的，那么count - 1，如果是现在需要的那么count + 1 最开始是用在一个数组里面找超过一半的数的，但是我上一道题用了其他方法所以没用到 注意因为是求1/3的数字，所以虽然有剩下的，但是剩下的不一定都是符合要求的，需要再数一下个数对不对（这才有了return这一行里面的东西） 人类的算法真是奇幻无穷 #274 h-indexGiven an array of citations (each citation is a non-negative integer) of a researcher, write a function to compute the researcher’s h-index. According to the definition of h-index on Wikipedia: “A scientist has index h if h of his/her N papers have at least h citations each, and the other N − h papers have no more than h citations each.” Example: Input: citations = [3,0,6,1,5]Output: 3Explanation: [3,0,6,1,5] means the researcher has 5 papers in total and each of them had received 3, 0, 6, 1, 5 citations respectively. Since the researcher has 3 papers with at least 3 citations each and the remaining two with no more than 3 citations each, her h-index is 3.Note: If there are several possible values for h, the maximum one is taken as the h-index. 1234567891011class Solution: def hIndex(self, citations: List[int]) -&gt; int: result = 0 for h_cand in range(len(citations) + 1): h_more = 0 for citation in citations: if citation &gt;= h_cand: h_more += 1 if h_more &gt;= h_cand: result = max(result,h_cand) return result 思路，非常直观的方法，直接iterate所有的元素，如果找到了更大的result的值就取最大的（根据题目要求） 注意的点在需要 h_more &gt;= h_cand而不是等于，因为给出的定义的意思是index-h是有h个的值大于等于h，h_more的个数会比h_cand多（但是因为取了下面的max，所以等于其实也是可以得） 这个的速度真的好慢，尝试一下binary search 1234567891011121314151617class Solution: def hIndex(self, citations: List[int]) -&gt; int: bucket = [0 for n in range(len(citations)+1)] for nums in citations: if nums &gt;= len(citations): bucket[len(citations)] += 1 else: bucket[nums] += 1 result = 0 for nums in range(len(bucket)): nums = len(bucket) - nums -1 result += bucket[nums] if result &gt;= nums: return nums return 0 用了桶排序的神奇方法 还是取决于定义，如果一共有5个paper的话，可以选的h的值有6个，分别是0 1 2 3 4 5，把这留个值分成六个桶，每个里面放的就是比这桶的inde等于的paper的数量 如果总数直接大于最大的桶数，就放在最后一个里面 这是在第一个循环干的事情 第二个循环里，把这些桶里面的值取出来就是比这个桶的index大于等于的paper的数量，从后往前数，如果这个paper的数量大于了现在的index，那就说明现在的index就是h！ 这里学到了一个创建固定长度列表的方法bucket = [0 for n in range(len(citations)+1)] 12345678class Solution: def hIndex(self, citations: List[int]) -&gt; int: citations.sort(reverse = True) result = 0 for i,n in enumerate(citations): if n &gt;= i+1: result = max(result,i+1) return result 再另一种思路，用了排序 如果把这个list按降序排序的话，index的数量加一就是目前数过的paper的数量，citation[index]就是这个数量上面对应的citation的数量，这两个值应该正好相等，或者citation更大一点，需要在排好序的内容里面找到这一项！ 这样速度比桶排序稍微慢一点但是还是蛮快的，起码比第一种要快很多了 #275 h-index 21234567891011class Solution: def hIndex(self, citations: List[int]) -&gt; int: n = len(citations) l, r = 0, n-1 while l &lt;= r: mid = (l+r)//2 if citations[mid] &gt;= n-mid: r = mid - 1 else: l = mid + 1 return n-l 可以依然沿用上面的方法，但是可能是因为数据量上去的原因，所以速度变慢了 这里可以加入二分法搜索取代上面的直接iterate while的条件是因为移动一位，所以会出现l&gt;r的情况，在这种情况下就可以停下来了 二分法就是这么写的！]]></content>
      <categories>
        <category>算法</category>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment2之Pytorch]]></title>
    <url>%2F2019%2F05%2F07%2FCS231nassignment2Pytorch%2F</url>
    <content type="text"><![CDATA[这部分需要在torch和TensorFlow两个framework里面选一个。 PyTorchWhat 加入了Tensor的object（类似于narray），不需要手动的backprop了 Why 在GPU上面跑，不需要CUDA就可以在自己的GPU上面跑NN functions很多 站在巨人的肩膀上！ 在实际使用中应该写的深度学习代码 学习资料 Justin Johnson has made an excellenttutorial for PyTorch. DetailedAPI doc If you have other questions that are not addressed by the API docs, the PyTorch forum is a much better place to ask than StackOverflow. 整体结构 第一部分，准备，使用dataset 第二部分，abstraction level1，直接在最底层的Tensors上面操作 第三部分，abstraction level2，nn.Module定义一个任意的NN结构 第四部分，abstraction level3，nn.Sequential，定义一个简单的线性feed - back网络 第五部分，自己调参，尽量让CIFAR - 10的精度尽可能高 Part 1.Preparationpytorch里面有下载dataset，预处理并且迭代成minibatch的功能 import torchvision.transforms as T 这个包包括了预处理以及增强data的功能，在这里选择了减去平均的RGB并且除以标准差 然后对不同的部分分别构建了一个dataset object（训练，测试，val），这个dataset会载入一次training example，并且在DataLoader部分构建minibatch 1234567891011121314151617181920212223242526272829NUM_TRAIN = 49000# The torchvision.transforms package provides tools for preprocessing data# and for performing data augmentation; here we set up a transform to# preprocess the data by subtracting the mean RGB value and dividing by the# standard deviation of each RGB value; we've hardcoded the mean and std.transform = T.Compose([ T.ToTensor(), T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])# We set up a Dataset object for each split (train / val / test); Datasets load# training examples one at a time, so we wrap each Dataset in a DataLoader which# iterates through the Dataset and forms minibatches. We divide the CIFAR-10# training set into train and val sets by passing a Sampler object to the# DataLoader telling how it should sample from the underlying Dataset.cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True, transform=transform)loader_train = DataLoader(cifar10_train, batch_size=64, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True, transform=transform)loader_val = DataLoader(cifar10_val, batch_size=64, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, transform=transform)loader_test = DataLoader(cifar10_test, batch_size=64) 需要一个是否使用GPU的flag，并且set到true。在这个作业里面不是必须用GPU跑，但是如果电脑不能enableCUDA的话，就会自动返回CPU模式。 除此之外，建立了两个global var，dtype代表float32，device代表用哪个 因为mac本身不支持CUDA，而且好像新版本的系统还不能安装N卡的部分，所以现在用的CPU 12345678910111213USE_GPU = Truedtype = torch.float32 # we will be using float throughout this tutorialif USE_GPU and torch.cuda.is_available(): device = torch.device('cuda')else: device = torch.device('cpu')# Constant to control how frequently we print train lossprint_every = 100print('using device:', device) Part2 Barebones PyTorch 虽然有很多高层的API已经有了很多功能，但是这部分从比较底层的部分来进行 建立一个简单的fc - relu net，两个中间层，没有bias 用Tensor的method来计算forward，并且用自带的autograd来计算back 如果设定了requires_grad = True，那么在计算的时候不仅会计算值，还会生成计算back的graph if x is a Tensor with x.requires_grad == True then after backpropagation x.grad will be another Tensor holding the gradient of x with respect to the scalar loss at the end PyTorch Tensors: Flatten Function Tensors是一个和narray很像的东西，定义了很多比较好用的功能，比如flatten来reshape image data 在Tensor里面一个图片的形状是NxCxHxW datapoint的数量 channels feature map的H和W 但是在affine里面我们希望一个datapoint可以表现成一个单独的vector，而不是channel和宽和高 所以在这里用flatten来首先读取NCHW的数据，然后返回这个data的view（相当于array里面的reshape，把它改成了Nx？？，其中？？可以是任何值） 123456def flatten(x): N = x.shape[0] # read in N, C, H, W # "flatten" the C * H * W values into a single vector per image return x.view(N, -1) Barebones PyTorch: Two-Layer Network当定义一个 two_layer_fc的时候，会有两层的中间带relu的forward，在写好了forward之后需要确保输出的形状是对的并且没有什么问题(最近好像对这个大小已经没有什么疑问了) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import torch.nn.functional as F # useful stateless functionsdef two_layer_fc(x, params): """ A fully-connected neural networks; the architecture is: NN is fully connected -&gt; ReLU -&gt; fully connected layer. Note that this function only defines the forward pass; PyTorch will take care of the backward pass for us. The input to the network will be a minibatch of data, of shape (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units, and the output layer will produce scores for C classes. Inputs: - x: A PyTorch Tensor of shape (N, d1, ..., dM) giving a minibatch of input data. - params: A list [w1, w2] of PyTorch Tensors giving weights for the network; w1 has shape (D, H) and w2 has shape (H, C). Returns: - scores: A PyTorch Tensor of shape (N, C) giving classification scores for the input data x. """ # first we flatten the image x = flatten(x) # shape: [batch_size, C x H x W] w1, w2 = params # Forward pass: compute predicted y using operations on Tensors. Since w1 and # w2 have requires_grad=True, operations involving these Tensors will cause # PyTorch to build a computational graph, allowing automatic computation of # gradients. Since we are no longer implementing the backward pass by hand we # don't need to keep references to intermediate values. # you can also use `.clamp(min=0)`, equivalent to F.relu() x = F.relu(x.mm(w1)) x = x.mm(w2) return xdef two_layer_fc_test(): hidden_layer_size = 42 # minibatch size 64, feature dimension 50 x = torch.zeros((64, 50), dtype=dtype) w1 = torch.zeros((50, hidden_layer_size), dtype=dtype) w2 = torch.zeros((hidden_layer_size, 10), dtype=dtype) scores = two_layer_fc(x, [w1, w2]) print(scores.size()) # you should see [64, 10]two_layer_fc_test() Barebones PyTorch: Three-Layer ConvNet 上下这两个都是，在测试的时候可以直接pass 0来测试tensor的大小是不是对的 网络的结构 conv with bias，channel_1 filters，KW1xKH1，2 zero - padding RELU conv with bias，channel_2 filters，KW2xKH2，1 zero - padding RELU fc with bias，输出C class 注意！在这里fc之后没有softmax的激活层，因为在后面计算loss的时候会提供softmax，计算起来更加有效率 注意2！在conv2d之前不需要flatten，在fc之前才需要flatten 123456789101112131415161718192021222324252627282930313233343536373839404142434445def three_layer_convnet(x, params): """ Performs the forward pass of a three-layer convolutional network with the architecture defined above. Inputs: - x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images - params: A list of PyTorch Tensors giving the weights and biases for the network; should contain the following: - conv_w1: PyTorch Tensor of shape (channel_1, 3, KH1, KW1) giving weights for the first convolutional layer - conv_b1: PyTorch Tensor of shape (channel_1,) giving biases for the first convolutional layer - conv_w2: PyTorch Tensor of shape (channel_2, channel_1, KH2, KW2) giving weights for the second convolutional layer - conv_b2: PyTorch Tensor of shape (channel_2,) giving biases for the second convolutional layer - fc_w: PyTorch Tensor giving weights for the fully-connected layer. Can you figure out what the shape should be? (N,channel_2*H*W) - fc_b: PyTorch Tensor giving biases for the fully-connected layer. Can you figure out what the shape should be? (C,) Returns: - scores: PyTorch Tensor of shape (N, C) giving classification scores for x """ conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params scores = None ################################################################################ # TODO: Implement the forward pass for the three-layer ConvNet. # ################################################################################ # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** x = nn.functional.conv2d(x, conv_w1, bias=conv_b1, padding=2) x = nn.functional.conv2d(F.relu(x), conv_w2, bias=conv_b2, padding=1) x = flatten(x) x = x.mm(fc_w) + fc_b scores = x # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ################################################################################ # END OF YOUR CODE # ################################################################################ return scores Barebones PyTorch: Initialization random_weight(shape) initializes a weight tensor with the Kaiming normalization method. -&gt; 使用了KAIMING normal zero_weight(shape) initializes a weight tensor with all zeros. Useful for instantiating bias parameters. 123456789101112131415161718192021222324252627def random_weight(shape): """ Create random Tensors for weights; setting requires_grad=True means that we want to compute gradients for these Tensors during the backward pass. We use Kaiming normalization: sqrt(2 / fan_in) """ if len(shape) == 2: # FC weight fan_in = shape[0] else: # conv weight [out_channel, in_channel, kH, kW] fan_in = np.prod(shape[1:]) # randn is standard normal distribution generator. w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in) w.requires_grad = True return wdef zero_weight(shape): return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)# create a weight of shape [3 x 5]# you should see the type `torch.cuda.FloatTensor` if you use GPU.# Otherwise it should be `torch.FloatTensor`random_weight((3, 5)) Barebones PyTorch: Check Accuracy 在这部分不需要计算grad，所以要关上torch.no_grad()避免浪费 输入 一个DataLoader来给我们想要check的data分块 一个表示模型到底是什么样子的model_fn，来计算预测的scores 这个model需要的参数 没有返回值但是会print出来acc 12345678910111213141516171819202122232425262728def check_accuracy_part2(loader, model_fn, params): """ Check the accuracy of a classification model. Inputs: - loader: A DataLoader for the data split we want to check - model_fn: A function that performs the forward pass of the model, with the signature scores = model_fn(x, params) - params: List of PyTorch Tensors giving parameters of the model Returns: Nothing, but prints the accuracy of the model """ split = 'val' if loader.dataset.train else 'test' print('Checking accuracy on the %s set' % split) num_correct, num_samples = 0, 0 with torch.no_grad(): for x, y in loader: x = x.to(device=device, dtype=dtype) # move to device, e.g. GPU y = y.to(device=device, dtype=torch.int64) scores = model_fn(x, params) _, preds = scores.max(1) num_correct += (preds == y).sum() num_samples += preds.size(0) acc = float(num_correct) / num_samples print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc)) BareBones PyTorch: Training Loop 用stochastic gradient descent without momentum来train，并且用torch.functional.cross_entropy来计算loss 输入 model_fc params learning_rate 没有输出 进行的操作 把data移动到GPU或者CPU 计算score和loss loss.backward() update params，这部分不需要计算grad BareBones PyTorch: Training a ConvNet 需要网络 Convolutional layer(with bias) with 32 5x5 filters, with zero - padding of 2 ReLU Convolutional layer(with bias) with 16 3x3 filters, with zero - padding of 1 ReLU Fully - connected layer(with bias) to compute scores for 10 classes 需要自己初始化参数，不需要tune hypers 注意1：fc的w的大小是D,C，跟数据无关需要从上一层的输出求 conv之后的图片大小从32-&gt; 30 12345678910111213141516171819202122232425262728293031learning_rate = 3e-3channel_1 = 32channel_2 = 16conv_w1 = Noneconv_b1 = Noneconv_w2 = Noneconv_b2 = Nonefc_w = Nonefc_b = None################################################################################# TODO: Initialize the parameters of a three-layer ConvNet. ################################################################################## *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****conv_w1 = random_weight((channel_1, 3, 5, 5))conv_b1 = zero_weight(channel_1)conv_w2 = random_weight((channel_2, channel_1, 5, 5))conv_b2 = zero_weight(channel_2)fc_w = random_weight((channel_2 * 30 * 30, 10))fc_b = zero_weight(10)# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****################################################################################# END OF YOUR CODE #################################################################################params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]train_part2(three_layer_convnet, params, learning_rate) Part3 PyTorch Module API 上面的所有过程是手算来track整个过程的，但是在更大的net里面就没有什么用了 nn.Module来定义网络，并且可以选optmi的方法 Subclass nn.Module. Give your network class an intuitive name like TwoLayerFC. __init__()里面定义自己需要的所有层. nn.Linear and nn.Conv2d 都在模块里自带了. nn.Module will track these internal parameters for you. Refer to the doc to learn more about the dozens of builtin layers. Warning: don’t forget to call the super().__init__() first!（调用父类） In the forward() method, define the connectivity of your network. 直接用init里面初始化好的方法来forward，不要再forward里面增加新的方法 用上面的方法来写一个三层的layer 注意需要初始化w和b的参数，用kaiming的方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class ThreeLayerConvNet(nn.Module): def __init__(self, in_channel, channel_1, channel_2, num_classes): super().__init__() ######################################################################## # TODO: Set up the layers you need for a three-layer ConvNet with the # # architecture defined above. # ######################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** self.conv_1 = nn.Conv2d(in_channel,channel_1,5,stride=1, padding=2,bias=True) nn.init.kaiming_normal_(self.conv_1.weight) nn.init.constant_(self.conv_1.bias, 0) self.conv_2 = nn.Conv2d(channel_1,channel_2,3,stride=1, padding=1,bias=True) nn.init.kaiming_normal_(self.conv_2.weight) nn.init.constant_(self.conv_2.bias, 0) self.fc_3 = nn.Linear(channel_2 * 32 * 32 , num_classes) nn.init.kaiming_normal_(self.fc_3.weight) nn.init.constant_(self.fc_3.bias, 0) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ######################################################################## # END OF YOUR CODE # ######################################################################## def forward(self, x): scores = None ######################################################################## # TODO: Implement the forward function for a 3-layer ConvNet. you # # should use the layers you defined in __init__ and specify the # # connectivity of those layers in forward() # ######################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** x = self.conv_1(x) x = self.conv_2(F.relu(x)) x = flatten(F.relu(x)) x = self.fc_3(x) scores = x # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ######################################################################## # END OF YOUR CODE # ######################################################################## return scoresdef test_ThreeLayerConvNet(): x = torch.zeros((64, 3, 32, 32), dtype=dtype) # minibatch size 64, image size [3, 32, 32] model = ThreeLayerConvNet(in_channel=3, channel_1=12, channel_2=8, num_classes=10) scores = model(x) print(scores.size()) # you should see [64, 10]test_ThreeLayerConvNet() Module API: Check Accuracy 不用手动pass参数了，直接就可以得到整个net的acc Module API: Training Loop 用optimizer这个object来update weights 输入 model optimizer epoch，可选 没有return，但是会打印出来training时候的acc 其实就是设置好model和optimizer就可以了 Part4 PyTorch Sequential API nn.Sequential没有上面的灵活，但是可以集成上面的一串功能 需要提前定义一个在forward里面能用的flatten 123456789101112131415161718192021# We need to wrap `flatten` function in a module in order to stack it# in nn.Sequentialclass Flatten(nn.Module): def forward(self, x): return flatten(x)hidden_layer_size = 4000learning_rate = 1e-2model = nn.Sequential( Flatten(), nn.Linear(3 * 32 * 32, hidden_layer_size), nn.ReLU(), nn.Linear(hidden_layer_size, 10),)# you can use Nesterov momentum in optim.SGDoptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)train_part34(model, optimizer) 实现三层，注意需要初始化参数 这里遇到了一个问题是当用random_weight实现的时候，acc会特别低 从这里发现可以重新定义另一个计算方法不同的weights 从这里得知如何给module增加新的function 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def xavier_normal(shape): """ Create random Tensors for weights; setting requires_grad=True means that we want to compute gradients for these Tensors during the backward pass. We use Xavier normalization: sqrt(2 / (fan_in + fan_out)) """ if len(shape) == 2: # FC weight fan_in = shape[1] fan_out = shape[0] else: fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW] fan_out = shape[0] * shape[2] * shape[3] # randn is standard normal distribution generator. w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / (fan_in + fan_out)) w.requires_grad = True return wchannel_1 = 32channel_2 = 16learning_rate = 1e-2model = Noneoptimizer = None################################################################################# TODO: Rewrite the 2-layer ConvNet with bias from Part III with the ## Sequential API. ################################################################################## *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****model = nn.Sequential( nn.Conv2d(3, channel_1,5,stride = 1,padding = 2), nn.ReLU(), nn.Conv2d(channel_1, channel_2,3,stride = 1,padding = 1), nn.ReLU(), Flatten(), nn.Linear(32*32*channel_2, 10),)def init_weights(m): print(m) if type(m) == nn.Linear: m.weight.data = xavier_normal(m.weight.size()) m.bias.data = zero_weight(m.bias.size())model.apply(init_weights)optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****################################################################################# END OF YOUR CODE ################################################################################train_part34(model, optimizer) Part5 来训练CIFAR-10吧！自己找net的结构，hyper，loss，optimizers来把CIFAR-10的val_acc在10个epoch之内升到70%以上！ Layers in torch.nn package: http://pytorch.org/docs/stable/nn.html Activations: http://pytorch.org/docs/stable/nn.html#non-linear-activations Loss functions: http://pytorch.org/docs/stable/nn.html#loss-functions Optimizers: http://pytorch.org/docs/stable/optim.html 一些可能的方法： Filter size: Above we used 5x5; would smaller filters be more efficient? Number of filters: Above we used 32 filters. Do more or fewer do better? Pooling vs Strided Convolution: Do you use max pooling or just stride convolutions? Batch normalization: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster? Network architecture: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include: [conv-relu-pool]xN -&gt; [affine]xM -&gt; [softmax or SVM] [conv-relu-conv-relu-pool]xN -&gt; [affine]xM -&gt; [softmax or SVM] [batchnorm-relu-conv]xN -&gt; [affine]xM -&gt; [softmax or SVM] Global Average Pooling: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in Google’s Inception Network (See Table 1 for their architecture). Regularization: Add l2 weight regularization, or perhaps use Dropout. 一些tips： 应该会在几百个iter里面就看到进步，如果params work well tune hyper的时候从一大片range和小的train开始，找到好一些的之后再围绕这个范围找（多训一点） 在找hyper的时候应该用val set 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647model = Noneoptimizer = None# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****channel_1 = 16channel_2 = 32channel_3 = 64channel_4 = 64fc_1 = 1024num_classes = 10model = nn.Sequential( nn.Conv2d(3, channel_1,3,stride = 1,padding = 1), nn.BatchNorm2d(channel_1), nn.ReLU(), nn.MaxPool2d(kernel_size = 2), nn.Conv2d(channel_1, channel_2,3,stride = 1,padding = 1), nn.BatchNorm2d(channel_2), nn.ReLU(), nn.MaxPool2d(kernel_size = 2), nn.Conv2d(channel_2, channel_3,3,stride = 1,padding = 1), nn.BatchNorm2d(channel_3), nn.ReLU(), nn.Conv2d(channel_3, channel_4,3,stride = 1,padding = 1), nn.BatchNorm2d(channel_4), nn.ReLU(), nn.MaxPool2d(kernel_size = 2), Flatten(), nn.Linear(4*4*channel_4, num_classes)# nn.Linear(fc_1,num_classes) )learning_rate = 1e-3optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****################################################################################# END OF YOUR CODE ################################################################################# You should get at least 70% accuracytrain_part34(model, optimizer, epochs=10) 第四层conv试过ksize=1，效果不是很好 BN好像效果很好 maxpool多一些，计算负担少而且效果好像比较好 最终val_acc在77-79左右，test_acc = 76.22]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于python生成动态变量名]]></title>
    <url>%2F2019%2F05%2F07%2F%E5%85%B3%E4%BA%8Epython%E7%94%9F%E6%88%90%E5%8A%A8%E6%80%81%E5%8F%98%E9%87%8F%E5%90%8D%2F</url>
    <content type="text"><![CDATA[动态生成变量名如果想要生成一系列的a0，a1，….a20这种变量名，直接手写太麻烦了 localslocal()，以字典的类型返回当前位置的全部局部变量 1234arrange_list = locals()for i in range(10): arrange_list['list_' + str(i)] = [] 调用动态变量，可以用字典的get方法得到变量的值 1234arrange_list = locals()for i in range(10): print(arrange_list.get('var'+str(i)), end = " ") 利用exec进行赋值12for i in range(5): exec('var&#123;&#125; = &#123;&#125;'.format(i, i)) 调用动态变量12for i in range(5): exec('print(var&#123;&#125;, end = " ")'.format(i))]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>动态生成变量名</tag>
        <tag>变量名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于多维数组的转置和增加新的维度]]></title>
    <url>%2F2019%2F04%2F25%2F%E5%85%B3%E4%BA%8E%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%E7%9A%84%E8%BD%AC%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在二维转置的时候，a[i][j] = a[j][i]在多维数组转置的时候，需要交换他们的下标比如原来的数组是(X,Y,Z)，转置之后是(Z,X,Y)这时候应该用的是np.transpose(A,(2,0,1)) np.newaxis -&gt; 增加新的维度原来是（6，）的数组，在行上增加维度变成（1,6）的二维数组，在列上增加维度变为(6,1)的二维数组]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>narray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习OpenCV十八章_Camera models & calibration]]></title>
    <url>%2F2019%2F04%2F22%2FOpenCVCameracalibration%2F</url>
    <content type="text"><![CDATA[camera models &amp; calibration物体会吸收一部分的光，然后反射一部分的光，反射的光就是他自己的颜色，这个光被我们的眼睛（或者相机）接收，然后投影到我们的视网膜（或者相机的图片）上，这之间的几何关系在CV上面非常重要 其中一个非常简单的模型就是pinhole camera model。光穿过一面墙上的一个小的aperture，这个是这章的模型的开始，但是真实pinhole模型不是很好因为他不能快速曝光（聚集的光不够）-&gt; 眼睛会更厉害一点，但是len还会distort图片。 这章的目的： 如何camera calibration 纠正普通的pinhole模型的len的偏差 calibration也同样是获取三维世界的主要方式，因为一个场景不仅仅是三维，他们还有物理的空间和体积，所以获取pixel和三维诗句坐标的关系也很重要 18章纠正的是len的distortion，19章构建整个3D的结构 homography transform -&gt; 一个非常重要的要素 camera model 投影到image plane上面，结果在这个plane上面总是对焦的focus，图片的大小和这个焦距的长度有关 对于理想的pinhole来说，image plane到pinhole的距离就是准确的焦距 从这个基础的模型上 -&gt; 得到一个计算起来更加简单的模型 交换pinhole和projection plane的位置 现在pinhole的位置变成了projective plane的中心 每一个离开物体表面（Q）的光线都朝着projection center走 在横轴和投影面上的交点被定义为principal point 这个新的平面和以前的projective平面一样，上面投影上的物体也都是和原来一样的尺寸 换了模型之后没有了负号：x/f = X/Z 在理想的模型里可能觉得这个principal point就是image的中心，但是实际上中心不会在横轴和投影面的交点上 引入了两个新的参数 cx 和 cy 这两个参数实际上就是中心点的偏差（平面上的偏差），所以得到投影在image plane上面的实际坐标如下 在上面的公式里面用了两个不同的f，fx和fy，这是因为 在实际的图片里来说，其实每个像素格不是正方形而是长方形的 fx = 实际的focal length * sx（每个mm里面的像素数量） -&gt; 最终得到的fx是像素格 注意： sx和sy在calibration的时候并不能直接测量 physical focal length也不能被实际测量 我们只能得到这两个东西的乘积，f basic of projective geometry projective transform -&gt; 把physical world里面的一组点Qi（Xi,Yi,Zi）map到一张图片上面(xi,yi)的过程 用这个东西的时候，一个比较方便的方法是用homogeneous coordinates associ‐ ated with a point in a projective space of dimension n are typically expressed as an (n + 1)-dimensional vector (e.g., x, y, z becomes x, y, z, w), with the additional restric‐ tion that any two points whose values are proportional are, in fact, equivalent points 投影平面上面的维度是两维，我们可以把它表示成三维的东西 -&gt; 把现在存在维度的数字除以增加的维度的值就可以得到以前的值 用这种办法，可以把之前的fx，fy,cx,cy重新组织成一个矩阵：camera intrinsics matrix 下面这个形式重新乘回来就是之前的关系 在opencv里面也有得到homogeneous coordinates和由结果反推回来的函数 注意，在pinhole里面的成像速度是非常慢的，如果需要更快速地形成图片，我们需要通过lens来聚焦非常广范围里面的光 -&gt; 但是结果就是lens会产生distortion Rodrigues Transform 在三维的范围里，经常会使用一个3x3的矩阵来表示一个物体的旋转 只要把需要旋转的vector乘上这个mat就可以得到相应的结果 但是不是很好直观的得到这个旋转矩阵 介绍一种在opencv里面的表示方法 -&gt; 更容易直观的理解意思 本质上来说就是用一个vector表示每个角度上需要旋转多少 Rodrigues Transform指的就是矩阵表示法和向量表示法之间的关系 数学原理：余弦定理？（知道两个向量可以求出来他们之间的角度） 这两个关系之间可以很轻易的互相转化，opencv里面也有相应的库 lens distortion 在实际使用中因为制造球形的镜头更容易一些，并且很难测量是不是平的，所以lens都会产生distortion 在这部分介绍了两种主要得distortion，how to model radial distortion -&gt; 镜片的形状产生的 tangential distortion -&gt; 组装整个相机的时候产生的 radial 相机的distortion一般都会产生在接近imager边缘的部分（fisheye effect） 远离lens中心的部分比起中心部分会折叠更多，所以如果投影一个正方形，边的部分都会鼓起来 如果相机比较便宜的话（web camera），周围的折叠会更多，而好的相机会更注重减少radial distortion的效果 对于辐射的畸变来说，distortion会随着接近边边而增加 在实际中这个畸变很小，所以可以用泰勒级数的r=0附近展开来解决 对于比较便宜的web camera，可以选用k1或者k2 对于鱼眼这种畸变很大的，可以用k3 在distort之后的位置可以用以下的公式表示 (x,y)是原来的位置，corrected是教政治和的位置 r是离开中心的半径 tangential 在制造相机的时候，lens和image plane没有完全平行导致的，所以投影上去会是一个几何变换 这个distortion基本是由两个参数组成：p1和p2 总结下来，在相机的distortion里一共有五个参数，k1k2k3p1p2，这五个函数构成了一个distortion vector(5x1) 虽然在图像里面还有一些其他的畸变，但是因为影响没有这两个大所以opencv没有考虑这部分 calibration 上一部分得到了如何表示相机的参数以及distortion的参数 这部分考虑如何计算这些参数 其中一个函数clibrationCamera() 用相机去照一个已经知道结构的东西，里面有很多已经定义好了的点 通过这个可以得到相机的相对位置和角度，同时也可以得到intrinsic parameters 平移矩阵和旋转矩阵 对于每张照的图片的物体，这个物体的pose可以用一个旋转矩阵+一个平移矩阵描述，也就是用这个矩阵把现实世界中的点转化到投影平面上 旋转矩阵 旋转运动无论在多少维都可以被描述为：一个坐标的vector乘对应大小的方阵 -&gt; 用一个新的坐标系来描述这个点的位置 -&gt; 其实也就是改成了极坐标系？ 三维范围里面的旋转可以用两个角度表示 绕着x，y，z三个方向旋转的角度以及对应的矩阵是这个样子的 这三个方向的R乘在一起就是最后的旋转矩阵R，但是这个的方向是反着的，所以还需要一个transpose转回来 平移矩阵 平移矩阵用来描述怎么从一个坐标系统shift到另一个坐标系统 -&gt; 也就是一个从第一个坐标系原点到第二个坐标系原点的offset 在calibration的时候，就是从物体坐标系的原点到了相机坐标系的原点 平移矩阵： T→ = origin_object − origin_camera. 综合 结合上面两个矩阵来说，从object上面的一个点投影到camera plane上面的一个点的关系为 Pc→ =R⋅(Po→ −T→) 注意分清楚这里面的矩阵和向量 把上面的这个公式，再加上camera自己的intrinsic-correction。整体就是opencv里面需要求的所有部分 所求参数 三维的旋转用三个角度表示，三维的平移用三个parameter表示(x,y,z) -&gt; 现在得到了6个参数 相机的intrinsic mat(fx,fy,cx,cy) -&gt; 一共四个参数 现在一共需要求10个参数（但是相机的intrinsic是不变的） 求参数 -&gt; 在求解的时候，如果使用一个平面物体，那么每张图片都可以得到8个参数（位置的6个会随着图片变化 + 只能用两个参数来求intrinsic） 至少需要2张图片来得到所有的参数 calibration boards 从原则上来说，任何有特征的东西都可以被用来calibration，包括棋盘，圆格，randpattern，arUco等等,有些方法是基于三维的物体的基础上的，但是二维平面的物体更好操作 在这里主要选择的是用棋盘进行calibration 关于chessboard的函数cv::findChessboardCorners() 可以用这个函数找到棋盘的corners， param 需要输入8bit图片 需要输入这个棋盘每行每列应该有的格子数（计算的是内部点） 输出的是这么corner的坐标 可选flag决定需不需要多余的filter cv::cornerSubPix() 上面一步找到的只是corner的大概位置 在find corner里面自动call了这个函数，为了能得到更精确的结果 如果需要得到更精确的结果，可以重复的call这个函数，但是会有tighter termination criteria cv::drawChessboardCorners() 为debug用，更明确的画出来找到的corner 如果没有找到所有的，会把其他可能的用红色circle画出来，如果找到了，每一行的颜色会不一样 下一步转到perspective transform，这个transform会形成一个3x3的homography mat 关于circle grid的函数cv::findCirclesGrid() 和上面的棋盘没有什么本质的区别，主要就是画出来一个是黑白格，另一个是白色的背景上面有黑色的圆点，输出小圆点的位置 这个方法需要圆点是对称的，上下一组算做一行，竖着一列算一列，怎么数非常重要 Homography planar homography是一个平面到另一个平面的projection mapping，所以从一个2D平面到相机平面的过程就是一个planar homography 用矩阵的乘法就可以表示这个过程 其中Q是现实中的点，q是成像器上面的点，整体关系为：q→ = s ⋅ H ⋅ Q→ s，一个随意的scale参数，homography就是由着一个参数决定的 conventionally factored H, H由两个部分组成 physical上面的transformation，实际就是我们看到的这个物体的位置W = [R,t→] projection，取决于相机的intrinsic q→ = s ⋅ M ⋅ W ⋅ Q→，其中M是相机的intrinsic mat 我们希望Q不是给所有空间定义的点，而是一个定义在我们看的平面上面的坐标，这样计算起来会方便（三维转二维） 所以把Q里面的Z的坐标改成了0，这样旋转矩阵就会被简化为一个3x1的列 并且第三个列乘了Z的0之后就被消掉了 最后就可以把H表示出来了 -&gt; 3x3 = intrinsic(3x3) x (rota + trans)(1x3) 在计算homography mat的时候，用了多张同样内容的东西来计算translation和intrinsic 三个旋转，三个平移 -&gt; 每张图片有6个未知的参数 每张图片可以得到8个等式 把一个正方形mapping成一个四边形可以得到4个不同的(x,y) points 所以每多一张图片就可以多出来计算两个新的参数的机会 这样看，pdst→ = H * psrc→，反着也可以推回来，这样我们就算不知道M也可以计算H，或者说我们是用H来计算M 在opencv里面，cv::findHomography()可以用take一堆有关系的点然后返回他们之间的homo mat，点越多计算的越准确 虽然有其他的方法可以计算结果，但是对测量误差不是很友好 three robust fitting methods method to cv::RANSAC 随机的选择提供的点的subset，然后只用这些subset来计算homo mat 然后把剩下的数据拿来计算一下靠谱和不靠谱的 最后保存最有潜力的inliers 在现实中比较好用，可以过滤掉一部分噪音 LMeDS algorithm 减少median error 不需要更多的info和data来运行 但是it will perform well only if the inliers constitute at least a majority of the data points RHO algorithm 加权的第一种方法，运行速度更快 camera calibration棋盘corner个数 到底有多少参数 camera intrinsic 四个 distortion五个（或者更多）三个辐射（可以增加到6个） + 两个平移 这五个参数是从2D -&gt; 2D的 三个corner points可以得到6个信息，足够处理这五个参数 所以一张图就够了（只是原则上这么说） extrinsic parameters，这个东西的实际位置 但是因为intrinsic和extrinsic之间有对应的关系，一张图片并不够 -&gt; 因为在一张图片里还需要计算extrinsic的部分 假设有N个corner，一共有K个images（不同的position） 一共会有 2 N K个，2是x,y的坐标会有两个，然后N个corner，K个图片 暂时忽略distortion的参数，这样需要4个in和6K个ex（因为每张图片的ex都是不一样的） 2NK &gt;= 6K + 4 如果N = 5，只需要一张图片就可以解决。但是为了得到homo mat，至少需要两个K(之前说到过的) 无论检测到多少corner，得到的有用信息就是四个角 -&gt; 由此推测至少两个K 在实际的应用里面，一般需要7x8，至少十张图，这样受到noise的影响更小 具体的数学计算 为了简单，首先假设在calibration的时候根本没有distortion 对于每个view，会得到一个Homo mat，把这个mat拆成一个列向量(3x1) 在前面也知道H可以拆成M和一个[r1,r2,t]的向量相乘，再乘上一个scale s H=[h1,h2,h3] =s⋅M⋅[r1,r2,t],其中landa是1/s: 旋转向量的基底(orthogonal)是互相垂直的，因为已经把scale这个参数提出去了，所以可以直接认为r1和r2是基了，这样的话他们的点乘是0 把r1和r2用M和h来表示，这样的话r1r2等于0就可以转化成一个hM的公式 r1和r2的模也相等，所以可以继续得到一个等式 设置一个矩阵B等于M.-T * M-1，这样可以计算出来B的值(B算出来是对称的) 把B带回原来的等式，化简，然后把K个等式叠加在一起 这样就可以推出来几个参数的表达式 calibration的函数cv::calibrateCamera().来解决calibration的问题 得到的结果包括in mat, dis_co, 旋转向量和平移向量 输出的in mat的大小是3x3 输出的dis_co的大小取决于用多少级的distortion，一般来说是4，5个的已经对fisheye足够了，8个的话calibration的精度就特别高了 如果需要高精度的calibration的话，需要的图片数量也会疯狂增加 输入的部分包括 物体的坐标，指的是在chessboard上面的坐标点，是二维的点，其实也就是第几个格子？ 注意这里，统计的单位是格子，所以如果想要得到physical上的距离，需要在calibration board上量出来一个格子的长度，然后乘这个格子的数量 image上面的坐标，corners 一口气计算所有的参数不是很好实现，一般使用的方法是先固定一部分计算另一部分，然后再固定另一部分计算这一部分。当所有的东西都估计的差不多了，再一起计算 最后还有一个参数是termination criteria，终止的基准 -&gt; epsilon 会根据一个error来计算是否终止 只计算extrinsiccv::slovePnP() 有的时候我们已经得到了相机的intrinsic，只希望得到object的位置 大部分内容和上面都是一样的，除了 物体的位置只需要一个view distCo和intrinsic都是自己设置好的，不需要计算 cv::solvePnPRansac() 上面的函数对于outliers的robust效果不是很好，对于chessboard来说，这个robust不是很重要，因为棋盘自己本身已经很可靠了。但是对于现实世界中的物体来说不是这么可靠 加入了RANSAC部分？ Undistortion 在calibration里面有两个需要解决的事情，一个是distortion，一个是三维表达的正确性 opencv自己有一个可以用的方法 cv::undistor() -&gt; 可以一瞬间完成 cv::initUndistortRectifyMap() + cv::remap() -&gt; 在video上面使用的时候效率更高一些 undistortion map 在把一张图片undistort的时候，我们需要把每个像素都对应到output里面对应的地方去，有几种不同的表达方法 2-channel float 有一个对于NxM的remapping，表示成NxM的array，有两个channel（分别对应X和Y方向的remap），里面是浮点数 对于每一个输入的像素位置(i,j)，有一个对于这两个位置的向量，来表达这两个量应该哪里去 如果计算出来的结果不是一个整数，那么用interpolation来计算最后应该占的格子的数量 第二种表达式是 2-array float，每一个array是一个channel的移动 第三种是fixed point，计算的速度更快一点，但是需要提供的信息的精确度更高 cv::convertMaps() 因为有上面的三种不同的表达形式，所以这个函数用来在各个形式之间转变 cv::initUndistortRectifyMap() 从刚才的部分知道了到底什么是undistortion map，现在开始讨论如何计算这个map 现在先从单目相机开始monocular，如果双目的话可以直接计算depth（下一章） 步骤，分开是因为计算map只需要一次 先计算undistortion map cv::initUndistortRectifyMap() 输入的参数是intrinsic mat和distortion coefficient（从camera calibration得到的） 可以得到一个新的camera mat，这样的话即使不undistortion也可以得到正确的图片（在多个相机的calibration的时候比较重要） 最后会输出两张map 然后在图片上undistort cv::remap() 当计算了上面的map之后，就可以用remap这个函数进行校正了 输入的map的种类也是上面提到的三种都可以 cv::undistort() 如果只有一张图片，或者对于每张图片都需要重新计算map的时候，就需要用这个函数了（所以在项目里面用这个的速度会变慢） sparse undistortion cv::distortionPoints() 如果我没有整张图片，只有一些图片上的点，然后我只关心这些图片上的点，可以用这个函数计算这张图片上面关注点的位置]]></content>
      <categories>
        <category>图像处理</category>
        <category>OpenCV</category>
        <category>Calibration</category>
      </categories>
      <tags>
        <tag>camera</tag>
        <tag>calibration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231nassignment2CNN]]></title>
    <url>%2F2019%2F04%2F18%2FCS231nassignment2CNN%2F</url>
    <content type="text"><![CDATA[target 之前已经实践了fc的相关东西，但是在实际的使用里大家使用的都是CNN 所以这部分就开始实践CNN了 convolution: Native forward pass CNN的核心部分就是卷积 in cs231n/layers.py，conv_forward_naive 首先这时候不用考虑效率问题，最轻松的写就可以了 输入的数据是N个data，每个有C个channel，H的高度和W的宽度 每个输入和F个不同的filter做卷积，每个卷积核对所有的channel作用，卷积核的大小是HHxWW input x, (N,C,H,W) w, fliter weights of shape (F,C,HH,WW) b, bias, (F,) conv_param: dict “stride” 步长 “pad” zero-padding的大小 注意在padding的时候不要调整x，而是得到一个padding之后的新的东西 output out, (N,F,H’,W’) H’ = 1 + (H + 2 * pad - HH) / stride W’ = 1 + (W + 2 * pad - WW) / stride cache: (x,w,b,conv_param) implement 首先需要对输入的图片进行padding np.pad 输入的array pad的宽度，如果默认的话就是前后都加，然后是这个数字的宽度 -&gt; 注意这里的时候因为一共有四个维度，前两个维度是不用pad的 mode = ‘constant’ constant_values，表示的是pad进去的值，可以前后pad的不一样，因为这里是0-padding所以这里是0 要对所有图片进行处理，需要在N个图片里选择一个 在filter的所有里面选择一个 考虑在H方向和W方向的移动步数，然后通过这个步数和步长的乘积在原图里面取需要做卷积的部分 注意这里可以不用考虑channel，因为图片和filter的channel是同样的层数，所以直接可以boardcast 然后这个部分和卷积核相乘（直接乘），求和，加上bias，就是这个像素点上应该的数值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def conv_forward_naive(x, w, b, conv_param): """ A naive implementation of the forward pass for a convolutional layer. The input consists of N data points, each with C channels, height H and width W. We convolve each input with F different filters, where each filter spans all C channels and has height HH and width WW. Input: - x: Input data of shape (N, C, H, W) - w: Filter weights of shape (F, C, HH, WW) - b: Biases, of shape (F,) - conv_param: A dictionary with the following keys: - 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions. - 'pad': The number of pixels that will be used to zero-pad the input. During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides) along the height and width axes of the input. Be careful not to modfiy the original input x directly. Returns a tuple of: - out: Output data, of shape (N, F, H', W') where H' and W' are given by H' = 1 + (H + 2 * pad - HH) / stride W' = 1 + (W + 2 * pad - WW) / stride - cache: (x, w, b, conv_param) """ out = None ########################################################################### # TODO: Implement the convolutional forward pass. # # Hint: you can use the function np.pad for padding. # ########################################################################### stride = conv_param['stride'] pad = conv_param['pad'] N, C, H, W = x.shape F, _, HH, WW = w.shape H_out = 1 + (H + 2 * pad - HH) // stride W_out = 1 + (W + 2 * pad - WW) // stride out = np.zeros((N, F, H_out, W_out)) x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), 'constant', constant_values=0) # 对原图片的每层进行卷积 for pics in range(N): image = x_pad[pics] for filters in range(F): for H_move in range(H_out): for W_move in range(W_out): image_conv = image[:, stride * H_move: stride * H_move + HH, stride * W_move: stride * W_move + WW] filter_conv = w[filters, :] out_pixel = np.sum(image_conv * filter_conv) + b[filters] out[pics, filters, H_move, W_move] = out_pixel ########################################################################### # END OF YOUR CODE # ########################################################################### cache = (x, w, b, conv_param) return out, cache 可视化中间的图像过程 这里输入了两个不同的输入图片 分别可视化了这个图片的不同weights Convolution: Naive backward pass愉快的简单计算back的过程，先不用考虑cost input dout cache（x,w,b,conv_param) -&gt; 参数是padding 和 stride output dx dw db 实现：conv是怎么求导的？其实排除位置的改变之外，forward只进行了三个操作 把x padding为x_pad wx_pad_conv + b -&gt; 求出一个大小和filter相同的矩阵 把求出来的一个(HH,WW)的矩阵的所有值求sum backward的思路 首先，每一张图片的每一个channel的，dout的大小和输出图片的大小一样 应该是H_out = 1 + (H + 2 * pad - HH) // stride这样求出来的结果 整个dout的size是(N,F,Hout,Wout)，其中N是之前图片的数量，F是新形成的图片的channel 所以在for循环中，dout中选中[n,f,hout,wout]，就可以得到这个特点定的值，称为df df的得到方法是wx+b得到一个矩阵，然后再对这个矩阵求和 因为求和实际就是累加，求导数的时候只要把每一个格子的dx，dw，db导数求出来，然后加在一起就行了 因为公式就是wx + b，所以dx是w，dw是x，db是常数 -&gt; 然后再把每个格子求出来的加在一起，注意各个矩阵的大小，dx应该是在x矩阵里取做卷积的部分，这部分的导数等于w乘df的和 最后，因为x被padding了，dx应该去dx_pad中没有被padding的部分，也就是从[pad:pad + H] 代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def conv_backward_naive(dout, cache): """ A naive implementation of the backward pass for a convolutional layer. Inputs: - dout: Upstream derivatives. - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive Returns a tuple of: - dx: Gradient with respect to x - dw: Gradient with respect to w - db: Gradient with respect to b """ dx, dw, db = None, None, None ########################################################################### # TODO: Implement the convolutional backward pass. # ########################################################################### x, w, b, conv_param = cache N, C, H, W = x.shape F, _, HH, WW = w.shape _, _, H_dout, W_dout = dout.shape stride = conv_param['stride'] pad = conv_param['pad'] x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), 'constant', constant_values=0) db = np.zeros_like(b) dw = np.zeros_like(w) dx_pad = np.zeros_like(x_pad) # print(dx_pad.shape) for pics in range(N): for filters in range(F): for H_move in range(H_dout): for W_move in range(W_dout): # f=sum(wx_pad + b) (df is a number now) df = dout[pics, filters, H_move, W_move] # d for sum, size (HH,WW) # dsum = df * np.ones((HH, WW)) db[filters] += df dx_pad[pics, :, H_move * stride: H_move * stride + HH, W_move * stride: W_move * stride + WW] += df * w[filters] dw[filters] += x_pad[pics, :, stride * H_move: stride * H_move + HH, stride * W_move: stride * W_move + WW] * df dx = dx_pad[:, :, pad:pad + H, pad:pad + W] ########################################################################### # END OF YOUR CODE # ########################################################################### return dx, dw, db Max-Pooling：Native forwardinput x, (N,C,H,W) pool_param -&gt; dict ‘pool_height’ ‘pool_width’ ‘stride’ 不需要进行padding output out, (N,C,H’,W’) H’ = 1 + (H - pool_height) / stride W’ = 1 + (W - pool_width) / stride cache(x,pool_param) 实现 直接找到相应的块然后求max 注意求max的时候要注意axis,我们需要求得是在一张图片每个channel上面的最大值，在这个式子里面因为已经确定了pics的值，实际上的out其实是一个三维的数组，所以应该求axis = (1,2)上面的最大值，而不是求(2,3上面的) 代码12345678910111213141516171819202122232425262728293031323334353637383940def max_pool_forward_naive(x, pool_param): """ A naive implementation of the forward pass for a max-pooling layer. Inputs: - x: Input data, of shape (N, C, H, W) - pool_param: dictionary with the following keys: - 'pool_height': The height of each pooling region - 'pool_width': The width of each pooling region - 'stride': The distance between adjacent pooling regions No padding is necessary here. Output size is given by Returns a tuple of: - out: Output data, of shape (N, C, H', W') where H' and W' are given by H' = 1 + (H - pool_height) / stride W' = 1 + (W - pool_width) / stride - cache: (x, pool_param) """ out = None ########################################################################### # TODO: Implement the max-pooling forward pass # ########################################################################### N, C, H, W = x.shape pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride'] H_out = 1 + (H - pool_height) // stride W_out = 1 + (W - pool_width) // stride out = np.zeros((N,C,H_out,W_out)) for pics in range(N): for h_out in range(H_out): for w_out in range(W_out): out[pics,:,h_out,w_out] = np.max(x[pics,:,stride * h_out: stride* h_out + pool_height,stride* w_out: stride*w_out + pool_width],axis = (1,2)) ########################################################################### # END OF YOUR CODE # ########################################################################### cache = (x, pool_param) return out, cache Max-pooling: Native backwardinput dout, size = (N,C,W_out,W_out) cache output dx, size = (N,C,W,H) 实现 max的值在实际上是一个router，local gradient对于最大的值的地方是1，其他值的地方影响是0 需要找到x里面值等于最大值的坐标，然后把这个坐标的dx改成对应dout的值（因为链式法则应该dout * 1），其他地方的dx都是0 关于找到这个点的坐标 我用了显得很傻的方法，在x的范围里面找到这个范围里最大的坐标，用了很多圈循环 实际上可以从max的值找到原来的坐标 numpy.unravel_index(indices, dims) 结合np.argmax，返回最大值的坐标 -&gt; ind = np.unravel_index(np.argmax(a, axis=None), a.shape) 这样的话找到的是在每个max的框框里最大值的坐标，在这个框框的范围里找到这个坐标就是需要改变的地方 1234567891011121314151617181920212223242526272829303132333435363738def max_pool_backward_naive(dout, cache): """ A naive implementation of the backward pass for a max-pooling layer. Inputs: - dout: Upstream derivatives - cache: A tuple of (x, pool_param) as in the forward pass. Returns: - dx: Gradient with respect to x """ dx = None ########################################################################### # TODO: Implement the max-pooling backward pass # ########################################################################### x, pool_param = cache N, C, H, W = x.shape pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride'] _,_,H_out,W_out = dout.shape dx = np.zeros_like(x) for pics in range(N): for channels in range(C): for h_out in range(H_out): for w_out in range(W_out): # for H in range(stride * h_out, stride* h_out + pool_height): # for W in range(stride * w_out, stride * w_out + pool_width): # if x[pics,channels,H,W] == np.max(x[pics,channels,stride * h_out: stride* h_out + pool_height,stride* w_out: stride*w_out + pool_width]): # dx[pics,channels,H,W] = dout[pics,channels,h_out,w_out] ind = np.unravel_index(np.argmax(x[pics,channels,stride * h_out: stride* h_out + pool_height,stride* w_out: stride*w_out + pool_width]), (pool_height,pool_width)) dx[pics,channels,stride * h_out: stride* h_out + pool_height,stride* w_out: stride*w_out + pool_width][ind] = dout[pics,channels,h_out,w_out] # print(ind) ########################################################################### # END OF YOUR CODE # ########################################################################### return dx Fast layers 在cs231n/fast_layers.py里面直接提供了比较快版本的计算方法 The fast convolution implementation depends on a Cython extension; to compile it you need to run the following from the cs231n directory: 1python setup.py build_ext --inplace 记得重启一下jupter 12345678910111213141516171819202122232425Testing conv_forward_fast:Naive: 5.283360sFast: 0.014807sSpeedup: 356.809600xDifference: 4.926407851494105e-11Testing conv_backward_fast:Naive: 9.893734sFast: 0.015421sSpeedup: 641.578958xdx difference: 1.949764775345631e-11dw difference: 5.155328198575201e-13db difference: 3.481354613192702e-14Testing pool_forward_fast:Naive: 0.212025sfast: 0.002980sspeedup: 71.143680xdifference: 0.0Testing pool_backward_fast:Naive: 0.391351sfast: 0.012568sspeedup: 31.138711xdx difference: 0.0 可以发现fast版本conv的速度会快300倍，而pooling也会快几十倍 conv sandwich layer -&gt; 已经写好了，conv + relu + poolThree-layer ConvNetcs231n/classifiers/cnn.py,implement一个三层的CNN结构 conv - relu - 2x2 maxpool - affine1 - relu - affine2 - softmax 输入图片的minibatch为(N,C,H,W) init input_dim: (C,H,W)是每张图片长什么样子 num_filters: conv层里面filter的个数 filters_size，直接把高和宽统一成一个数字了，反正都是方形的 hidden_dim：用fc层的数量 num_classes: 最后输出的class的数量 weight_scale：初始化的时候的scale reg：L2 dtype：计算所用的datatype（如 np.float32) loss + gradient 需要初始化三层的参数，W123和b123 初始化weights（正态分布）和bias（全是0） -&gt; 注意fc和conv层的不一样 因为在loss中有帮助input的大小保持的操作，所以第二层的图片可以不考虑padding和stride的变化 W1的大小是filter的大小(F,C,HH,WW)，需要filter的数量，channel的数量，以及每个filter的大小，b1是(filter,) conv_relu之后进行了一次max pool，所以图片的大小缩小了一半 后面两个affine的大小就跟输入，hidden_num和最后的num_classes有关系了，b的大小跟输出走 注意第二个affine之后不需要relu loss用之前写好的softmax 注意需要regularization 直接用之前写好的把gradient back回去就可以了 Sanity check loss¶在建立一个新的net的时候，第一件事就应该是这个 用softmax的时候，我们希望random weight，没有reg的结果是log(C) 如果加上了reg，这个数量会轻微增加一点 overfit small data 直接用非常少的数据来训练一个新的网络，应该能在这个上面overfit 应该会产生一个非常高的训练精度和非常低的val精度 注意在loss里面的时候需要记录下来scores，我就是因为变量名写错了所以一直bug 最后训练出来的train_acc接近100%，而val_acc只有百分之20 1234567891011121314151617181920np.random.seed(231)num_train = 100small_data = &#123; 'X_train': data['X_train'][:num_train], 'y_train': data['y_train'][:num_train], 'X_val': data['X_val'], 'y_val': data['y_val'],&#125;model = ThreeLayerConvNet(weight_scale=1e-2)solver = Solver(model, small_data, num_epochs=15, batch_size=50, update_rule='adam', optim_config=&#123; 'learning_rate': 1e-3, &#125;, verbose=True, print_every=1)solver.train() 训练这个三层的网络 直接用所有数据训练这个网络，应该得到的train_acc应该在40% 最后训练了1个epoch，980次iter1(Epoch 1 / 1) train acc: 0.496000; val_acc: 0.489000 可视化第一层的filter Spatial Batch Normalization 在之前我们已经看到BN对于训练NN很有用了，根据15年的一个论文，CNN里面也可以用BN -&gt; SBN 普通的BN会接收(N,D)大小的input，并且output是同样的大小，normal的时候用data的总数N CNN里面，input为(N,C,H,W)，output大小相同（也就是和X同样尺寸） 如果用卷积得到的特征map，we expect the statistics of each feature channel to be relatively consistent both between different imagesand different locations within the same image -&gt; 所以在SBN里面，计算对每个C里面的特征计算mean和var Spatial batch normalization: forwardcs231n/layers.pyinput: x,(N,C,H,W) gamma,scale parameter (C,) beta,shift param (C,) bn_param: dict mode: train/test eps momentum running_mean running_varoutput out,(N,C,H,W) cache, back的时候需要的东西 注意，可以调用之前写的关于 batchnorm_forward 的内容，代码应该少于五行 这里需要用到多维数组的转置，需要把矩阵变成（N H W） * C的格式，然后在求完bn之后再转回去 之前fc里面使用的时候的大小是(N,D)，这样的话是在所有的N上面取平均 这里的C代替了以前的D，NHW代替了以前的N(把每张特征图看做一个特征处理（一个神经元），这里的特征图指的是一层的东西) 这里用到的是一张特征图里面的所有神经元的参数共享 Spatial batch normalization: backward 输入dout(N,C,H,W)和cache，输出dx，dgamma和dbeta 同样也是直接调用之前的，变形方法和之前一样 Group Normalization 同样原理，把维度变化之后使用 np.newaxis()]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231nassignment2之Dropout]]></title>
    <url>%2F2019%2F04%2F18%2FCS231nassignment2Dropout%2F</url>
    <content type="text"><![CDATA[Target regularization NN randomly setting some features to 0 during forward pass Geoffrey E. Hinton et al, “Improving neural networks by preventing co-adaptation of feature detectors”, arXiv 2012 Dropout forward + backwardin cs231n/layers.py IO input x,input data, of any shape dropout_params p，每个neuron是不是保留的可能性是p mode：’train’的时候会进行dropout，‘test’的时候会直接return input seed：用来generate random number for dropout output out, 和x同样大小 cache, tuple(dropout_params, mask). In training, mask is used to multiply the input 在实现中不推荐用vanilla的方法 123456NOTE: Please implement **inverted** dropout, not the vanilla version of dropout.See http://cs231n.github.io/neural-networks-2/#reg for more details.NOTE 2: Keep in mind that p is the probability of **keep** a neuronoutput; this might be contrary to some sources, where it is referred toas the probability of dropping a neuron output. 实现 在训练的时候在hidd层都drop了一部分，如果愿意的话也可以在input层就drop 在predict的时候不再drop了！但是需要根据drop的比例对output的数量进行scale -&gt; 所以这样就会变得很麻烦（vanilla的方法） 比如比例是p，drop之后剩下了px 那在test的时候x的大小也应该变成px(x -&gt; px) inverted dropout，在训练的时候就对大小进行放缩，在test的时候不接触forward pass 12H1 = np.maximum(0, np.dot(W1, X) + b1)U1 = (np.random.rand(*H1.shape) &lt; p) / p # /p!!! back的实现更容易了，如果这个点被drop了的话对再往前的dx就没有影响，如果这个点没有被drop的话对之前的影响就是常数 code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081def dropout_forward(x, dropout_param): """ Performs the forward pass for (inverted) dropout. Inputs: - x: Input data, of any shape - dropout_param: A dictionary with the following keys: - p: Dropout parameter. We keep each neuron output with probability p. - mode: 'test' or 'train'. If the mode is train, then perform dropout; if the mode is test, then just return the input. - seed: Seed for the random number generator. Passing seed makes this function deterministic, which is needed for gradient checking but not in real networks. Outputs: - out: Array of the same shape as x. - cache: tuple (dropout_param, mask). In training mode, mask is the dropout mask that was used to multiply the input; in test mode, mask is None. NOTE: Please implement **inverted** dropout, not the vanilla version of dropout. See http://cs231n.github.io/neural-networks-2/#reg for more details. NOTE 2: Keep in mind that p is the probability of **keep** a neuron output; this might be contrary to some sources, where it is referred to as the probability of dropping a neuron output. """ p, mode = dropout_param['p'], dropout_param['mode'] if 'seed' in dropout_param: np.random.seed(dropout_param['seed']) mask = None out = None if mode == 'train': ####################################################################### # TODO: Implement training phase forward pass for inverted dropout. # # Store the dropout mask in the mask variable. # ####################################################################### mask = (np.random.randn(*x.shape) &lt; p) / p out = x * mask ####################################################################### # END OF YOUR CODE # ####################################################################### elif mode == 'test': ####################################################################### # TODO: Implement the test phase forward pass for inverted dropout. # ####################################################################### out = x ####################################################################### # END OF YOUR CODE # ####################################################################### cache = (dropout_param, mask) out = out.astype(x.dtype, copy=False) return out, cachedef dropout_backward(dout, cache): """ Perform the backward pass for (inverted) dropout. Inputs: - dout: Upstream derivatives, of any shape - cache: (dropout_param, mask) from dropout_forward. """ dropout_param, mask = cache mode = dropout_param['mode'] dx = None if mode == 'train': ####################################################################### # TODO: Implement training phase backward pass for inverted dropout # ####################################################################### dx = dout * mask ####################################################################### # END OF YOUR CODE # ####################################################################### elif mode == 'test': dx = dout return dx FC with DP 应该在每层的relu之后，增加dropout的部分 在之前定义的function里面加上新的dropout部分，因为倔强的想加在定义好的函数里面，所以产生了一些奇怪的延伸问题 如果想要可选参数，在def function里面直接定义好就行了 如果返回值不需要，直接在返回的时候_就好了 注意在fc_net里面如果dropout = 1 的话，实际上的flag是没有意义的 1234567891011121314151617181920212223242526272829303132333435363738def affine_Normal_relu_dropout_forward(self, x, w, b, mode, gamma=None, beta=None, bn_params=None): Normal_cache = None dp_cache = None a, fc_cache = affine_forward(x, w, b) if mode == "batchnorm": mid, Normal_cache = batchnorm_forward(a, gamma, beta, bn_params) elif mode == "layernorm": mid, Normal_cache = layernorm_forward(a, gamma, beta, bn_params) else: mid = a dp, relu_cache = relu_forward(mid) if self.use_dropout: out, dp_cache = dropout_forward(dp, self.dropout_param) else: out = dp cache = (fc_cache, Normal_cache, relu_cache, dp_cache) return out, cachedef affine_Normal_relu_dropout_backward(self, dout, cache, mode): fc_cache, Normal_cache, relu_cache, dp_cache = cache dgamma = 0.0 dbeta = 0.0 if self.use_dropout: ddp = dropout_backward(dout, dp_cache) else: ddp = dout da = relu_backward(ddp, relu_cache) if mode == "batchnorm": dmid, dgamma, dbeta = batchnorm_backward_alt(da, Normal_cache) elif mode == "layernorm": dmid, dgamma, dbeta = layernorm_backward(da, Normal_cache) else: dmid = da dx, dw, db = affine_backward(dmid, fc_cache) return dx, dw, db, dgamma, dbeta regularization experiment 训练一个2层的网络，500个training，一个没有dropout，另一个0.25的dp 并且可视化了最终的结果 从结果上来看感觉，如果epoch比较少的话，dropout的效果会更好 加上dropout，normalization，的fc网络全部代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260class FullyConnectedNet(object): """ A fully-connected neural network with an arbitrary number of hidden layers, ReLU nonlinearities, and a softmax loss function. This will also implement dropout and batch/layer normalization as options. For a network with L layers, the architecture will be &#123;affine - [batch/layer norm] - relu - [dropout]&#125; x (L - 1) - affine - softmax where batch/layer normalization and dropout are optional, and the &#123;...&#125; block is repeated L - 1 times. Similar to the TwoLayerNet above, learnable parameters are stored in the self.params dictionary and will be learned using the Solver class. """ def __init__(self, hidden_dims, input_dim=3 * 32 * 32, num_classes=10, dropout=1, normalization=None, reg=0.0, weight_scale=1e-2, dtype=np.float32, seed=None): """ Initialize a new FullyConnectedNet. Inputs: - hidden_dims: A list of integers giving the size of each hidden layer. - input_dim: An integer giving the size of the input. - num_classes: An integer giving the number of classes to classify. - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=1 then the network should not use dropout at all. - normalization: What type of normalization the network should use. Valid values are "batchnorm", "layernorm", or None for no normalization (the default). - reg: Scalar giving L2 regularization strength. - weight_scale: Scalar giving the standard deviation for random initialization of the weights. - dtype: A numpy datatype object; all computations will be performed using this datatype. float32 is faster but less accurate, so you should use float64 for numeric gradient checking. - seed: If not None, then pass this random seed to the dropout layers. This will make the dropout layers deteriminstic so we can gradient check the model. """ self.normalization = normalization self.use_dropout = dropout != 1 self.reg = reg self.num_layers = 1 + len(hidden_dims) self.dtype = dtype self.params = &#123;&#125; ############################################################################ # TODO: Initialize the parameters of the network, storing all values in # # the self.params dictionary. Store weights and biases for the first layer # # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be # # initialized from a normal distribution centered at 0 with standard # # deviation equal to weight_scale. Biases should be initialized to zero. # # # # When using batch normalization, store scale and shift parameters for the # # first layer in gamma1 and beta1; for the second layer use gamma2 and # # beta2, etc. Scale parameters should be initialized to ones and shift # # parameters should be initialized to zeros. # ############################################################################ pr_num = input_dim # can't use enumerate beacuse I need the number more than the size of hidden_dims for layer in range(self.num_layers): layer += 1 weights = 'W' + str(layer) bias = 'b' + str(layer) # 这时候是最后一层(the last layer) if layer == self.num_layers: self.params[weights] = np.random.randn( hidden_dims[len(hidden_dims) - 1], num_classes) * weight_scale self.params[bias] = np.zeros(num_classes) # other layers else: hidd_num = hidden_dims[layer - 1] self.params[weights] = np.random.randn( pr_num, hidd_num) * weight_scale self.params[bias] = np.zeros(hidd_num) pr_num = hidd_num if self.normalization in ["batchnorm", "layernorm"]: self.params['gamma' + str(layer)] = np.ones(hidd_num) self.params['beta' + str(layer)] = np.zeros(hidd_num) # print(len(self.params)) # print(self.params) ############################################################################ # END OF YOUR CODE # ############################################################################ # When using dropout we need to pass a dropout_param dictionary to each # dropout layer so that the layer knows the dropout probability and the mode # (train / test). You can pass the same dropout_param to each dropout layer. self.dropout_param = &#123;&#125; if self.use_dropout: self.dropout_param = &#123;'mode': 'train', 'p': dropout&#125; if seed is not None: self.dropout_param['seed'] = seed # With batch normalization we need to keep track of running means and # variances, so we need to pass a special bn_param object to each batch # normalization layer. You should pass self.bn_params[0] to the forward pass # of the first batch normalization layer, self.bn_params[1] to the forward # pass of the second batch normalization layer, etc. self.bn_params = [] if self.normalization == 'batchnorm': self.bn_params = [&#123;'mode': 'train'&#125; for i in range(self.num_layers - 1)] if self.normalization in ["batchnorm", "layernorm"]: self.bn_params = [&#123;&#125; for i in range(self.num_layers - 1)] # Cast all parameters to the correct datatype for k, v in self.params.items(): self.params[k] = v.astype(dtype) def loss(self, X, y=None): """ Compute loss and gradient for the fully-connected net. Input / output: Same as TwoLayerNet above. """ X = X.astype(self.dtype) mode = 'test' if y is None else 'train' # Set train/test mode for batchnorm params and dropout param since they # behave differently during training and testing. if self.use_dropout: self.dropout_param['mode'] = mode if self.normalization == 'batchnorm': for bn_param in self.bn_params: bn_param['mode'] = mode scores = None ############################################################################ # TODO: Implement the forward pass for the fully-connected net, computing # # the class scores for X and storing them in the scores variable. # # # # When using dropout, you'll need to pass self.dropout_param to each # # dropout forward pass. # # # # When using batch normalization, you'll need to pass self.bn_params[0] to # # the forward pass for the first batch normalization layer, pass # # self.bn_params[1] to the forward pass for the second batch normalization # # layer, etc. # ############################################################################ cache = &#123;&#125; temp_out = X for i in range(self.num_layers): w = self.params['W' + str(i + 1)] b = self.params['b' + str(i + 1)] if i == self.num_layers - 1: scores, cache['cache' + str(i + 1)] = affine_forward(temp_out, w, b) else: if self.normalization in ["batchnorm", "layernorm"]: gamma = self.params['gamma' + str(i + 1)] beta = self.params['beta' + str(i + 1)] temp_out, cache['cache' + str(i + 1)] = self.affine_Normal_relu_dropout_forward( temp_out, w, b, self.normalization, gamma, beta, self.bn_params[i]) else: # temp_out, cache['cache' + # str(i + 1)] = affine_relu_forward(temp_out, w, b) temp_out, cache['cache' + str(i + 1)] = self.affine_Normal_relu_dropout_forward( temp_out, w, b, mode=self.normalization) ############################################################################ # END OF YOUR CODE # ############################################################################ # If test mode return early if mode == 'test': return scores loss, grads = 0.0, &#123;&#125; ############################################################################ # TODO: Implement the backward pass for the fully-connected net. Store the # # loss in the loss variable and gradients in the grads dictionary. Compute # # data loss using softmax, and make sure that grads[k] holds the gradients # # for self.params[k]. Don't forget to add L2 regularization! # # # # When using batch/layer normalization, you don't need to regularize the scale# # and shift parameters. # # # # NOTE: To ensure that your implementation matches ours and you pass the # # automated tests, make sure that your L2 regularization includes a factor # # of 0.5 to simplify the expression for the gradient. # ############################################################################ loss, dscores = softmax_loss(scores, y) reg_loss = 0.0 pre_dx = dscores # dgamma = self.params['gamma'] for i in reversed(range(self.num_layers)): i = i + 1 reg_loss = np.sum(np.square(self.params['W' + str(i)])) loss += reg_loss * 0.5 * self.reg # 最后一层 if i == self.num_layers: pre_dx, dw, db = affine_backward( pre_dx, cache['cache' + str(i)]) else: if self.normalization in ["batchnorm", "layernorm"]: pre_dx, dw, db, dgamma, dbeta = self.affine_Normal_relu_dropout_backward( pre_dx, cache['cache' + str(i)], self.normalization) grads['gamma' + str(i)] = dgamma grads['beta' + str(i)] = dbeta else: pre_dx, dw, db, _, _ = self.affine_Normal_relu_dropout_backward( pre_dx, cache['cache' + str(i)], self.normalization) dw += self.reg * self.params['W' + str(i)] db += self.reg * self.params['b' + str(i)] grads['W' + str(i)] = dw grads['b' + str(i)] = db ############################################################################ # END OF YOUR CODE # ############################################################################ return loss, grads def affine_Normal_relu_dropout_forward(self, x, w, b, mode, gamma=None, beta=None, bn_params=None): Normal_cache = None dp_cache = None a, fc_cache = affine_forward(x, w, b) if mode == "batchnorm": mid, Normal_cache = batchnorm_forward(a, gamma, beta, bn_params) elif mode == "layernorm": mid, Normal_cache = layernorm_forward(a, gamma, beta, bn_params) else: mid = a dp, relu_cache = relu_forward(mid) if self.use_dropout: out, dp_cache = dropout_forward(dp, self.dropout_param) else: out = dp cache = (fc_cache, Normal_cache, relu_cache, dp_cache) return out, cache def affine_Normal_relu_dropout_backward(self, dout, cache, mode): fc_cache, Normal_cache, relu_cache, dp_cache = cache dgamma = 0.0 dbeta = 0.0 if self.use_dropout: ddp = dropout_backward(dout, dp_cache) else: ddp = dout da = relu_backward(ddp, relu_cache) if mode == "batchnorm": dmid, dgamma, dbeta = batchnorm_backward_alt(da, Normal_cache) elif mode == "layernorm": dmid, dgamma, dbeta = layernorm_backward(da, Normal_cache) else: dmid = da dx, dw, db = affine_backward(dmid, fc_cache) return dx, dw, db, dgamma, dbeta]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>Drop out</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenGL笔记]]></title>
    <url>%2F2019%2F04%2F16%2FOpenGL%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Learn OpenGl on Modern OpenGL. -&gt; 从graphics的programming开始讲的 Getting StartOPENGL 是一个进行图像处理的工具 可以被认为是API，但是实际上是specification 明确说明了每个function应该的输入和输出，以及如何perform 用户在用这个说明来解决问题，因为没有给出明确的implement的过程，所以只要结果符合规则，怎么implement都可以 Core-profile vs Immediate mode 以前的版本用的是immediate mode 比较好用来画图 具体的是实现都在lib里面，developer不是很好的能看到如何计算 效率越来越低 Core-profile 在3.2版本之后改成了这个 强制使用modern practices，如果想要用被分出去的function就会直接报错 效率高，更灵活，更难学 extensions 支持extensions，只要检查支不支持graphic card就可以知道能不能用 可以直接用比较新的东西，不用等着OPENGL更新新的功能 需要在用之前判断他是不是available的，如果不是需要用原来的方法搞 State Machine OpenGL自己就是一个State Machine：一个var的集合，来判断他现在应该如何操作 state -&gt; context 改变state：设定一些options，操作一些buffer，在现在的context来render 例子： 如果我想画三角形，而不是画线了，就改变draw的state 只要这个改变传达到了，下一条线就画的是三角形了 state-changing用来改变context，state-using在现在的state上面开始进行操作 Objects 一个集合来表现OpenGL的subset的state 比如可以用一个object来表示对window的设定，可以设置大小，设置支持的颜色等等123456// The State of OpenGLstruct OpenGL_Context &#123; ... object_name* object_Window_Target; ... &#125;; 12345678910// create objectunsigned int objectId = 0;glGenObject(1, &amp;objectId);// bind object to contextglBindObject(GL_WINDOW_TARGET, objectId);// set options of object currently bound to GL_WINDOW_TARGETglSetObjectOption(GL_WINDOW_TARGET, GL_OPTION_WINDOW_WIDTH, 800);glSetObjectOption(GL_WINDOW_TARGET, GL_OPTION_WINDOW_HEIGHT, 600);// set context target back to defaultglBindObject(GL_WINDOW_TARGET, 0); 流程 首先创建了一个object，里面存了一个ref是这个object的id 然后把这个object和context的目标位置bind在了一起 设置了这些window的参数 最后un-bind这两个东西，把window target改回原来的值 这样的话我们可以创建很多object，提前设置好里面的量，等到需要用的时候就直接bind就可以用了 比如我们有一堆object包含了小人，小马，小鹿 想画哪个就把哪个绑定到draw里面，就可以直接画出来了 Crateing a window因为操作系统的问题，所有操作系统上面不是很一样。但是已经有一些提供这些功能的函数了，这里用的是GLFW GLFW 一个lib，用C写的，主要目的是提供把东西渲染到屏幕的功能 可以创建一个context，定义窗口的params，处理用户的输入 已经一口气配置好了这些！https://www.jianshu.com/p/25d5fbf792a2 GLAD 因为openGL还需要不同版本的driver的支持，需要有东西来处理这部分的内容 和其他的东西不同，GLAD用的是web service 在这个网页上选择好语言，版本号，确保profile是core，然后生成 直接下载下来对应的zip，然后把include放进include里面，.c文件放在project里面 莫名其妙并不需要这一步，神奇，可能是我在include里面已经搞进来了！！ Hello Window初始化12345678910int main()&#123; glfwInit(); glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3); glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3); glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE); //glfwWindowHint(GLFW_OPENGL_FORWARD_COMPAT, GL_TRUE); return 0;&#125; 首先进行了初始化 然后configure了GLFW，设置了a large enum of possible options prefixed with GLFW_. （第三行就是最小） -&gt; 大概是设置要用GLFW的版本号 然后也告诉了他想用core]]></content>
      <categories>
        <category>OpenGl</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment2之Batch Normalization]]></title>
    <url>%2F2019%2F04%2F15%2FCS231Nassignment2BN%2F</url>
    <content type="text"><![CDATA[target 之前的内容讲了lr的优化方法，比如Adam，另一种方法是根据改变网络的结构，make it easy to train -&gt; batch normalization 想去掉一些uncorrelated features(不相关的特征)，可以在训练数据之前preprocess，变成0-centered分布，这样第一层是没有问题的，但是后面的层里还是会出问题 所以把normalization的部分加入了DN里面，加入了一个BN层，会估计mean和standard deviation of each feature，这样重新centre和normalized learnable shift and scale parameters for each feature dimension 核心思想：粗暴的用BN来解决weights初始化的问题 ref：https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html Batch normalization: forward这个东西的要义就是NN里面的一层，不对维度改变，但是会改变这些值的分布 首先setup，并且载入好了preprocess的数据cs231n/layers.py -&gt; batchnorm_forward keep exp decay 来运行mean &amp; variance of each feature -&gt; 在test的时候去normalize data test-time: 计算sample mean和varience的时候用大量的训练数据而不是用所有图片的平均值，但是在作业里面用的是平均值，因为可以省去一步estimate（torch7 也用的是平均值） 12running_mean = momentum * running_mean + (1 - momentum) * sample_meanrunning_var = momentum * running_var + (1 - momentum) * sample_var I/O input x，data(N,D) gamma：scale parameter(D,) beta：shift parameter(D,) bn_param: 一个dict mode：‘train’ or ‘test’ eps：为了数字上的稳定性的一个常数 momentum：在计算mean和variance上面的一个常数 running mean：(D,)，是running mean running var：(D,) output out：(N,D) cache:在back的时候用 todo 用minibatch的统计来计算mean和variance，用这两个值把data normalize，并且用gamma和beta拉伸这个值，以及shift这些值的位置 在分布的上面，虽然求得是running variance，但是需要normalize的时候考虑的是standard（也就是平方根） implement 其实是和如何计算息息相关的，知道输入，求这个玩意的normal的步骤如下（其中的x就是这个minibatch的全部数据） 求mu，也就是x的mean（注意这里要对列求mean，也就是把所有图片的像素均匀分布，最后得到的结果是D个不是N个） 求var，知道这个东西，可以直接用 np.var(x, axis = 0)来求方差 求normalize： x - x.mean / np.sqrt(x.var + eps) 其中刚开始求出来的var就是方差，也就是标准差的平方 eps是偏差值，这个值加上方差开方是标准差 scale和shift，乘scale的系数，加shift的系数 最后需要计算什么cache和back的推导息息相关 Batch normalization: backward 可以直接画出来计算normal的路径，然后根据这个路径back 要义就是一步一步的求导！一步一步的链式法则 注意的就是求mean回来的导数，理解上来说就是这个矩阵在求导的过程中升维了，从(D,)变成了(N,D)，而在最开始求得时候所有的数字的贡献都是1，所以往回走的时候乘一个（N，D）的全是1的矩阵，并且1/N的常数还在 Batch normalization: alternative backward 在sigmoid的back的过程中有两种不同的方法 一种是写出来整体计算的图（拆分成各种小的计算），然后根据这张图的再back回去 另一种是在纸上先简化了整体的计算过程，然后再直接实现，这样代码会比较简单 ref:https://kevinzakka.github.io/2016/09/14/batch_normalization/ 最终目标 f: BN之后的整体输出结果 y：对normal之后的线性变换（gamma + beta） x’：normal的input mu：batch mean varbatch vatiance 需要求 df/dx,df/dgamma,df/dbeta -&gt; 最终结果整体速度比以前快了x2.5左右，这一步的主要目的就是用来提速的 可以把整体的计算分为以下的三个步骤 这三部分的代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208def batchnorm_forward(x, gamma, beta, bn_param): """ Forward pass for batch normalization. During training the sample mean and (uncorrected) sample variance are computed from minibatch statistics and used to normalize the incoming data. During training we also keep an exponentially decaying running mean of the mean and variance of each feature, and these averages are used to normalize data at test-time. At each timestep we update the running averages for mean and variance using an exponential decay based on the momentum parameter: running_mean = momentum * running_mean + (1 - momentum) * sample_mean running_var = momentum * running_var + (1 - momentum) * sample_var Note that the batch normalization paper suggests a different test-time behavior: they compute sample mean and variance for each feature using a large number of training images rather than using a running average. For this implementation we have chosen to use running averages instead since they do not require an additional estimation step; the torch7 implementation of batch normalization also uses running averages. Input: - x: Data of shape (N, D) - gamma: Scale parameter of shape (D,) - beta: Shift paremeter of shape (D,) - bn_param: Dictionary with the following keys: - mode: 'train' or 'test'; required - eps: Constant for numeric stability - momentum: Constant for running mean / variance. - running_mean: Array of shape (D,) giving running mean of features - running_var Array of shape (D,) giving running variance of features Returns a tuple of: - out: of shape (N, D) - cache: A tuple of values needed in the backward pass """ mode = bn_param['mode'] eps = bn_param.get('eps', 1e-5) momentum = bn_param.get('momentum', 0.9) N, D = x.shape running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype)) running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype)) out, cache = None, None if mode == 'train': ####################################################################### # TODO: Implement the training-time forward pass for batch norm. # # Use minibatch statistics to compute the mean and variance, use # # these statistics to normalize the incoming data, and scale and # # shift the normalized data using gamma and beta. # # # # You should store the output in the variable out. Any intermediates # # that you need for the backward pass should be stored in the cache # # variable. # # # # You should also use your computed sample mean and variance together # # with the momentum variable to update the running mean and running # # variance, storing your result in the running_mean and running_var # # variables. # # # # Note that though you should be keeping track of the running # # variance, you should normalize the data based on the standard # # deviation (square root of variance) instead! # # Referencing the original paper (https://arxiv.org/abs/1502.03167) # # might prove to be helpful. # ####################################################################### mean = np.mean(x, axis=0) xmu = x - mean sq = np.square(xmu) var = np.var(x, axis=0) sqrtvar = np.sqrt(var + eps) ivar = 1. / sqrtvar normalize_raw = xmu * ivar normalize_result = gamma * normalize_raw + beta out = normalize_result running_mean = momentum * running_mean + \ (1 - momentum) * mean running_var = momentum * running_var + (1 - momentum) * var cache = (normalize_raw, gamma, xmu, ivar, sqrtvar, var, eps) ####################################################################### # END OF YOUR CODE # ####################################################################### elif mode == 'test': ####################################################################### # TODO: Implement the test-time forward pass for batch normalization. # # Use the running mean and variance to normalize the incoming data, # # then scale and shift the normalized data using gamma and beta. # # Store the result in the out variable. # ####################################################################### x_normalize = (x - running_mean) / (np.sqrt(running_var + eps)) out = x_normalize * gamma + beta ####################################################################### # END OF YOUR CODE # ####################################################################### else: raise ValueError('Invalid forward batchnorm mode "%s"' % mode) # Store the updated running means back into bn_param bn_param['running_mean'] = running_mean bn_param['running_var'] = running_var return out, cachedef batchnorm_backward(dout, cache): """ Backward pass for batch normalization. For this implementation, you should write out a computation graph for batch normalization on paper and propagate gradients backward through intermediate nodes. Inputs: - dout: Upstream derivatives, of shape (N, D) - cache: Variable of intermediates from batchnorm_forward. Returns a tuple of: - dx: Gradient with respect to inputs x, of shape (N, D) - dgamma: Gradient with respect to scale parameter gamma, of shape (D,) - dbeta: Gradient with respect to shift parameter beta, of shape (D,) """ dx, dgamma, dbeta = None, None, None ########################################################################### # TODO: Implement the backward pass for batch normalization. Store the # # results in the dx, dgamma, and dbeta variables. # # Referencing the original paper (https://arxiv.org/abs/1502.03167) # # might prove to be helpful. # ########################################################################### normalize_raw, gamma, xmu, ivar, sqrtvar, var, eps = cache N, D = dout.shape dbeta = np.sum(dout, axis=0) dgammax = dout dgamma = np.sum(dgammax * normalize_raw, axis=0) dnormalize_raw = dgammax * gamma divar = np.sum(dnormalize_raw * xmu, axis=0) dxmu = dnormalize_raw * ivar dsqrtvar = -1. / (sqrtvar ** 2) * divar dvar = 0.5 * 1. / np.sqrt(var + eps) * dsqrtvar dsq = 1. / N * np.ones((N, D)) * dvar dxmu2 = 2 * xmu * dsq dx1 = (dxmu + dxmu2) dmu = -1 * np.sum(dxmu + dxmu2, axis=0) dx2 = 1. / N * np.ones((N, D)) * dmu dx = dx1 + dx2 ########################################################################### # END OF YOUR CODE # ########################################################################### return dx, dgamma, dbetadef batchnorm_backward_alt(dout, cache): """ Alternative backward pass for batch normalization. For this implementation you should work out the derivatives for the batch normalizaton backward pass on paper and simplify as much as possible. You should be able to derive a simple expression for the backward pass. See the jupyter notebook for more hints. Note: This implementation should expect to receive the same cache variable as batchnorm_backward, but might not use all of the values in the cache. Inputs / outputs: Same as batchnorm_backward """ dx, dgamma, dbeta = None, None, None ########################################################################### # TODO: Implement the backward pass for batch normalization. Store the # # results in the dx, dgamma, and dbeta variables. # # # # After computing the gradient with respect to the centered inputs, you # # should be able to compute gradients with respect to the inputs in a # # single statement; our implementation fits on a single 80-character line.# ########################################################################### normalize_raw, gamma, xmu, ivar, sqrtvar, var, eps = cache N, D = dout.shape dbeta = np.sum(dout, axis=0) dgamma = np.sum(dout * normalize_raw, axis=0) # intermediate partial derivatives dxhat = dout * gamma # final partial derivatives dx = (1. / N) * ivar * (N * dxhat - np.sum(dxhat, axis=0) - normalize_raw * np.sum(dxhat * normalize_raw, axis=0)) ########################################################################### # END OF YOUR CODE # ########################################################################### return dx, dgamma, dbeta Fully Connected Nets with Batch Normalizationin cs231n/classifiers/fc_net.py, add the BN layers into the net. 应该在每个relu之前加上BN，所以在这里不能直接用之前的affine，relu的过程，因为中间又插了一个新的BN层，所以要写一个新的function 最后一层之后的输出不应该BN(应该是涉及到循环的问题) 实现中遇到的问题 self.bn_params的参数类型不是dict而是list，代表的是所有层里面的参数的所有和，当进入到每层的时候具体对应的才是这里的dict 当把affine_BN_relu结合在一起的时候，注意最后一层输出的地方没有BN，所以没有他的cache，需要分开讨论，不然cache的数量不对 注意这个fc_net的class因为需要实现多种不同的功能，所以对于是不是BN要加上条件判断 确实非常像搭乐高了！！ 这里主要，写到这才发现最后一层的时候好像是不需要relu也不需要batchnorm 定义好的函数块123456789101112131415def affine_BN_relu_forward(self, x, w, b, gamma, beta, bn_params): a, fc_cache = affine_forward(x, w, b) mid, BN_cache = batchnorm_forward(a, gamma, beta, bn_params) out, relu_cache = relu_forward(mid) cache = (fc_cache, BN_cache, relu_cache) return out, cache def affine_BN_relu_backward(self, dout, cache): fc_cache, BN_cache, relu_cache = cache da = relu_backward(dout, relu_cache) dmin, dgamma, dbeta = batchnorm_backward_alt(da, BN_cache) dx, dw, db = affine_backward(dmin, fc_cache) return dx, dw, db, dgamma, dbeta 结论 可视化之后可以发现加了norm的话好像会下降的快一点 Batch normalization and initialization 进行试验，了解BN和weight initialization的关系 训练一个八层的网络，包括和不包括BN，用不同的weight initialization plot出来train acc, val_acc,train_loss和weight initialization的关系 BN的作用从图中可以看出来，有了BN以后，weight init对最终结果的影响明显会降低： weight的初始化对最终结果影响很严重，比如如果全是0的话，得到的所有neuron的功能都是一样的 BN其实就是在实际中解决weight init的办法，这样可以减少初始化参数的影响 核心思想就是如果你需要更好的分布，你就加一层让他变成更好的分布 在计算的过程中越乘越小（或者越大），所以计算出来的结果越来越接近0 所以这时候如果把一些input重新分布了，就会减少这个接近0的可能性 Batch normalization and batch size 试验验证BN和batch size的关系 训练6-layer的网络，分别with和without BN，使用不同的batch size By increasing batch size your steps can be more accurate because your sampling will be closer to the real population. If you increase the size of batch, your batch normalisation can have better results. The reason is exactly like the input layer. The samples will be closer to the population for inner activations. Layer Normalization（LN） 前面的所有的BN已经可以让Net更好的被训练了，但是BN的大小和batch的大小有关，所以在实际应用的时候会受到一些限制 在复杂的网络里面的时候，batch_size是被硬件机能限制的 每个minibatch的数据分布可能会比较接近，所以训练之前要shuffle，否则结果会差很多 其中一种解决的方法就是layer normalization 不是在batch上面normal 在layer上面normal each feature vector corresponding to a single datapoint is normalized based on the sum of all terms within that feature vector. Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. “Layer Normalization.” stat 1050 (2016): 21. LN 综合一层的所有维度的输入，计算该层的平均输入和平均方差 然后用同一个规范化操作转换各个维度的输入 相当于以前我们希望可以正则到这个minibatch里面的大家都差不多，现在我们不管batch了，而是调整到一张图片里面的所有数据都是normal的 implementcs231n/layers.py -&gt; layernorm_backward forward + back input x, (N,D) gamma, scale beta,shift ln_params: eps output output,(N,D) cache 实现方法 -&gt; 实际上就是从对一列的操作变成了对一行的操作 比如之前对x取mean就是求每列的mean，现在变成了取每行的mean 在所有normal之后并且scale之前，把这个矩阵在tranpose回来 back 把需要参与计算的东西都tranpose 然后把计算完的dx tranpose回来 fc_nets在fc_nets里面稍加改动，在normalization里面增加BN_NORM和Layer_NORM的选项就可以了，整体改动不大123456789101112131415161718192021def affine_Normal_relu_forward(self, x, w, b, gamma, beta, bn_params, mode): a, fc_cache = affine_forward(x, w, b) if mode == "batchnorm": mid, Normal_cache = batchnorm_forward(a, gamma, beta, bn_params) elif mode == "layernorm": mid, Normal_cache = layernorm_forward(a, gamma, beta, bn_params) out, relu_cache = relu_forward(mid) cache = (fc_cache, Normal_cache, relu_cache) return out, cache def affine_Normal_relu_backward(self, dout, cache, mode): fc_cache, Normal_cache, relu_cache = cache da = relu_backward(dout, relu_cache) if mode == "batchnorm": dmid, dgamma, dbeta = batchnorm_backward_alt(da, Normal_cache) elif mode == "layernorm": dmid, dgamma, dbeta = layernorm_backward(da, Normal_cache) dx, dw, db = affine_backward(dmid, fc_cache) return dx, dw, db, dgamma, dbeta 可以从图像看出来，layernorm中，batchsize的影响变小了]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>Batch Normalization</tag>
        <tag>Layer Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CppPrimer笔记]]></title>
    <url>%2F2019%2F04%2F15%2FCppPrimer%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第一部分 Basics C++在编译的时候就会检查类型 allow programmers to define types that include operations as well as data 第二章 varibles and basic type2.1 bulid-in基础类型包括算数类型（arithmtic type）和void类型，void用于没有返回值的函数 2.1.1 算数类型include：intergal（bool&amp; char）/ float 不同的长度的叫法不同： char -&gt; wchar_t, char16_t, char32_t(后面两个for unicode，自然语言里面) int -&gt; short, long, long long float -&gt; double, long double 扩展：在储存信息方面，储存在每个byte里面，是最小的计量单位（8bits）。在内存中，每个byte拥有一个address，需要知道地址和类型才知道到底存储的是什么东西。 signed &amp; unsigned（除了bool）： signed：包括0的正数和负数 unsigned：大于等于零 没有写unsigned的话就是signed的 对于char来说，signed，unsigined和char三种 char到底算不算unsigned显示取决于编译器 unsigned从0-255 signed一般是-128 ～ 127 关于到底怎么用： 当确定不可能是负值的时候用unisgned short太短了，long太长了，平常用int，int不够用long long 如果用char，为了考虑到不同编译的结果不同，确定好用signed还是unsigned 用double，float的精度不够，long double小号内存 2.1.2 类型转换（conversions）在把一个东西加到另一个的过程中，支持自动的类型转换 当bool被赋值为非0时，为true，0时为false 给int赋值double，自动变成整数；给doubl赋值int，变成整数后面.0 当unsigned超出界限时，the result is the remainder of the value modulo the number of values the target type can hold. 当signed超出界限的时候，结果是undefined。不知道会发生什么事。 在写代码的时候尽量避免没有定义的，或者在implement中才定义的行为，因为这样容易导致在这个电脑上行得通但是换个地方没准就行不通了 一些引起的问题： int和unsigned相加，会引起wrap around 两个unsigned相减如果小于零会出问题 在for循环的条件里，unsiged作为变量的话永远不会小于零 求求你了反正不要混着用 2.1.3 literals每个literal都有一个type，这是由form和value决定的。 整数类型的 0开头的整数是八进制，0x是16进制 负号不是literal的一部分，比如 -42，42是literal，负号是operator float类型的 十进制带小数点的数字 用exponent，E或者e，e后面的东西就是十的多少次方 character string的类型是一个char的array，complier会在每个string 后面加上‘\0’（null character） escape -&gt; 一些带有奇怪意义的\n，\t,\’等等 如果在\后面跟着多于三个数字，只会读取前三个 \x会读取跟在她后面的所有hex digits 单独改变一个literal，在数字后面加后缀suffix U：unsigned type L：long ULL：unsigned long long 但是如果你要给1024后面加个f就不行，因为1024是整形（这时候又开始怀念python） bool 2.2 变量 Variables变量提供的是：命名好的存储空间，程序可以对他执行，每个变量都有type（其实也就是class或者object吗） 2.2.1 变量的定义 assignment(赋值)和initialize(初始化)在c++里面是不一样的，初始化是在创建的时候赋予的值，赋值是之后改变这个变量的值 list的初始化：尖括号。 在使用bulit-in的时候，可能无法list初始化这个变量，因为会丢失一些信息。比如把一个double扔到int里面 是否可以不初始化就使用取决于class的定义。比如要是int没有初始化的时候是0，string没有初始化的时候是空字符串 2.2.2 变量declaration（声明？）和定义 在分开编写代码的时候，需要知道调用的函数从哪里来。注意：不初始化变量容易出问题，建议初始化每个bulit-in declaration：让程序知道函数的名字 在前面加上extern，就可以declare但是不define 但是如果已经初始化了函数，就不能加extern了，会引起错误 在其他函数的地方调用的时候（use a varible in multiple files），不需要再define了，但是需要声明 defination：创建相应的实体 除了干declaration的事情，他还分配内存，或者提供初始值 变量被define一次，但是可以被declaration无数次。 2.2.3 identifiers（定义的名字） 要求 数字，字母，下划线underscore 对大小写有区分 不能使用C++的关键词 不能含有两个相连的下划线 2.2.4 名字的scope 使用的意义：同一个名字可能会在程序的其他地方被使用，所以要用scope确定这个名字在哪个范围里面有意义（不是namespace啊啊啊啊啊竟然是大括号我震惊） nested scope 在外层被定义的名字可以在内层被重新定义 温情建议：局部变量和全局变量不要使用一个名字 2.3 compound types（有范围的类型？）就是定义在其他类型之上的类型。在c++里面有两个，pointer和reference 2.3.1 reference（lvalue reference）在创建的时候，copy的不是对象的值，而是把refer和值绑在了一起，在创建之后不能再和别的东西绑在一起。reference必须初始化。 不是对象，是一个已经存在的对象的另一个名字 给refer赋值的时候，实际上是赋值给refer所绑定的对象 当给一个refer赋值另一个refer的时候，其实是绑到了同一个对象（但是不应该这么定义） 定义 在refer的名字之前加上&amp;，但是在后续使用的时候可以不带了 refer只能初始化成一个对象，不能是一个具体的值 类型要正确 2.3.2 pointer和refer不同，指针是一个object，他们可以被assign或者copy，在定义的时候不必须初始化，一个指针可以指向不同的东西。在定义的时候用 * 来表示 取址 pointer可以得到另一个对象的address。&amp;也可以作为取址符号得到一个对象的地址（和refer不一样！！） 类型必须匹配 pointer的值（可以是以下四个之一） 指向一个对象 指向一个在对象中末尾的位置（没有使用的实际意义） null指针，表示还没有和其他的绑定 无效的？？如果是无效的话是不能访问的 访问对象 当一个pointer指向对象的时候，使用dereference（ ）来得到她的值。（pointer p是一个地址， p是他这个地址上的值 空指针NULL 使用nullptr定义 assign一个int变量给pointer是非法的，即使这个数是0（赋值的时候给的是变量的地址，带&amp;的） 真诚建议：初始化所有的pointer，没有初始化的很难分辨出来到底这个地址是有效还是无效 assignment 写成 pi = &amp;val的时候，改变的是pi的值，他指向了val 写成 * pi = 0 的时候，改变的是val的值，val变成了0 void* Pointers void* 是一个很牛逼的指针，可以hold所有对象的地址 作用：可以传到函数或者作为函数的返回值，可以和其他指针比较，可以赋值给另一个void* 指针，但是不能操控对象的地址 2.3.3 理解 定义多个变量 虽然在定义指针的时候可以加空格，但是 int* p1，p2之中，p1是指针，p2是int pointer到pointer 写成一串星号可以表示从pointer到pointer refer到pointer &amp;r可以定义成一个pointer（指针写在=右边） 2.4 修饰词 const 作用：希望定义一个variable，value不可以被改变，这时候就用上了const。在创建的时候必须初始化 在实际操作的时候，到底是不是const对数值没有影响，可以用非const来初始化const或者用const来初始化其他的 在创建之后，编译的时候所有的变量名都会换成变量的值 const对每个file来说是local的 如果希望定义一个在所有file里面都可以用的，然后在其他使用的时候声明，这时候用extern const（在定义和后续声明的时候都需要使用） 2.4.1 refer to const 可以refer到一个const，但是之后就不能通过refer来改变变量的值，也不能把一个const的变量赋值给一个非const的refer const refer的意思是这个refer不能被用来改变变量，但是被绑的变量本身可以改变。比如const int &amp;r2 = i，这时候改变i是合法的 2.4.2 pointer and const pointer to const不能被用于改变指向的东西 但是pointer是const的和变量自己改不改没关系。变量是const的话指针必须是const的 const pointer 指针本身就是一个对象，所以指针自己也可以是const的 必须被初始化，一旦初始化了，他的内容（所指向的地址）就不能改变了。 定义的时候用 int * const cpr = &amp;num （const的位置改变了） 但是可以用const pointer来改变所指向东西的值！！！只是这两个东西绑定了不能改了而已 2.4.3 top-level 可以分开考虑pointer和对象 top-level：pointer自己是一个const -&gt; 本身就是const的，可以出现在任何的对象里面 low-level：指向一个const -&gt; 只出现在refer和pointer里面 当copy一个对象的时候，top-level是被忽略的常量指针就是一个常量，不能把常量给普通但是可以把普通给常量 2.4.4 constexpr 常量表达式是编译的时候不能改变的，const object或者literal都是常量表达式。只有在初始化的时候知道了的值才是常量表达式。如果是个const int但是不确定到底是什么，那么就还不算 在前面加上 constexpr，这时候只有当后面的变量是常量的时候才能用 可以在compile的时候判定 当这个类型不是literal的时候，不能定义成常量表达式（literal包括算数，pointer和refer） 在函数内定义的变量一般不会储存在固定的地址，所以这时候指针不能是constexpr（6.1.1） 当使用constexpr的时候，作用在的是指针上而不是指向的东西上1const int *p = nullptr; // p is a pointer to a const int constexpr int *q = nullptr; // q is a const pointer to int 2.5 types 类型2.5.1 type aliases 即为对另一个type的化名 -&gt; 简化比较复杂的type，更好使用 定义方法1： 12typedef double wages; // wages is a synonym for doubletypedef wages base, *p; // base is a synonym for double, p for double* 方法2: using SI = Sales_item； 和pointer以及const -&gt; 用的时候直接替代会出问题123typedef char *pstring;const pstring cstr = 0; // cstr is a constant pointer to charconst pstring *ps; // ps is a pointer to a constant pointer to char 2.5.2 auto 作用：有的时候没法定义变量的type，这时候可以用auto，编译的时候会自动指出变量的类型（从初始化的结果推断出来的） 写成一行定义的时候，auto不能包括不同的类型（如int和double） 指针refer，const和auto 当用auto然后用一个refer初始化的时候，得到的结果是refer绑定的object 如果需要auto之后的结果还是const的，需要在auto前面加上const1234const int ci = i, &amp;cr = ci;auto b = ci; // b is an int (top-level const in ci is dropped)auto c = cr; // c is an int (cr is an alias for ci whose const is top-level) autod=&amp;i; // d isan int*(&amp; ofan int objectis int*)auto e = &amp;ci; // e is const int*(&amp; of a const object is low-level const) 2.5.3 decltype 作用：从expr里面推断出来type，但是不用这个expr来初始化的时候。这时候用decltype(fun())，这时候fun用来判断变量的类型，但是不call 如果是必须初始化的东西（比如pointer或者refer）必须初始化 decltype(* p) is int&amp;, not plain int decltype((variable))肯定是一个refer，但是decltype(variable)只有当variable是refer的时候才是 第三章 string，vector，array第二章说的是c++里面的built-in类型，除此之外还有很多library的类型（标准库），定义了很多高于计算机内部直接访问的数字和字母的类型。 3.1 namespace声明 每次都声明函数的namespace比较麻烦，可以在所有的开始之前用using namespace :: name来声明使用的特定的函数的namespace。或者直接用他代表所有的。 头文件里面不应该用using，因为include的时候就加到所有的东西里面了，那就没有意义了 3.2 stringstring定义在std的namespace里面 3.2.1 定义和初始化 一个不知道的初始化方法： string s(n,’b’)，输出结果是n个b 使用s(“hiya”)和s=”hiya”一个是direct的初始化方法，另一个是copy的初始化方法。比较容易读的方法是创建： string s = string(10,’b’) 3.2.2 string的操作 string的读和写，cout和cin（iostream库） 当键盘有输入的时候，while(cin &gt;&gt; word)这种感觉的东西当条件，cin是会识别空格然后分开的！！！ getline()可以读取一整行，存在第二个参数里面，并且帮忙跳到新的一行 .empty()和.size()可以判定是否为空，以及string里面的char的数量 string:: size_type size的返回值的类型是size_type 注意：因为返回值类型不同，所以当比较size的时候，如果和一个int的负数比较，int会被转换成unisgned的一个巨大的数字，从而导致比较的失败 -&gt; 所以在使用size的语句里面，不用int比较好（亲身证明确实如此，换成double就没事了） 比较字符串 == 或者 ！= 来比较两个是否相等，需要是相同的长度且包括相同的字母 比较两个的大小时 如果长度不同，如果短的每个的字母都和长的相同，那短的比较小 如果任何一位上面的char不同，那就是第一个不同的char比较的结果 add 字符串可以直接相加（指s1+s2） 可以把string和literal混着加，但是两个带引号的不能连在一起直接加（这是什么脑残规则） 练习 可以直接通过索引vector的方法索引string里面的char（但是一个词一个词读取直接用cin&gt;&gt; s也是可以的（我是傻逼吗）） 二择的判断条件可以写成 ((str1.size() &gt; str2.size()) ? str1 : str2) 3.2.3 string里面的chars 有时候需要处理每一个字母，有的时候需要处理特殊的一些字母，定义在函数cctype里面]]></content>
      <categories>
        <category>编程语言</category>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于numpy里面random.rand和randn的区别]]></title>
    <url>%2F2019%2F04%2F11%2F%E5%85%B3%E4%BA%8Erandomrand%E5%92%8Crandn%2F</url>
    <content type="text"><![CDATA[python里面常用的两个产生随机数的函数，两个不太一样 其中 np.random.rand()是用来产生0-1之间的随机数的，这个最近应用最多的地方是在产生一个从a-b范围里面的数字，这时候可以先产生一个巨大的随机0-1的矩阵，然后再乘以a和b之间的差 np.random.randn()产生的是随机正态分布的标准值，外面可以乘上std就是需要的正态分布，这样可以用来初始化深度网络的weights，括号里填的都是生成的东西的维度 另外一个问题，randn的参数需要的是inter，所以在输入的时候要不是选择 np.random.randn(x0.shape[0], x0.shape[1]) np.random.randn(*x0.shape)]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>random</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment2之FCnet]]></title>
    <url>%2F2019%2F04%2F11%2FCS231Nassignment2FCnet%2F</url>
    <content type="text"><![CDATA[This part is from the assignment 2018:stanford cs231n assignment2 目标 之前已经实现了两层的fc net，但是在这个网络里面的loss和gradient的计算用的是数学方法 这样的计算可以在两层的网络里实现，但是多层的情况下实现起来太困难了 所以在这里把电脑分成了forward pass和backward pass forward的过程中，接受所有的input，weights，和其他的参数，返回output和cache（存储back的时候需要的东西） 12345678910def layer_forward(x, w): """ Receive inputs x and weights w """ # Do some computations ... z = # ... some intermediate value # Do some more computations ... out = # the output cache = (x, w, z, out) # Values we need to compute gradients return out, cache back的时候会接受derivative和之前存储的cache，然后计算最后的gradient 12345678910111213def layer_backward(dout, cache): """ Receive dout (derivative of loss with respect to outputs) and cache, and compute derivative with respect to inputs. """ # Unpack cache values x, w, z, out = cache # Use values in cache to compute derivatives dx = # Derivative of loss with respect to x dw = # Derivative of loss with respect to w return dx, dw 这样就可以组合各个部分达到最终需要的效果了，无论多深都可以实现了 还需要一部分的优化部分，包括Dropout，Batch/Layer的Normalization Affine layer：forwardinput x：大小（N，d_1…d_k)，minibatch of N，每张图片的维度是d_1到d_k，所以拉成一长条的维度是 d_1 d_2… d_k w：weights，(D,M)，把这个长度是d的图片，输出的时候就变成M了 b:bias,(M,) -&gt; 这个bias会被broadcast到all lines （bias的值是最终分类的class的值，在不是最后一层的时候就是output的值），相当于一个class分一个bias（一列） output output,(N,M) cache:(x,w,b) implement 这里的实现直接reshape就可以了，-1的意思是这个维度上不知道有多少反正你自己给我算算的意思，但是需要N行是确定了的 注意这里验证的时候虽然input的是size，但是实际上是把数字填到这个里面的，所以取N的时候实际上是x.shape[0] Affine layer:backwardinput dout: upstream derivative, shape(N,M) cache: Tuple x w b return dx: (N,d1,d2…,dk) dw:(D,M) db:(M,) implement 注意这里用到的是链式法则：df/dx = df/dq * dq/dx 这里的df/dq就是已经求出来的dout q的式子是 Wx + b，对这三个变量分别求导，求出来大家的，别忘了求导之后的东西需要再乘dout 结果到底怎么算应该按每个矩阵的shape来推出来 ReLU activationforward input：x，随便什么尺寸都可以，这部分只是计算relu这个函数 output out，计算出来的结果 cache，储存x，用来back的运算 implement -&gt; 直接把小于0的部分设置成0就可以了 backward input 返回回来的dout cache output： 计算出来的x的梯度 implement: 求导，当原来的x大于0的时候，导数是1，链式法则是dout。小于等于0的时候是dout 所以直接对dout进行操作就可以了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107def affine_forward(x, w, b): """ Computes the forward pass for an affine (fully-connected) layer. The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N examples, where each example x[i] has shape (d_1, ..., d_k). We will reshape each input into a vector of dimension D = d_1 * ... * d_k, and then transform it to an output vector of dimension M. Inputs: - x: A numpy array containing input data, of shape (N, d_1, ..., d_k) - w: A numpy array of weights, of shape (D, M) - b: A numpy array of biases, of shape (M,) Returns a tuple of: - out: output, of shape (N, M) - cache: (x, w, b) """ out = None ########################################################################### # TODO: Implement the affine forward pass. Store the result in out. You # # will need to reshape the input into rows. # ########################################################################### out = x.reshape(x.shape[0], -1).dot(w) + b ########################################################################### # END OF YOUR CODE # ########################################################################### cache = (x, w, b) return out, cachedef affine_backward(dout, cache): """ Computes the backward pass for an affine layer. Inputs: - dout: Upstream derivative, of shape (N, M) - cache: Tuple of: - x: Input data, of shape (N, d_1, ... d_k) - w: Weights, of shape (D, M) - b: Biases, of shape (M,) Returns a tuple of: - dx: Gradient with respect to x, of shape (N, d1, ..., d_k) - dw: Gradient with respect to w, of shape (D, M) - db: Gradient with respect to b, of shape (M,) """ x, w, b = cache dx, dw, db = None, None, None ########################################################################### # TODO: Implement the affine backward pass. # ########################################################################### dx = dout.dot(w.T).reshape(x.shape) dw = (x.reshape(x.shape[0], -1).T).dot(dout) db = np.sum(dout, axis=0) ########################################################################### # END OF YOUR CODE # ########################################################################### return dx, dw, dbdef relu_forward(x): """ Computes the forward pass for a layer of rectified linear units (ReLUs). Input: - x: Inputs, of any shape Returns a tuple of: - out: Output, of the same shape as x - cache: x """ out = None ########################################################################### # TODO: Implement the ReLU forward pass. # ########################################################################### out = x.copy() out[out &lt;= 0] = 0.0 ########################################################################### # END OF YOUR CODE # ########################################################################### cache = x return out, cachedef relu_backward(dout, cache): """ Computes the backward pass for a layer of rectified linear units (ReLUs). Input: - dout: Upstream derivatives, of any shape - cache: Input x, of same shape as dout Returns: - dx: Gradient with respect to x """ dx, x = None, cache ########################################################################### # TODO: Implement the ReLU backward pass. # ########################################################################### dout[x &lt;= 0] = 0 dx = dout ########################################################################### # END OF YOUR CODE # ########################################################################### return dx sandwich layer在文件cs231n/layer_utils.py里面，有一些比较常见的组合，可以集成成新的函数，这样用的时候就可以直接调用不用自己写了 loss layer -&gt; 和assignment1里面写的内容是一样的two-layer networkcs231n/classifiers/fc_net.py TwoLayerNet init__ 需要初始化weights和bias，weights应该是0.0中心的高斯（=weight_scale），bias应该是0，都存在self.para的字典里面，第几层的名字就叫第几 input 图片的size hidden的个数 class的数量 weight scale，看初始的weights怎么分布 reg，regularization时候的权重 forward 用前面已经写好的东西计算前向 最后得到scores 再用scores计算loss，注意 计算loss也是一层 计算loss的时候注意他这里loss的参数是scores和lable backward back的时候不要忘记了loss也是一层，所以输入第二个sandwich的时候输入的应该是dscores而不是scores？！！！！ 计算gradient，注意他的function里面已经除了总数！ 别忘了加上L2的regularization Solver把之前那些训练啊，验证啊，计算accuracy之类的部分全都扔到一个class里面叫做solver，打开cs231n/solver.py 作用 solver部分包括所有训练分类所需要的逻辑部分，在optim.py里面还用了不同的update方法来实现SGD 这个class接受training和validation的数据和labels，所以可以检查分类的准确率，是否overfitting 需要先构成一个solver的instance，把需要的model，dataset，和不同的东西（learning rate，batch，etc）放进去 先用train()来训练，然后model的para都存着所有训练完的参数 训练的过程也会记录下来（accuracy的改变啥的） 最后训练的结果大约在50%12345678910111213141516171819model = TwoLayerNet()solver = None############################################################################### TODO: Use a Solver instance to train a TwoLayerNet that achieves at least ## 50% accuracy on the validation set. ###############################################################################solver = Solver(model, data, update_rule = 'sgd', optim_config=&#123;'learning_rate': 1e-3,&#125;, lr_decay=0.95, num_epochs=10, batch_size=100, print_every=100)solver.train()############################################################################### END OF YOUR CODE ######################################################################## 可视化这个最终的结果，loss随着epoch的变化和training acc以及val acc的变化12345678910111213141516# Run this cell to visualize training loss and train / val accuracyplt.subplot(2, 1, 1)plt.title('Training loss')plt.plot(solver.loss_history, 'o')plt.xlabel('Iteration')plt.subplot(2, 1, 2)plt.title('Accuracy')plt.plot(solver.train_acc_history, '-o', label='train')plt.plot(solver.val_acc_history, '-o', label='val')plt.plot([0.5] * len(solver.val_acc_history), 'k--')plt.xlabel('Epoch')plt.legend(loc='lower right')plt.gcf().set_size_inches(15, 12)plt.show() 记下来了这个loss和acc的history，所以就可以直接用来可视化了！ Multilayer network现在开始实现有多层的net 需要注意的问题主要是数数数对了，注意数字和layer的数量的关系 为了保证验证的准确，需要把loss的regularization算对才可以 反向往回推的时候，可以用 reversed(range(a))这个东西来进行 总体来说和两层的差不多，就是加进来了for循环 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202class FullyConnectedNet(object): """ A fully-connected neural network with an arbitrary number of hidden layers, ReLU nonlinearities, and a softmax loss function. This will also implement dropout and batch/layer normalization as options. For a network with L layers, the architecture will be &#123;affine - [batch/layer norm] - relu - [dropout]&#125; x (L - 1) - affine - softmax where batch/layer normalization and dropout are optional, and the &#123;...&#125; block is repeated L - 1 times. Similar to the TwoLayerNet above, learnable parameters are stored in the self.params dictionary and will be learned using the Solver class. """ def __init__(self, hidden_dims, input_dim=3 * 32 * 32, num_classes=10, dropout=1, normalization=None, reg=0.0, weight_scale=1e-2, dtype=np.float32, seed=None): """ Initialize a new FullyConnectedNet. Inputs: - hidden_dims: A list of integers giving the size of each hidden layer. - input_dim: An integer giving the size of the input. - num_classes: An integer giving the number of classes to classify. - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=1 then the network should not use dropout at all. - normalization: What type of normalization the network should use. Valid values are "batchnorm", "layernorm", or None for no normalization (the default). - reg: Scalar giving L2 regularization strength. - weight_scale: Scalar giving the standard deviation for random initialization of the weights. - dtype: A numpy datatype object; all computations will be performed using this datatype. float32 is faster but less accurate, so you should use float64 for numeric gradient checking. - seed: If not None, then pass this random seed to the dropout layers. This will make the dropout layers deteriminstic so we can gradient check the model. """ self.normalization = normalization self.use_dropout = dropout != 1 self.reg = reg self.num_layers = 1 + len(hidden_dims) self.dtype = dtype self.params = &#123;&#125; ############################################################################ # TODO: Initialize the parameters of the network, storing all values in # # the self.params dictionary. Store weights and biases for the first layer # # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be # # initialized from a normal distribution centered at 0 with standard # # deviation equal to weight_scale. Biases should be initialized to zero. # # # # When using batch normalization, store scale and shift parameters for the # # first layer in gamma1 and beta1; for the second layer use gamma2 and # # beta2, etc. Scale parameters should be initialized to ones and shift # # parameters should be initialized to zeros. # ############################################################################ pr_num = input_dim # can't use enumerate beacuse I need the number more than the size of hidden_dims for layer in range(self.num_layers): layer += 1 weights = 'W' + str(layer) bias = 'b' + str(layer) # 这时候是最后一层(the last layer) if layer == self.num_layers: self.params[weights] = np.random.randn( hidden_dims[len(hidden_dims) - 1], num_classes) * weight_scale self.params[bias] = np.zeros(num_classes) # other layers else: hidd_num = hidden_dims[layer - 1] self.params[weights] = np.random.randn( pr_num, hidd_num) * weight_scale self.params[bias] = np.zeros(hidd_num) pr_num = hidd_num if self.normalization in ["batchnorm", "layernorm"]: self.params['gamma' + str(layer)] = np.ones(hidd_num) self.params['bata' + str(layer)] = np.zeros(hidd_num) # print(len(self.params)) # print(self.params) ############################################################################ # END OF YOUR CODE # ############################################################################ # When using dropout we need to pass a dropout_param dictionary to each # dropout layer so that the layer knows the dropout probability and the mode # (train / test). You can pass the same dropout_param to each dropout layer. self.dropout_param = &#123;&#125; if self.use_dropout: self.dropout_param = &#123;'mode': 'train', 'p': dropout&#125; if seed is not None: self.dropout_param['seed'] = seed # With batch normalization we need to keep track of running means and # variances, so we need to pass a special bn_param object to each batch # normalization layer. You should pass self.bn_params[0] to the forward pass # of the first batch normalization layer, self.bn_params[1] to the forward # pass of the second batch normalization layer, etc. self.bn_params = [] if self.normalization == 'batchnorm': self.bn_params = [&#123;'mode': 'train'&#125; for i in range(self.num_layers - 1)] if self.normalization == 'layernorm': self.bn_params = [&#123;&#125; for i in range(self.num_layers - 1)] # Cast all parameters to the correct datatype for k, v in self.params.items(): self.params[k] = v.astype(dtype) def loss(self, X, y=None): """ Compute loss and gradient for the fully-connected net. Input / output: Same as TwoLayerNet above. """ X = X.astype(self.dtype) mode = 'test' if y is None else 'train' # Set train/test mode for batchnorm params and dropout param since they # behave differently during training and testing. if self.use_dropout: self.dropout_param['mode'] = mode if self.normalization == 'batchnorm': for bn_param in self.bn_params: bn_param['mode'] = mode scores = None ############################################################################ # TODO: Implement the forward pass for the fully-connected net, computing # # the class scores for X and storing them in the scores variable. # # # # When using dropout, you'll need to pass self.dropout_param to each # # dropout forward pass. # # # # When using batch normalization, you'll need to pass self.bn_params[0] to # # the forward pass for the first batch normalization layer, pass # # self.bn_params[1] to the forward pass for the second batch normalization # # layer, etc. # ############################################################################ cache = &#123;&#125; temp_out = X for i in range(self.num_layers): w = self.params['W' + str(i + 1)] b = self.params['b' + str(i + 1)] if i == self.num_layers - 1: scores, cache['cache' + str(i + 1)] = affine_relu_forward(temp_out, w, b) else: temp_out, cache['cache' + str(i + 1)] = affine_relu_forward(temp_out, w, b) ############################################################################ # END OF YOUR CODE # ############################################################################ # If test mode return early if mode == 'test': return scores loss, grads = 0.0, &#123;&#125; ############################################################################ # TODO: Implement the backward pass for the fully-connected net. Store the # # loss in the loss variable and gradients in the grads dictionary. Compute # # data loss using softmax, and make sure that grads[k] holds the gradients # # for self.params[k]. Don't forget to add L2 regularization! # # # # When using batch/layer normalization, you don't need to regularize the scale # # and shift parameters. # # # # NOTE: To ensure that your implementation matches ours and you pass the # # automated tests, make sure that your L2 regularization includes a factor # # of 0.5 to simplify the expression for the gradient. # ############################################################################ loss, dscores = softmax_loss(scores, y) reg_loss = 0.0 pre_dx = dscores for i in reversed(range(self.num_layers)): i = i + 1 reg_loss = np.sum(np.square(self.params['W' + str(i)])) loss += reg_loss * 0.5 * self.reg pre_dx, dw, db = affine_relu_backward( pre_dx, cache['cache' + str(i)]) dw += self.reg * self.params['W' + str(i)] db += self.reg * self.params['b' + str(i)] grads['W' + str(i)] = dw grads['b' + str(i)] = db ############################################################################ # END OF YOUR CODE # ############################################################################ return loss, grads 检测网络是否overfitting 选择了一个三层的网络，小幅度改变learning rate和init scale 尝试去overfitting出现了一些问题不是太能overfitting我不知道为什么 update rules在得到了back出来的dw之后，就需要用这个dw对w进行update，这里有一些比较常见的update方法 普通的update 仅仅沿着gradient改变的反方向进行(反方向是因为计算出来的gradient是上升的方向)x += - learning_rate * dx SGD + momentumhttp://cs231n.github.io/neural-networks-3/#sgd 是对这个update一点物理上比较直观的理解（其实名字叫做动量） 可以理解为这个东西是在一个平原上跑的一个球，我们需要求的w是这个球的速度，得到的dw是这个球的加速度，而这个球的初速度是0 可以理解为这个球找最低点的时候，除了每步按dw update，还在上面加上了前面速度的影响，也就是加上了惯性！123# Momentum updatev = mu * v - learning_rate * dx # integrate velocityx += v # integrate position Nesterov Momentum(NAG) 在原来的基础上：真实移动方向 = 速度的影响（momentum）+ 梯度的影响 （gradient） 现在：既然我们已经知道了要往前走到动量的影响的位置，那么我根据那个位置的梯度再进行update，岂不是跑的更快！ 总的来说就是考虑到了前面的坡度（二阶导数），如果前面的坡度缓的话我就再跑快点，如果陡的话就跑慢点123v_prev = v # back this upv = mu * v - learning_rate * dx # velocity update stays the samex += -mu * v_prev + (1 + mu) * v # position update changes form cs231n/optim.py 加入了新的计算update的方法 具体的原理还没有看，但是计算就是这样计算的 12345678910111213141516171819202122232425262728293031def sgd_momentum(w, dw, config=None): """ Performs stochastic gradient descent with momentum. config format: - learning_rate: Scalar learning rate. - momentum: Scalar between 0 and 1 giving the momentum value. Setting momentum = 0 reduces to sgd. - velocity: A numpy array of the same shape as w and dw used to store a moving average of the gradients. """ if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-2) config.setdefault('momentum', 0.9) v = config.get('velocity', np.zeros_like(w)) next_w = None ########################################################################### # TODO: Implement the momentum update formula. Store the updated value in # # the next_w variable. You should also use and update the velocity v. # ########################################################################### v = config['momentum'] * v - config['learning_rate'] * dw w += v next_w = w ########################################################################### # END OF YOUR CODE # ########################################################################### config['velocity'] = v return next_w, config 可以看出来最终的结果会比普通的SGD上升的更快 分别又尝试了RMSProp and Adam]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>Fully Connected Net</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于空间投影增强（SAR）的论文]]></title>
    <url>%2F2019%2F04%2F11%2F%E5%85%B3%E4%BA%8E%E7%A9%BA%E9%97%B4%E6%8A%95%E5%BD%B1%E5%A2%9E%E5%BC%BA%E7%9A%84%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[相关介绍链接koike lab roomAlive 关于opencv的fisheye calibration：http://ninghang.blogspot.com/2012/08/fish-eye-camera-calibration.html 关于calibration的视角的问题，matlab可以找到全部视角：https://www.mathworks.com/help/vision/ug/fisheye-calibration-basics.html DeepCalib: A Deep Learning Approach for Automatic Intrinsic Calibration of Wide Field-of-View Cameras（CVMP ‘18）使用深度学习对fish eye相机的视野进行补全。 文章中公开的code和其他资料 Abstract 广角相机的calibration在各种各样的地方都有应用 3D重建 image undistortion AR camera motion estimation 现在存在的calibration都需要多张图片进行校准（chessboard） 提出了一种完全自动的标定方法，基于CNN 在网上找了非常多的omnidirectional的图片进行训练，生成了具有100多万张图片的dataset Intro 广角相机的calibration最重要的是测量 intrinsic parameters 两个wide FOV的重要参数：focal length &amp; distortion parameter 现存的calibration方法有很多限制： 需要一个object的多个角度的观察 需要观察一个特定的structures 在多张图片中观察相机的移动 现在最有名的方法是chessboard 他们提出的方法可以解决上述的问题，并且针对网上下下来的照片也可以用 主要方法 一个CNN with Inception-V3 architecture 目标 收集具有不同intrinsic parameters的图片，然后自动生成具有不同的focal length和distortion的图片（没懂） 对比不同的CNN结构 related work 以前存在的calibration方法主要可以分成四个不同的部分 最常使用的就是棋盘一类的，需要观察一张图片的不同部分，从而得到结果。问题主要是出在比价麻烦，而且无法对野生的照片进行calibration 基于图片上面的geometric structure，line，消失的点等。不能处理general environments self-calibration（自身还有一些容易收到影响的问题） 需要多张图片 需要camera motion estimation 基于DL的，但是都是解决了部分问题 没有把参数全都估计出来 dataset是从prespective的图片生成回来的，会有不完整的部分（但是他们的很完整而且会有很多应用） Approach选择model -&gt; 自动生成large-scale dataset -&gt; network的structure Projection &amp; distrotion model（考虑生成dataset的机器） 广角相机需要具体的projection model把3D的世界map到图片里面去 考虑了几个model Brown-Conrady’s model(1971) 在实际应用里面不适合广角相机的大的distortion hardly reversible division model [Fitzgibbon 2001] 只是为了fisheye设计的，对相机没有普适性 impossible to revert 这篇文章里面的model unified spherical model [Barreto 2006; Mei and Rives 2007] 原因 fully reversible 可以解决很大的distortion projection和back-projection都admit closed-form solution -&gt; 计算效率非常高（没怎么看懂） generation dataset 因为根本做不到并且还没有那么大的dataset，所以他们打算人工合成一些(synthetically) 没有选择用prspective的图片生成 在普通的图片里面加上distortion会把图片里应该看不到的地方看到（边缘都会变成黑色的） -&gt; 生成的图片不真实 使用panoramas得到图片 因为全景图都是360度的，那么多少度的广角都能驾驭 可以假设把相机放在任何地方 对于给的一张全景图，可以自动生成不同焦距，不同distortion的图片，这样就得到了很大的dataset network architecture Inception-V3 structure 基于上面的，实践了三种不同的网络 一层网络，输出两个不同的结果，一个是f一个是distortion DualNet，由两个独立网络组成，一个输出f，一个输出distortion，这两个值是相互独立的。 SeqNet，两个连在一起的网络，先从A网络里得到f，再把图片和f放进B得到最终的distortion 解决问题： classification regression resultnet的参数 net在imageNet上面pre-train了，然后再进行了进一步的训练 evaluation对比上面不同三个网络的performance user study估计出来的结果很难说明到底是不是成功的undistort了，所以设计了user study Combining Multiple Depth Cameras and Projectors for Interactions On, Above, and Between Surfaces（‘2010）感觉算是比较最早的SAR的部分，重点就是用多个视角的depth camera来捕捉用户的动作，完成相应的交互，不知道在桌子上的投影和在墙上的投影是怎么实现的 abstract 可以交互的displays和surface 可以投影到非常规的投影表面上面去 可以把这些东西扔来扔去，之类的 Intro the user may touch to manipulate a virtual object projected on an un-instrumented table（这个现在已经不新鲜了） office size room depth camera的妙用 这个空间的任何地方都是surface，都可以投影 整个空间是一个大的电脑 可以投影到user自己的身上去-&gt; 可以投影到用户的手上 3D mesh data 硬件构成：multiply的depth camera&amp; projector 支持的interaction 可以交互的非显示器部分（比如墙壁或者桌子） 所有的部分可以连接成一个可交互的部分，可以通过肢体来进行两个屏幕之间的交互（同时摸这两个东西他就会换位置） 可以从display上面pick up出东西来 检测出用户的动作来，支持动作的交互 implement 在天花板上装了三个depth camera和三个projector，可以看到交互的地方，不需要特别精准的calibration PrimeSense camera，有IR和RGBcamera depth image可以用来分离静止的物体 calibration both the cameras and the projectors are registered with the real world. camera a fixed grid of retro-reflective dots 3D camera pose estimation de- scribed by Horn[13] interactive space calibration之后camera就可以捕捉real time的3D mesh model 因为camera和projector一起校准过了，所以投影就可以正确的投影在相应的地方了 根据mesh model可以得到手的三维图形，根据这个图形就可以知道手在touch哪个地方了 在tracking上面用了更简单的算法：[28] [29] 直接对3D的mesh进行操作比较复杂，所以对2D的画面进行了操作 virtual camera first transforming each point in every depth camera image from local camera to world coordinates, and then to virtual camera coordinates by virtual camera view and projection matrices. z方向的坐标由xy写出来 -&gt; 把一张深度图片压成了一个2D的图片 结合多个角度判断用户的最终动作 用上方的摄像机的图片判断用户是不是同时接触两个东西了 空间里的mene -&gt; 在特殊的一个地方有投影 RoomAlive: Magical Experiences Enabled by Scalable, Adaptive Projector-Camera Units(UIST ‘14)感觉是一个比较完全的屋内投影的例子了 Abstract 可以动态的适应任何的屋子 touch, shoot, stomp, dodge, steer投影上去的东西，以及和物理的环境交互 projector-depth camera unit -&gt; 所以就不需要特别多的calibration（可以重点看看这个unit是怎么制作的） Intro 做了一个游戏系统 projector和depth camera一体的东西 cover the room’s walls and furniture with input/output pixels track用户的动作，并且根据动作在屋子里面生成对应的东西 capture &amp; analyze屋子里的结构，得到房间里面的墙以及地板之类的特征 a distributed framework for tracking body movement and touch detection using optical-flow based particle tracking [4,15], and pointing using an infrared gun [19]. -&gt; 其实还不是没有依据视觉来捕捉这个东西 居然装了6个相机-投影仪的unit （procam） related workSpatial Augmented Reality (SAR) use light to change appearance physical objects illumiroom -&gt; 非常喜欢这个idea projection mapping很多都需要在特定的东西上面mapping -&gt; 但是这个可以在整间屋子的任意部分mapping System unit -&gt; color camera + IR camera emitter + wide FOV projector + computer in a large living room (说明这种研究里面屋子的大小也非常的重要) + 6 units plug-in to the Unity3D commercial game engine （怪不得能做游戏 硬件 wide field of view projectors 每个部分connected to他自己特定的电脑 所有的部分都装在房间的屋顶上 auto calibration 并不需要calibration所有的相机 在units之间有一部分的overlap，所以东西在校准的时候观察同一个东西就行了？ 用opencv的校准function chain together所有的部分然后得到了各个相机的关系 auto scene analysis 所有的unit得到的深度信息，生成之后寻找连续的平面（墙，地板等等） Hough transform（并不会这个东西） 游戏 unity3D的plug-in 游戏设计者只需要在设计界面里添加东西就行了 mapping 事实渲染整个东西的任务没有完全解决 4个技术 content in a uniformly random way 哈。。。居然是随机投影出来的 针对不同类型的被投影的东西，会根据不同的原理出现在不同的地方（比如石头只会出现在地面上） 投影的东西针对用户现在的位置，只投在用户自己看得到的地方 在移动屋子里的物理物品的时候，改变屋子的部分 tracking user interface body movement, touching, stomping, pointing/shooting and traditional controller input [4,15]捕捉了depth map -&gt; ‘proxy particles’,就是动作游戏里面的体感捕捉的算法 -&gt; tracked by using a depth-aware optical flow algorithm gun的input选择了红外枪 也支持寻常的游戏手柄 rendering RoomAlive tracks the player’s head position and renders all virtual content with a two-pass view dependent rendering 这部分主要讲游戏怎么设计的limitation calibration errors！这样在交叠的地方会出现重影 system latency 延迟QAQ 在overlap的sensors上面解决tracking issues Peripheral Expansion of Depth Information via Layout Estimation with Fisheye Camera( ‘16)从RGBD鱼眼相机提取深度信息（但是这个用了多个相机的system） abstract 一个普通的RGB相机和一个fish eye，把视角扩展到了180° developed a new method to generate scaled layout hypotheses from relevant corners, combining the extraction of lines in the fisheye image and the depth information overcome severe occlusions. intro 主要就是把现有的RGB fisheye camera和Depth camera结合起来，得到鱼眼的深度信息 Pedestrian Detection in Fish-eye Images using Deep Learning: Combine Faster R-CNN with an effective Cutting Method(SPML ‘18)用鱼眼相机和RCNN来检测行人（感觉这个检测的目标比较小） -&gt; 怎么感觉挺水的 abstract 鱼眼相机的边缘扭曲问题 -&gt; rotary cutting to solve the problem 把相机分成了边缘部分和中间部分 Method 裁剪图片 绕着鱼眼相机的中心旋转，30度，12次 每次旋转完截取三组图片，分别是靠边缘的和靠中心的 -&gt; 更好检测人群（垂直的） 使用这些裁剪的图片training]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>projection mapping</tag>
        <tag>space augumented</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 1Q AR笔记]]></title>
    <url>%2F2019%2F04%2F09%2F2019Q1AR%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[整体步骤 检测marker 为他计算计算6DOF pose render 3D 图片 把图片和marker组合在一起 slides学习opengl(学习老师的cpp代码风格) Ex1:检测marker的四个基础点打开相机 注意初始化 videocapture 应该是在while的循环之前的 thresholding 函数 cv::threshold(…) src dst thres的值，也就是阈值的边界值 double maxval，在最后一个参数是binary的时候，确定binary的最大值 type，二值图，反二值图，保留原色等乱七八糟的 注意binary之前要先 cvtcolor到灰度图！！！ cv::adaptiveThreshold &lt;- 试试这个函数的作用，不用自己设置threshold了 并不想set这些hypers manully 除了寻常需要设置的东西之外，还需要 adaptiveMethod block size（需要被用来计算threshold的value） C：需要被从整体中减去的一个constant 有些变量的地方用到了const &lt;- 感觉应该学学老师的编程风格detect connected componentscv::findContours 函数 图片 contours vector&lt;std::vector&lt;cv::Point&gt; &gt; hierarchy vector&lt;cv::Vec4i&gt;，contour的拓扑学信息 mode （注意在这里选择要外轮廓还是内外都要） method：估计contour的方法 offset（当从ROI提取轮廓然后在整张图片里面分析的情况） 去除过小的contour12345678910vector&lt;vector&lt;Point&gt;&gt; :: iterator itc = contours.begin(); while(itc != contours.end())&#123; if(itc -&gt; size() &lt; 60)&#123; itc = contours.erase(itc); &#125; else&#123; itc++; &#125; &#125; 老师的代码里面是先进行了估计，计算了bound的面积，然后根据 面积大小，占整张图片的百分比 几个角 cv::isContourConvex ：检查这个marker的凸性，毕竟形状不能是凹的，直接输入这个多边形的array，输出的就是bool 估计contour的多边形 approxPolyDP 被估计的contour 估计出来的的多边形 估计的参数，影响估计的精度 -&gt; const auto epsilon = 0.05 * cv::arcLength(contour, true); 计算一个curve的长度 closed，估计出来的多边形是不是封闭的 画出来只有四个角的多边形 drawContours() 图片 需要画的轮廓s （注意这是这张图片里面的所有轮廓） 需要画的index thickness 线的 线得种类 Optional information about hierarchy. maxLevel Maximal level for drawn contours. If it is 0, only the specified contour is drawn. If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This parameter is only taken into account when there is hierarchy available. offset：可选 老师用的方法是计算了各个点和计算了各个边，所以可以画出来边上还有好多点的结果 一种神奇的定义颜色的方式，直接随机出来 const cv::Scalar kEdgeColor(rand() &amp; 255, rand() &amp; 255, rand() &amp; 255); -&gt; 这样的话出来的每一帧的框的颜色都会改变![] 用cv::polylines来画出来polys的curve，并且要画closed的 画出来图片的delimiters(为下一步做准备) 目的：从一个corner到另一个corner的方向vector 步骤 首先在corner上面用circle画出来 检查每个edge 每个edge上有6个点，然后计算两个corner之间的dx和dy的方向，除以部分的数量（7）就是每个小块的方向，然后把这个小块重复6次12const double dx = (double)(contour_approx[(i+1)%kNumOfCorners].x-contour_approx[i].x)/(double)(kNumOfEdgePoints+1);const double dy = (double)(contour_approx[(i+1)%kNumOfCorners].y-contour_approx[i].y)/(double)(kNumOfEdgePoints+1); 第一部分结束后的完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#include "ARdetection.hpp"ARDetection::ARDetection(void)&#123; &#125;Mat ARDetection:: SearchMarkers(Mat frame)&#123; Mat img_gray; Mat dst; // threshold之前要先改成灰度图 cvtColor(frame, img_gray, CV_BGR2GRAY); adaptiveThreshold(img_gray, dst, 255, ADAPTIVE_THRESH_MEAN_C, THRESH_BINARY, 33, 5);// threshold(img_gray, dst, 104, 255, THRESH_BINARY); imshow("lalala", dst); waitKey(1); // 每个点，一圈点事是一个contour，一堆kcontours vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(dst, contours, hierarchy, RETR_LIST , CHAIN_APPROX_NONE); // 检测图片过小部分// vector&lt;vector&lt;Point&gt;&gt; :: iterator itc = contours.begin();//// while(itc != contours.end())&#123;// if(itc -&gt; size() &lt; 60)&#123;// itc = contours.erase(itc);// &#125;// else&#123;// itc++;// &#125;// &#125; vector&lt;vector&lt;Point&gt;&gt; polys(contours.size()); for(int i=0; i &lt; contours.size(); i++ )&#123; // 根据每个轮廓调节这个参数的大小 const auto epsilon = 0.05*cv::arcLength(contours[i], true); approxPolyDP(contours[i], polys[i], epsilon, true); // if(polys[i].size() == 4)&#123;//// 用这个函数画不出来斜的东西//// rectangle(frame, polys[i][0],polys[i][2], Scalar(0,255,0));// drawContours(frame, polys, i, Scalar(255,0,0), 5, 8, vector&lt;Vec4i&gt;(), 0, Point());// &#125; Rect rect = boundingRect(polys[i]); const int marker_size = rect.area(); const int ImageSize = img_gray.cols * img_gray.rows; const int marker_size_min = int(ImageSize * 0.02); const int marker_size_max = int(ImageSize * 0.95); const int marker_corners_num = 4; const bool is_vaild = (marker_size &gt; marker_size_min) &amp;&amp; (marker_size &lt; marker_size_max) &amp;&amp; (polys[i].size() == marker_corners_num) &amp;&amp; isContourConvex(polys[i]); if (is_vaild == false) continue; // 这样画出来的是随机的颜色的 const Scalar EdgeColor(rand() &amp; 255, rand() &amp; 255, rand() &amp; 255); polylines(frame, polys[i], true, EdgeColor,5); // 下面需要把每个边分成6个部分 for(int j = 0; j &lt; marker_corners_num; ++j)&#123; const int edge_point_num = 6; const int circle_size = 5; circle(frame, polys[i][j], circle_size, Scalar(0,255,0),FILLED); const double dx = (double)(polys[i][(j+1)%marker_corners_num].x - polys[i][j].x)/(double)(edge_point_num + 1); const double dy = (double)(polys[i][(j+1)%marker_corners_num].y - polys[i][j].y)/(double)(edge_point_num + 1); for(int k = 0; k&lt; edge_point_num; ++k)&#123; const double edge_point_x = (double)(polys[i][j].x) + (double)(k+1)*dx; const double edge_point_y = (double)(polys[i][j].y) + (double)(k+1)*dy; Point edge_point((int)edge_point_x,(int)edge_point_y); circle(frame, edge_point, circle_size, Scalar(0,0,255),-1); &#125; &#125; &#125; return frame;&#125; 第一部分运行之后的结果 Ex2. find marker precisely 现在有corner，corner之间的line，这个line还被分成6个部分（~因为大小是6x6？的= 两个边 + 中间四个格？？~） 光检测边缘是不够的，想知道这个边缘实际是什么样子的 现在只知道虚线的部分是什么样子的 并且现在已经把每个边都分部分了 -&gt; 画出来垂直的分块的线，找到这个线和颜色突变的交点，重新画出来新的线（实线） 希望找到颜色突变的地方 但是实际上的颜色不是突变的，是白 -&gt; 灰 -&gt; 黑 操作步骤 预准备 在每个side找六个点 在边上提取三个像素宽度的stride cv::GetQuadrangleSubPix() -&gt; 不会用！ 从输入的array得到四边形 src输入图像 dst提取出来的四边形 变换的矩阵 Sober operator 图像处理里面的常用算子 -&gt; 主要用于边缘检测，用来运算与灰度的相似值 包含两组3x3的矩阵，中间的3x1的0，分别为横向和纵向，另外两面对称 然后与图片做卷积，分别计算x方向和y方向的灰度值 然后把Gx和Gy求平方和的根，最后得出来这个像素点的灰度值（或者有的时候也可以用绝对值来计算，这样计算的消耗小一点） 步骤 在每个stripe里找到最高的change 对这个change和她周围的点（在原来的曲线上），找到一个新的二次曲线 找到这个曲线的顶点（一阶导数） 如何得到图片的subpixel color不是原来的方方正正的，而是沿着那个直线方向的新的方方正正 就是取各个颜色在面积上的平均， 老师的代码 sublixSampleSafe -&gt; 这个函数输入测试的图片（gray和已经得到的subpix点 把这个点求floor，int，得到基准点，然后检查是不是在图片里面（不是的话返回gray，127） 如果在图片里面的话计算出来实际的坐标 确定marker的id corner detection exact sides 找到沿着刚才计算出来6个点的一条线fitLine precise corner 从各个边的交点计算精确的corner（因为各个边已经很精确了） Marker Rectification 创建一个6x6 pix的ID图片 (-0.5,-0.5) to (5.5,5.5) 从原图片的perspective warp到Id image cv::getPerspectiveTransform or cv::warpPerspective 需要找到的是linear transformation]]></content>
      <categories>
        <category>AR</category>
        <category>上课笔记</category>
      </categories>
      <tags>
        <tag>AR</tag>
        <tag>OpenCV</tag>
        <tag>OpenGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Notes for papers about smart kitchen]]></title>
    <url>%2F2019%2F04%2F08%2Fkitchen%2F</url>
    <content type="text"><![CDATA[Choptop: An Interactive Chopping Board2018 abstract 一个可交互的案板 可以给用户菜谱的指导，承重，计时 用户可以通过按案板来进行对画面的操作 上面就是长成上图的样子，案板底下充满了各种传感器。 Intro 针对学生不会自己做饭，不吃新鲜的饭的问题，缺时间 -&gt; 一步一步的把怎么做饭写出来了，包括图片动画等东西 built-in timer用来每一步计时 使用mobile device来提高菜单的交互性 考虑到手的脏等问题，所以不是按屏幕而是按案板（这里考虑能不能像pac pac一样，用手势操作） load sensors，可以解决称重的问题 思路 主要目的是一种新的学习做饭的方法 （作为做饭有天赋的人，我认为这样没有灵魂！） related work smart kitchen Research has aimed to improve the cook- ing process, promote healthier eating and make it simpler to procure ingredients，大家都从不同的角度实现智能厨房 他这个论文的东西成本不是特别高也不是特别大（是在嘲讽我吗） load sensing 之前已经用了很多force的传感器 之前也有用过带重量传感器的案板，以及带扭矩传感器的刀[9] 他们认为camera没有什么用，并且把所有的硬件都藏起来了 装这个senser的方法参考了[12] Design硬件 整个硬件是self-contained的 屏幕是单片机控制的 力量传感系用 检测按压用的是edge detection -&gt; 防止检测到其他东西（原理不是太懂） UI The interface updates based on the information delivered from the recognition engine 成功之后还会有声音 按案板的不同部分就可以往不同的方向移动 user study 找了十三个人，准备一道沙拉 [3]里面找到了一个调查问卷System Usability Scale future SVM训练了74%的测试率（好低），提高正确率 提高自动翻页（？ 用用户的手机来达到屏幕的作用 进一步分user study CookTab: Smart cutting board for creating recipe with real-time feedback2012 abstract 考虑到很多厨师做饭随心所欲，而且不会记录下来精准的用量 一个可以记录下来用量的案板 intro 专门针对切菜部分的记录的软件 记录用的材料的名字，菜量，视频和调味方法，然后会有real-time的feedback related work [3]可以记录视频，声音，用的camera和mic 加重量感应的， [2]四个承重的模块，加速度传安琪 但是他们的系统会有real-time的feedback 不好意思好像就是在pad上切菜 Enabling Nutrition-Aware Cooking in a Smart KitchenCHI 2007（大概只能看个概念了） abstract 目标：health cooking（是不是大家的目标都是那么伟大） sensors，detect cooking activities, and digtail feedback intro 主要目标就是如果人加东西假的过量了或者怎么着，就会提醒 Smart Kitchens for People with Cognitive Impairments: A Qualitative Study of Design RequirementsCHI-2018]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Smart Kitchen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于mac消去分区]]></title>
    <url>%2F2019%2F04%2F08%2F%E5%85%B3%E4%BA%8Emac%E6%B6%88%E5%8E%BB%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[最近刚接手了别人的mbp，他因为装双系统分区之后，windows分区无法消去。试了一圈之后发现只要再新建一个分区，然后再一起合并就可以了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于Python的字典多key，value返回key]]></title>
    <url>%2F2019%2F04%2F05%2F%E5%85%B3%E4%BA%8EPython%E7%9A%84%E5%AD%97%E5%85%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E4%B8%9C%E8%A5%BF%2F</url>
    <content type="text"><![CDATA[多个keys1dict = &#123;(key11,key12): value&#125; 大概就是长这个样子的，key的个数多少没有限制，访问value的时候1dict[(key11,key12)] 多个value1dict = &#123;key1:(value1, value2)&#125; 访问的时候大概就是这么取值12dict[key]dict[key][index] 从value找到key 先通过list(dict.key())获得所有的key，变成一个list list(dict.value())得到所有的value的list 上面这两个list的index相同，先获取value的index，然后再作为key的index去key里找 例子：12dict = &#123;'a': 1, 'b': 2&#125;list(dict.key())[list(dict.value()).index('1')]]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>字典</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Human Computer Interaction笔记]]></title>
    <url>%2F2019%2F04%2F04%2FHCI%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Introduction 50% 出勤和 50% report koike森森还有assignment History 1945 Vannevar Bush memex的概念，二战末提出了一种信息机器的设想（个人图书馆） 这种机器内部用微缩胶卷（microfilm）存储信息，也就是自动翻拍，可以不断往里面添加新的信息；桌面上有阅读屏，用来放大阅读微缩胶卷；还有许多个按钮，每一个按钮代表一个主题，只要按一下，相应的微缩胶卷就会显示出来。每一个胶卷内部还记录着相关的其他胶卷的编号，可以方便地切换，形成同主题阅读。在Bush博士的设想中，这种机器还可以与图书馆联网。通过某种机制，将图书馆收藏的胶卷，自动装载到本地机器上。因此，只通过这一个机器，就可以实现海量的信息检索。from 百度百科 as we may think：同时提出了wearable电脑 1946 Eniac 第一个计算机 没有keyboard和display，只能手动 1951 UNIVAC 可以I/O 先在纸条上打孔，然后再放进去读 Ivan Sutherland 1963 SketchPad，display上面有图像了，并且可以对画面进行操作 可以用光笔操作，不是用键盘操作了 CAD鼻祖 第一个VR设备居然也是他做出来的 Sword of Damocles(1968) Douglas Engelbart 居然出现了鼠标，装了rotatory wheels（竖着的那种），可以在两个方向移动（所以装了两个吗？） Alan Kay PC之父 1972 Dynabook，card board做的，因为CPU和GPu太大了，屏幕也没有，并不是真的 1996年东芝做了个叫这个名字的PC 1973 Xerox Alto real working machine 1981 Xerox Star 出现了桌面系统 GUI Ted Nelson Hypertext Editing System -&gt; pen to jumo to another page Steve Jobs &amp; Bill Atkinson HyperCard 1987，同类的信息都在同一张卡上，然后所以的卡都连在一起，可以在卡中间jump。事情就变得非常简单了 Tim Berners-Lee father of WWW （World Wide Web） Richard Bold put that there可以用手势交互，语言交互 非常大的一个display和projector Mark Weiser father of the concept of Ubiquitous conputing(1991) 预言了以后大家家家有电脑 Jaron Lanier VPL data glove &amp; HMD 1989，手套里面有纤维 I/O的硬件如何组合 -&gt; 新的HCI方式 design &amp; evaluationACM SIGCHI What’s HCI CS design 是一个交叉学科 学会 CHI -&gt; 更注重想法，和转化成实现 UIST -&gt; 更注重implement IEEE/ACM Ubicomp CSCM 重要性 保证安全性，提升生活质量 在商业上的产品化 推荐书 The Design of Everyday things 核心思想Affordence: 人想象的这个东西的用途和实际的用途，让用户看到这个东西就知道是干什么用的 比如傻屌的看见按键不知道按哪个的瓦斯炉 方向不同的车座靠背调节 根本不知道哪个是哪个的电灯开关 想法 mapping ui to real layout design is stupid 七个准则 by Norman 8 golden rules by. Shneiderman consistency（一致性） -&gt; 比如mac的pull down最下面都是quit，大家的位置都差不多 Short-term memory 比如菜单里面的个数 7+-2 magic number Bringing Design to software T.Winograd GUI &amp; hypermediaGUI CUI -&gt; GUI CUI: I: keyboard O: charracters in display GUI I: keyboard + mouse O: bitmap in display desktop like a real office emvironment (metaphor) document,folder,trash 对于没有用过电脑的人来说非常容易理解 visualizing to icons operating mouse Jobs居然copy了这个东西 跟现在的也没有太多概念（standard interface for computer) pros visual by icon，因为视觉看出来的东西比较好理解 direct manipulation interaction abstract by folders cons number of icons make user confused more computing more physical space typing is faster with keyboard 其他的一些想法 room metaphor[henderson86] different romms for different task multiply desk (就像mbp的多个桌面一样) based on user studys &lt;- how they use each applications 并没有变成主流，哭哭 超整理法 super-organizing metaphor 如何整理物理文件 organize by time, not name sequentially, not hierarchically implemented by Freeman -&gt; Lifestream GUI的一个特征WIMPwindow, icon, menu, pointer（like mouse） 整体来看 苹果把pull down在左上角，因为从左往右拉比较容易 windows: 因为不想和apple一样所以扔到底下了 difficult for icon 比如路上的标志设计的就很迷，大家都不知道是干啥的 direct manipulation 比如在删除东西的时候CHI需要自己输入，但是GUI可以直接拖进trash里面 WYSIWYGwhat you see is what you get PUI?I: recognizationO: large/ small displays PUI(perceptual)GUI -&gt; PUI PUI: using various input(sensors) GUI: mouse&amp; keyboard, not intuitive vision-based HCIwhy? natural &amp; intuitive? not special device, unwired multimodel application recognition detection &amp; recognition object &lt;-(detect) - object detection system(find the thing in the real world) object database &lt;-recognition- objection recognition system(know what it is) detect the hand colors（shapes?） infrared camera near infrared -&gt; vedio cameras(capture near infrared light to the object) far infrared -&gt; capture the heat hand location(手指在哪，手在哪) hand regions find the centre -&gt; morphlogical operation fingers -&gt; pattern matching(有很多不同的方法) tracking gesture recognition difficult segmentation of hands/body -&gt; depth camera recognition of 3D pose(occlusion) 如果用户转身了，手会被其他的东西挡住 detecting begin/end of gestures 一个非常重要的问题！ high-speed gestures (systems) pac！pac！ as many hands as possible advantages robust against light conditions real-time with 40 people with 2 hands No instruction necessary 3D gestures(for navigation in VR) 2 cameras recognise hand shapes pattern classification(NN) object recognition tag-based pre-registration of objects difficult to attached on something(unbralla, glove…) unnartual overlook based on color information(‘1991) 3D histogram(RGB) translation/rotation invariant(如果图片改变了方向或者变了，但是颜色信息还没变) 但是颜色相似的时候没法分辨 PTAM(‘2007) recgonize feature positions features &amp; markers gaze recognition infrared cameras &amp; LEDs pattern matching corners of eyes,mouse,shaping triangle -&gt; face direction 4/18interactive surface 例子 handheld:phone, tablet horizantal: desk vertical: wall digital desk(‘93) overhead projector + camera + desk metaDesk(‘97) -&gt; 用两个奇怪的方块，对这个map进行操作 LCD tabletop LCD -&gt; larger, thinner,lighter,higher resolusion, less expensive before that use projectors(dark) use as window? real glass is expensive then LCD principles of polarization 滤光吗，两个方向的（偏振片） 这样可以用来检测手，把手之后的背景光滤掉 非常好用 可以用来检测手 AR marker这样的东西实在是太丑了 design invisiable markers 把偏振光片减成了ar marker的样子，人看不到但是机器可以识别 background &amp; motivation traditional surfaces are planar &amp; regid difficult to make 3D surface photoelasricity -&gt; 透明的材料对不同载荷下颜色不同 -&gt; 也可以用来作为影响偏振光的因素，可以用来按，按下去光就能过去惹 electrical shock 为什么会有这种电人的display啊！（BIRIBIRI） beyond 2D surfaceCaytrick surface(‘18)4/22 information visualization更快，更精准的理解info(shape/ colors -&gt; information) SciVis &amp; InfoVis Sci 用户比较专业 用来理解专业的现象 physical data, measured data, simulation data Info abstract data 给人民群众看的，感觉更加直观 how to layout the data three issues scalability 如果我们想要vis info，如果data的量太大了，有些东西看起来就很复杂(eg.trees) limited display size human cant understand tech layout scalability filiter layout graph drawing 好看，economically（多级化的，中心辐射，引力型 -&gt; 不同关系的相斥，圆环状） tree structure TreeMap 5/9 Cognitive processwhy important 看到的不一定就是真实的 通过改变HCI，可以改变看到的东西的 Seven Stage Model]]></content>
      <categories>
        <category>HCI</category>
        <category>上课笔记</category>
      </categories>
      <tags>
        <tag>HCI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xcode的breakpoint1.1问题以及打开摄像头]]></title>
    <url>%2F2019%2F04%2F04%2FXcodebreakpoint%2F</url>
    <content type="text"><![CDATA[最近上课又要捡起来c++了，半年前才换的mac用xcode没vs顺手，好几次遇到了挺神奇的错误。 thread 1 breakpoint 1.1这个问题其实就是你在代码里面自己加上了断点(breakpoint)，估计是不小心点到的。取消了断点就行 关于Xcode允许相机xcode的相机许可我之前折腾了一个下午才发现怎么搞。 首先，需要有一个允许相机的Info.plist文件，文件内容如下 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;&lt;plist version="1.0"&gt;&lt;dict&gt;&lt;key&gt;NSCameraUsageDescription&lt;/key&gt;&lt;string&gt;Used to capture new image for photo effect&lt;/string&gt;&lt;key&gt;CFBundleName&lt;/key&gt;&lt;string&gt;$&#123;OPENCV_APPLE_BUNDLE_NAME&#125;&lt;/string&gt;&lt;key&gt;CFBundleIdentifier&lt;/key&gt;&lt;string&gt;$&#123;OPENCV_APPLE_BUNDLE_ID&#125;&lt;/string&gt;&lt;key&gt;CFBundleVersion&lt;/key&gt;&lt;string&gt;$&#123;OPENCV_LIBVERSION&#125;&lt;/string&gt;&lt;key&gt;CFBundleShortVersionString&lt;/key&gt;&lt;string&gt;$&#123;OPENCV_LIBVERSION&#125;&lt;/string&gt;&lt;key&gt;CFBundleSignature&lt;/key&gt;&lt;string&gt;????&lt;/string&gt;&lt;key&gt;CFBundlePackageType&lt;/key&gt;&lt;string&gt;FMWK&lt;/string&gt;&lt;/dict&gt;&lt;/plist&gt; 其中，NSCameraUsageDescription这部分就是打开相机的许可。 但是这个文件直接放在项目里是不行的，需要复制下来，打开products的路径，然后复制到这个路径里面，才可以成功的打开相机]]></content>
      <categories>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>Xcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于SVM的理解]]></title>
    <url>%2F2019%2F04%2F03%2F%E5%85%B3%E4%BA%8ESVM%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本文参考支持向量机通俗导论内容 SVM到底是啥support vecttor machine，比如在二维平面上，要把一个东西分成两类，SVM就是平面上的一条直线，并且在这两类的正中间，离两边一样远。换句话说，学习策略是把间隔最大化，从而得到凸二次规划问题的解（虽然不是很理解，但是凸问题应该是比较好求解） 分类标准 logistic regression 线性分类器：x表示数据，y表示类别，分类器则需要在n维找到一个超平面hyper plane，超平面的方程就是W.T 也就是 W.T.dot(x) + b = 0 (令人震惊w居然是超平面的方程) 逻辑回归 逻辑回归就是从特征里面学到一个0/1的分类模型 模型的线性组合作为自变量，取值范围是负无穷到正无穷，所以使用logistic函数（竟然就是simoid函数把他们投影到（0，1）上面，得到的值就是y = 1的概率 线性分类器如果把分类的两类改成 -1和1（只是为了方便选了这个数字），其实就是把wx加了b 这时候的点的位置可以用 f(x) = wx + b表示，如果f(x)等于0，那么这个点在超平面上，如果大于0就是在1的类型里，小于0在-1的类型里 这时候问题变成了寻找间隔最大的超平面 function margin，geometrical margin函数距离 当平面上的点是 wx+b = 0 确定了以后， wx+b的绝对值就是点x到超平面的距离 同时 wx+b 的符号和 y（分类标签）的符号对比，如果一致的话是一个类别，不一致的话是另一个 -&gt; y(wx+ b)的正负来表示分类的正确与否 （也就是两个东西同号得正分类正确） 引出函数间隔的定义（这里的y是乘上对应类别的y，所以能得到绝对值） 在训练集中，所有点到超平面的距离的最小点就是function margin 几何距离 但是如果单纯这么评定，当w和b成比例改变的时候，函数间隔也会改变，所以还需要几何间隔上面的式子乘以y（对应类别的标签）就可以得到绝对值了。 也就是说几何margin的主要部分就是把之前的内容除了一个w的范数，变成了标准化之后的长度 最大间隔分类器 max margin classifier对于一组数据来说，超平面和数据点的距离越大，这个数据的分类确信度（confidence）就越高 最大的间距的目标函数即： max\gama， 其中gama是比所有其他间隔都短的函数间隔 如果让最小的函数间隔等于1（为了方便计算），然后求几何间隔，可以得知需要的目标函数变为最大化 1/||w||，其中w是超平面 深入SVM线性可分和不可分原始问题和对偶问题duality 之前的目标函数是 1/||w||，所以求这个的最大值，就是求1/2*||w||^2的最小值（这里求最大值就是求倒数的最小值，然后1/2和平方都是为了方便加的） 目标函数变成二次的，约束条件是线性的，凸二次问题，可以用QP（一个写的差不多的包） -&gt; 目标最优的时候loss 由于这个问题的结构，可以转换成对偶问题求解 给每一个约束条件加上一个拉格朗日乘子 alpha 把这个融合进入目标函数里面 当所有的约束条件都满足的时候，目标函数的结果就是之前需要求的目标函数。 再对这个目标函数（新的）求最小值，得到的结果就是本来需要求的最小值 最后，因为上面的问题不是很好求解，把它的max和min交换了一下，先求所有的间隔的最小值，然后再求这里面alpha条件可以满足的最大值，这两个问题就是对偶问题 d &lt;= p ，在某些条件满足的情况下这两个值相等，这时候求出来对偶问题就可以求出来原始问题的解 转换对偶问题的原因： 对偶问题更容易求解 可以引入核函数，这样可以直接引入非线性问题 K.K.T条件 上一段说的，满足对偶问题的解等价的条件就是KKT条件 KTT条件的意义：非线性规划问题（nonlinear processing）能有最优化解法的充要条件 这部分没有写证明，但是上面的求最值的问题可以被证明是满足KKT条件的问题，所以可以用解决对偶问题的方式来求解。 对偶问题的求解步骤参考内容 https://www.zhihu.com/question/21094489 https://blog.csdn.net/v_JULY_v/article/details/7624837]]></content>
      <categories>
        <category>图像处理</category>
        <category>Machine Learning</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>分类器</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment1之Image Features]]></title>
    <url>%2F2019%2F04%2F03%2FCS231Nassignment1Feature%2F</url>
    <content type="text"><![CDATA[目标 之前实现的都是写好了一个linear classifier然后直接对输入图片的raw pixel进行分类 这部分是先从raw data得到相应的图片特征，然后再对特征进行分类 前面的简单的setup和load data都和之前的一样。 Extract Features 对每张图片计算HOG以及在HSV的color space上面的hue channel。（这是两个不同的功能） HOG可以提取图片的texture的特征，忽略颜色的影响。而颜色的histogram表示的是颜色而忽略texture，颜色的特征会拉成一个新的vector然后进行分类。 如果我们把这两个东西结合可能会有更好的结果。 在这部分的代码里面，直接给出来了提取hog feature和color histogram的两个function，用这两个直接提取出了特征然后构成了一个新的extract_features，由图片内容和特征组成。 然后预处理了特征，减去平均值，除以std（这样大家都在同一个scale里面），最后加上了一个bias的dim 12345678910111213141516171819202122232425from cs231n.features import *num_color_bins = 10 # Number of bins in the color histogramfeature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]X_train_feats = extract_features(X_train, feature_fns, verbose=True)X_val_feats = extract_features(X_val, feature_fns)X_test_feats = extract_features(X_test, feature_fns)# Preprocessing: Subtract the mean featuremean_feat = np.mean(X_train_feats, axis=0, keepdims=True)X_train_feats -= mean_featX_val_feats -= mean_featX_test_feats -= mean_feat# Preprocessing: Divide by standard deviation. This ensures that each feature# has roughly the same scale.std_feat = np.std(X_train_feats, axis=0, keepdims=True)X_train_feats /= std_featX_val_feats /= std_featX_test_feats /= std_feat# Preprocessing: Add a bias dimensionX_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))]) 训练SVM来处理features用处理多个类别的SVM来给这些特征分类，得到的结果应该比直接分类得到的结果好。大概结果为0.44左右，注意这里面用的是grid search而不是random search 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# Use the validation set to tune the learning rate and regularization strengthfrom cs231n.classifiers.linear_classifier import LinearSVMlearning_rates = [1e-9, 1e-8, 1e-7]regularization_strengths = [5e4, 5e5, 5e6]results = &#123;&#125;best_val = -1best_svm = None################################################################################# TODO: ## Use the validation set to set the learning rate and regularization strength. ## This should be identical to the validation that you did for the SVM; save ## the best trained classifer in best_svm. You might also want to play ## with different numbers of bins in the color histogram. If you are careful ## you should be able to get accuracy of near 0.44 on the validation set. #################################################################################for lr in learning_rates: for rs in regularization_strengths: svm = LinearSVM() svm.train(X_train_feats, y_train, learning_rate = lr, reg = rs, num_iters = 1000, verbose = True) y_pred_val = svm.predict(X_val_feats) y_pred_train = svm.predict(X_train_feats) train_acc = np.mean(y_pred_train) val_acc = np.mean(y_pred_val == y_val) results[(lr, rs)] = (train_acc,val_acc) if val_acc &gt; best_val: best_val = val_acc best_svm = svm################################################################################# END OF YOUR CODE ################################################################################## Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val) 同时也可视化了不是这个类别却被分到这个类别的错误sample：123456789101112131415161718# An important way to gain intuition about how an algorithm works is to# visualize the mistakes that it makes. In this visualization, we show examples# of images that are misclassified by our current system. The first column# shows images that our system labeled as "plane" but whose true label is# something other than "plane".examples_per_class = 8classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for cls, cls_name in enumerate(classes): idxs = np.where((y_test != cls) &amp; (y_test_pred == cls))[0] idxs = np.random.choice(idxs, examples_per_class, replace=False) for i, idx in enumerate(idxs): plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1) plt.imshow(X_test[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls_name)plt.show() （感觉自己训练了一个傻子） 用两层的nerual net试试看 首先去除上文中bias的dim 然后交叉训练，找到最好的参数 这部分半天loss下不去的原因主要是lr选的太小了 1234567891011121314151617181920212223242526272829303132333435363738394041424344from cs231n.classifiers.neural_net import TwoLayerNetinput_dim = X_train_feats.shape[1]hidden_dim = 500num_classes = 10hidden_size = [300,400,500,600]learning_rate = [1,1e-1,1e-2]reg = [1e-4,1e-3,1e-2]# net = TwoLayerNet(input_dim, hidden_dim, num_classes)best_net = Nonebest_acc = -1result = &#123;&#125;################################################################################# TODO: Train a two-layer neural network on image features. You may want to ## cross-validate various parameters as in previous sections. Store your best ## model in the best_net variable. #################################################################################for lr in learning_rate: for hidd in hidden_size: for rs in reg: net = TwoLayerNet(input_size, hidden, num_class) status = net.train(X_train_feats, y_train, X_val_feats, y_val, num_iters=1200, batch_size=400, learning_rate=lr, learning_rate_decay=0.95, reg=rs, verbose= True) val_acc = (net.predict(X_val_feats) == y_val).mean() result[(lr, rs, hidd)] = (val_acc) if val_acc &gt; best_acc: best_acc = val_acc best_net = net# print(result)# for lr, rs, hidd in sorted(result):# val_accuracy = result[(lr, rs, hidd)]# print('lr %e reg %e hidden_units %e val accuracy: %f' % (# lr, rs, hidd , val_accuracy))print('best validation accuracy achieved during cross-validation: %f' % best_acc)print('best parameter is :',list (result.keys()) [list (result.values()).index (best_acc)])################################################################################# END OF YOUR CODE ################################################################################# best validation accuracy achieved during cross-validation: 0.605000best parameter is : (1, 0.0001, 500) 一点感觉 感觉要是lr太小的话，即使增加iteration的次数，后面的改变也不大 lr最基础的范围应该先定下来 最后换了换参数居然训出来了60%的val正确率 test的正确率在55.8左右]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>Image Feature</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment1之two_layer_net]]></title>
    <url>%2F2019%2F04%2F02%2FCS231Nassignment1twolayernet%2F</url>
    <content type="text"><![CDATA[目标 Implement a neural network with fc layers for classifiction Test it on CIFAR-10 dataset 初始化auto-reloading external modules 定义relative error123def rel_error(x, y): """ returns relative error """ return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y)))) 这里插入一下np.max和np.maximum的区别 max是求序列的最值，可以输入一个参数，axis表示的是求最值的方向 maximum至少输入两个参数，会把两个参数逐位比较，然后输出比较大的那个结果 但是好像在这里的使用上面，说明x和y不是一个单独的值，应该是两个数组 12345&gt;&gt; np.max([-4, -3, 0, 0, 9])9&gt;&gt; np.maximum([-3, -2, 0, 1, 2], 0)array([0, 0, 0, 1, 2]) 不是很理解这里为什么要除以x+y 设置参数cs231n/classifiers/neural_net.pyself.params储存了需要的参数，参数都被存储在dict里面，一个名字对应一个内容 两层神经网络的参数如下： W1，第一层的weights，（D，H），其中H是第二层的neruon的个数。因为只有一层的时候，D个输入对应C个输出，现在有两层的fc，对应的输出就是第二层的units个数 b1，第一层的bias，（H，） W2，第二层的weights，（H，C） b2，第二层的biasbias都需要初始化为相应大小的0，weights初始化成0-1之间的比较小的数字 Forward pasa计算scores 这部分非常简单，两次Wx+b，并且在第一次之后记得激活就可以了 激活函数用的relu，内容就是score小于0的部分让他直接等于0 计算loss 这里用的是softmax计算loss，和softmax的作业内容一样，将所有的scores exp，求占的百分比，求出来的部分-log，然后把所有的求和 这里用到了boardcasting的问题，注意（100，1）这样的才可以boardcasting，（100，）的是一维数组，需要把它reshape成前面的样子才可以 这里最后的结果还总是差一点，最后发现是因为regularzation的时候多乘了0.5，看题呜呜呜 Backward pass 由于b是线性模型的bias，偏导数是1，直接对class的内容求和然后除以N就是最终结果 对W求导的时候需要用到链式法则，然后直接代码实现一下就行了 这里遇到的主要问题是loss的值会影响他估计的值，因为loss的regularzation改了，所以答案一直对不上。 Training predict 训练和之前写的差不多，训练网络，主要包括写training部分的随机mini-batch和更新weights，记得lr更新的时候要带负号 预测也差不多，算出来scores，找到最大的score就是分类的结果。注意找最大的时候要用argmax，找到的是最大的东西的indice，不然得到的是得分12345678910111213net = init_toy_model()stats = net.train(X, y, X, y, learning_rate=1e-1, reg=5e-6, num_iters=100, verbose=False)print('Final training loss: ', stats['loss_history'][-1])# plot the loss historyplt.plot(stats['loss_history'])plt.xlabel('iteration')plt.ylabel('training loss')plt.title('Training Loss history')plt.show() 使用写好的来训练CIFAR-101234567891011121314input_size = 32 * 32 * 3hidden_size = 50num_classes = 10net = TwoLayerNet(input_size, hidden_size, num_classes)# Train the networkstats = net.train(X_train, y_train, X_val, y_val, num_iters=1000, batch_size=200, learning_rate=1e-4, learning_rate_decay=0.95, reg=0.25, verbose=True)# Predict on the validation setval_acc = (net.predict(X_val) == y_val).mean()print('Validation accuracy: ', val_acc) 这时候得到的准确度应该在28%左右，可以优化 进一步优化 一种可视化的方法是可视化loss function和准确率的关系，分别在训练和val集上面 另种是可视化第一层的weights 两种方法的结果如下： debug模型 问题 loss大体上都是linearly的下降的，说明lr可能太低了 在training和val的准确率上没有gap，说明model的容量太小的，需要增大size 如果容量过大还会导致overfiiting，这时候gap就会很大 tuning hypers 题目里面的建议是tuning几个hyper，还是和之前一样，直接random，search 这里选了三个参数，分别是units的数量，learning rate和reg的强度，随便设置了一下界限 最终计算出来的val准确率是：49.5% 秀秀秀！！ 可视化weigh之后的结果是 震惊，居然最后的测试正确率也达到了49.4！！！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051best_net = None # store the best model into this ################################################################################## TODO: Tune hyperparameters using the validation set. Store your best trained ## model in best_net. ## ## To help debug your network, it may help to use visualizations similar to the ## ones we used above; these visualizations will have significant qualitative ## differences from the ones we saw above for the poorly tuned network. ## ## Tweaking hyperparameters by hand can be fun, but you might find it useful to ## write code to sweep through possible combinations of hyperparameters ## automatically like we did on the previous exercises. ##################################################################################best_acc = -1learning_rates = [1e-3, 1e-2]regularization_strengths = [1e-2, 6e-1]hidden_size = [50, 150]random_search = np.random.rand(30, 3)random_search[:, 0] = random_search[:, 0] * \ (learning_rates[1] - learning_rates[0]) + learning_rates[0]random_search[:, 1] = random_search[:, 1] * \ (regularization_strengths[1] - regularization_strengths[0] ) + regularization_strengths[0]random_search[:, 2] = random_search[:, 2] * \ (hidden_size[1] - hidden_size[0]) + hidden_size[0]for lr, rs, hidd in random_search: input_size = 32 * 32 * 3 hidden = int(hidd) num_class = 10 net = TwoLayerNet(input_size, hidden, num_class) status = net.train(X_train, y_train, X_val, y_val, num_iters=1000, batch_size=200, learning_rate=lr, learning_rate_decay=0.95, reg=rs, verbose=True) val_acc = (net.predict(X_val) == y_val).mean() if val_acc &gt; best_acc: best_acc = val_acc best_net = netprint("best net is with val acc", best_acc)################################################################################## END OF YOUR CODE ################################################################################## 代码部分nerual_net.py部分的完整代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289from __future__ import print_functionimport numpy as npimport matplotlib.pyplot as pltclass TwoLayerNet(object): """ A two-layer fully-connected neural network. The net has an input dimension of N, a hidden layer dimension of H, and performs classification over C classes. We train the network with a softmax loss function and L2 regularization on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture: input - fully connected layer - ReLU - fully connected layer - softmax The outputs of the second fully-connected layer are the scores for each class. """ def __init__(self, input_size, hidden_size, output_size, std=1e-4): """ Initialize the model. Weights are initialized to small random values and biases are initialized to zero. Weights and biases are stored in the variable self.params, which is a dictionary with the following keys: W1: First layer weights; has shape (D, H) b1: First layer biases; has shape (H,) W2: Second layer weights; has shape (H, C) b2: Second layer biases; has shape (C,) Inputs: - input_size: The dimension D of the input data. - hidden_size: The number of neurons H in the hidden layer. - output_size: The number of classes C. """ self.params = &#123;&#125; self.params['W1'] = std * np.random.randn(input_size, hidden_size) self.params['b1'] = np.zeros(hidden_size) self.params['W2'] = std * np.random.randn(hidden_size, output_size) self.params['b2'] = np.zeros(output_size) def loss(self, X, y=None, reg=0.0): """ Compute the loss and gradients for a two layer fully connected neural network. Inputs: - X: Input data of shape (N, D). Each X[i] is a training sample. - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it is not passed then we only return scores, and if it is passed then we instead return the loss and gradients. - reg: Regularization strength. Returns: If y is None, return a matrix scores of shape (N, C) where scores[i, c] is the score for class c on input X[i]. If y is not None, instead return a tuple of: - loss: Loss (data loss and regularization loss) for this batch of training samples. - grads: Dictionary mapping parameter names to gradients of those parameters with respect to the loss function; has the same keys as self.params. """ # Unpack variables from the params dictionary W1, b1 = self.params['W1'], self.params['b1'] W2, b2 = self.params['W2'], self.params['b2'] N, D = X.shape # Compute the forward pass scores = None ############################################################################# # TODO: Perform the forward pass, computing the class scores for the input. # # Store the result in the scores variable, which should be an array of # # shape (N, C). # ############################################################################# # first layer, shape(N,H) X1 = X.dot(W1) + b1 # 这里加了一个第一层之后的relu激活 relu = np.maximum(0, X1) # final result, shape(N,C) scores = relu.dot(W2) + b2 ############################################################################# # END OF YOUR CODE # ############################################################################# # If the targets are not given then jump out, we're done if y is None: return scores # Compute the loss loss = None ############################################################################# # TODO: Finish the forward pass, and compute the loss. This should include # # both the data loss and L2 regularization for W1 and W2. Store the result # # in the variable loss, which should be a scalar. Use the Softmax # # classifier loss. # ############################################################################# num_train = N scores = scores - np.reshape(np.max(scores, axis=1), (num_train, -1)) scores = np.exp(scores) scores_sum = np.sum(scores, axis=1).reshape(N, 1) # scores_sum = np.sum(scores, axis=1, keepdims=True) p = scores / scores_sum loss = np.sum(-np.log(p[np.arange(N), y])) loss /= num_train # 这里不要乘0.5的系数 # loss += reg * np.sum(W1 * W1) + reg * np.sum(W2 * W2) loss += 0.5 * reg * np.sum(W1 * W1) + 0.5 * reg * np.sum(W2 * W2) # ############################################################################# # # END OF YOUR CODE # # ############################################################################# # # Backward pass: compute gradients grads = &#123;&#125; # ############################################################################# # # TODO: Compute the backward pass, computing the derivatives of the weights # # # and biases. Store the results in the grads dictionary. For example, # # # grads['W1'] should store the gradient on W1, and be a matrix of same size # # ############################################################################# dscores = p dscores[range(N), y] -= 1.0 # dscores /= N # shape dW2(CxN) x(NxH) -&gt; (CxH) # dW2 = np.dot(relu.T, p) dW2 = np.dot(relu.T, dscores) # print(dW2) # 每个class会有一个b，对b求导是1 # shape db2 (C,) db2 = np.sum(p, axis=0) # (NxC) x (HxC).T -&gt; (N,H) dW_relu = np.dot(dscores, W2.T) dW_relu[relu &lt;= 0] = 0 # (NxD).T x (N,H) -&gt; (D,H) dW1 = (X.T).dot(dW_relu) db1 = np.sum(dW_relu, axis=0) dW2 /= N dW1 /= N dW2 += reg * W2 dW1 += reg * W1 db1 /= N db2 /= N grads['W1'] = dW1 grads['b1'] = db1 grads['W2'] = dW2 grads['b2'] = db2 ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, grads def train(self, X, y, X_val, y_val, learning_rate=1e-3, learning_rate_decay=0.95, reg=5e-6, num_iters=100, batch_size=200, verbose=False): """ Train this neural network using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) giving training data. - y: A numpy array f shape (N,) giving training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - X_val: A numpy array of shape (N_val, D) giving validation data. - y_val: A numpy array of shape (N_val,) giving validation labels. - learning_rate: Scalar giving learning rate for optimization. - learning_rate_decay: Scalar giving factor used to decay the learning rate after each epoch. - reg: Scalar giving regularization strength. - num_iters: Number of steps to take when optimizing. - batch_size: Number of training examples to use per step. - verbose: boolean; if true print progress during optimization. """ num_train = X.shape[0] iterations_per_epoch = max(num_train / batch_size, 1) # Use SGD to optimize the parameters in self.model loss_history = [] train_acc_history = [] val_acc_history = [] for it in range(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: Create a random minibatch of training data and labels, storing # # them in X_batch and y_batch respectively. # ######################################################################### rand_mini = np.random.choice(num_train, batch_size, replace=True) X_batch = X[rand_mini] y_batch = y[rand_mini] ######################################################################### # END OF YOUR CODE # ######################################################################### # Compute loss and gradients using the current minibatch loss, grads = self.loss(X_batch, y=y_batch, reg=reg) loss_history.append(loss) ######################################################################### # TODO: Use the gradients in the grads dictionary to update the # # parameters of the network (stored in the dictionary self.params) # # using stochastic gradient descent. You'll need to use the gradients # # stored in the grads dictionary defined above. # ######################################################################### self.params['W1'] -= learning_rate * grads['W1'] self.params['W2'] -= learning_rate * grads['W2'] self.params['b1'] -= learning_rate * grads['b1'] self.params['b2'] -= learning_rate * grads['b2'] ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print('iteration %d / %d: loss %f' % (it, num_iters, loss)) # Every epoch, check train and val accuracy and decay learning rate. if it % iterations_per_epoch == 0: # Check accuracy train_acc = (self.predict(X_batch) == y_batch).mean() val_acc = (self.predict(X_val) == y_val).mean() train_acc_history.append(train_acc) val_acc_history.append(val_acc) # Decay learning rate learning_rate *= learning_rate_decay return &#123; 'loss_history': loss_history, 'train_acc_history': train_acc_history, 'val_acc_history': val_acc_history, &#125; def predict(self, X): """ Use the trained weights of this two-layer network to predict labels for data points. For each data point we predict scores for each of the C classes, and assign each data point to the class with the highest score. Inputs: - X: A numpy array of shape (N, D) giving N D-dimensional data points to classify. Returns: - y_pred: A numpy array of shape (N,) giving predicted labels for each of the elements of X. For all i, y_pred[i] = c means that X[i] is predicted to have class c, where 0 &lt;= c &lt; C. """ y_pred = None ########################################################################### # TODO: Implement this function; it should be VERY simple! # ########################################################################### W1 = self.params['W1'] W2 = self.params['W2'] b1 = self.params['b1'] b2 = self.params['b2'] scores = X.dot(W1) + b1 scores[scores &lt; 0] = 0.0 scores = scores.dot(W2) + b2 y_pred = np.argmax(scores, axis=1) ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于Normalization的方法以及实现]]></title>
    <url>%2F2019%2F04%2F01%2FNormalize%2F</url>
    <content type="text"><![CDATA[在处理数据的时候，因为数据的大小差别会比较大，为了避免数据的特征被其他特征吃掉，需要对数据进行normalization的处理 (0,1) 标准化找到最大值和最小值，以最大值为1，最小值为0，计算其他数据在0到1之间的分布。 12def normal0_1(x,Max,Min): return (x-Min)/(Max-Min) 使用np.max()，np.min来找最大值和最小值 正态分布输入原始数据的均值和标准差，对数据处理，处理之后的数据是标准正态分布（均值是0，标准差是1） 12def Normalization(x, mu, sigma): return (x-mu) / sigma 使用np.average()和np.std()找到均值和标准差 Sigmoid函数sigmoid函数关于（0， 0.5）中心对称，在中心附近斜率较大，在负无穷接近0，正无穷接近1 12def sigmood(x): return 1.0/(1+np.exp(-float(x)))]]></content>
      <categories>
        <category>数学问题</category>
      </categories>
      <tags>
        <tag>Normalize</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231N作业assignment1之softmax]]></title>
    <url>%2F2019%2F04%2F01%2FCS231Nassignment1softmax%2F</url>
    <content type="text"><![CDATA[Softmax这部分主要是softmax的loss要如何计算Assignment From: Assignment1 目标 implement a fully-vectorized loss function for the Softmax classifier implement the fully-vectorized expression for its analytic gradient check your implementation with numerical gradient use a validation set to tune the learning rate and regularization strength optimize the loss function with SGD visualize the final learned weights 预处理（和之前一样（ 载入数据 初始化数据 拉长 normalize 分成训练集测试集validation等等 softmax classifiernaive_softmax_loss中心思想：把得到的score（Wx + b）先exp，然后normalize，最后求-log 输入： W：大小(D,C)，weights X：大小(N,D)，输入的mini-batch y：大小(N,)，标签 reg：regularization的系数 输出： loss dW，即改变的gradient 计算loss 先将所有的scores做exp（这一步可以先进行），这样所有的score都会变成正数 然后对不同class的score分别求normalize（虽然说是normalize，实际求的是这个种类的score在所有的score里面所占的比例） 然后将正确的类型所占的比例求log，再求负号，得出来的就是每个图片的loss（这里注意0的log是无穷，计算不出来） 所有图片的loss求和，然后除以图片总数，regularzation，得出来的就是最终的结果 计算dW 可以这样理解 W是一个参数矩阵，这个矩阵的变化由两个部分组成 第一部分是往什么方向变，这个取决于最后算出来的loss的分布 第二部分是变多少合适，这时候还需要乘一个系数X[i] 所以当算出来loss并且y[i] = j的时候，实际上就是这张图正确分类情况下的错误分类的概率，所以W的改变方向应该是这个的反方向 这张图的其他class的loss则应该是改变的方向 这样就可以看出来 SVM和softmax的不同之处了 对于SVM来说，仅仅通过与0比大小得出一个值，相当于一个0，1的开关，只能根据结果得到一个移动的方向 但是对于softmax来说，不仅得到了方向，还得到了这个方向的占比，所以loss越大的数影响就会越大 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def svm_loss_naive(W, X, y, reg): """ Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] # 有多少需要训练的个数 num_train = X.shape[0] loss = 0.0 for i in range(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in range(num_classes): # 如果这个类型是正确的，那就不用管了 if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:, y[i]] += -X[i] dW[:, j] += X[i] # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. # reg是lanbda loss += reg * np.sum(W * W) dW += 2 * reg * W ############################################################################# # TODO: # # Compute the gradient of the loss function and store it dW. # # Rather that first computing the loss and then computing the derivative, # # it may be simpler to compute the derivative at the same time that the # # loss is being computed. As a result you may need to modify some of the # # code above to compute the gradient. # ############################################################################# return loss, dW softmax_loss_vectorized提高计算速度 跟svm部分的计算思路一样，直接使用矩阵运算 在求整个score矩阵的变化的时候，正确分类的loss应该被减掉，但是现在是被加上的，所以需要在正确分类的地方加一个-1 debug了很久的地方是：计算dW的时候不需要计算log，因为没有log之前已经是这个loss所占的百分比了：求log是为了变成凸函数，loss没有求log之前并不是凸函数，但是凸函数容易找到最值的优化问问题，所以要求log。但是在计算dW的时候和log没关系 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def softmax_loss_vectorized(W, X, y, reg): """ Softmax loss function, vectorized version. Inputs and outputs are the same as softmax_loss_naive. """ # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) num_class = W.shape[1] num_train = X.shape[0] ############################################################################# # TODO: Compute the softmax loss and its gradient using no explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# # size（N，C） scores = X.dot(W) scores = np.exp(scores) # 对每行求和 scores_sum = np.sum(scores, axis=1) scores_sum = np.repeat(scores_sum, num_class) scores_sum = scores_sum.reshape(num_train, num_class) # true_divide返回浮点数，普通的返回正数，size（N，C） percent = np.true_divide(scores, scores_sum) # 只有正确种类需要求loss Li = -np.log(percent[np.arange(num_train), y]) loss = np.sum(Li) # 注意这里不需要求log dS = percent.copy() dS[np.arange(num_train), y] += -1 dW = (X.T).dot(dS) loss /= num_train loss += reg * np.sum(W * W) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW 验证，选hyper和SVM的部分一样，随机搜索hyper，验证结果，训练迭代500次，最终的准确率在36%左右1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of over 0.35 on the validation set.from cs231n.classifiers import Softmaxresults = &#123;&#125;best_val = -1best_softmax = Nonelearning_rates = [1e-7, 5e-7]regularization_strengths = [2.5e4, 5e4]################################################################################# TODO: ## Use the validation set to set the learning rate and regularization strength. ## This should be identical to the validation that you did for the SVM; save ## the best trained softmax classifer in best_softmax. #################################################################################hyper_values = np.random.rand(50,2)hyper_values[:,0] = (learning_rates[1] - learning_rates[0]) * hyper_values[:,0] + learning_rates[0]hyper_values[:,1] = (regularization_strengths[1] - regularization_strengths[0]) * hyper_values[:,1] + regularization_strengths[0]for lr, rs in hyper_values: softmax = Softmax() softmax.train(X_train,y_train,lr,rs,num_iters = 500,verbose = True) train_pred = softmax.predict(X_train) train_acc = np.mean(y_train == train_pred) val_pred = softmax.predict(X_val) val_acc = np.mean(y_val == val_pred) results[(lr,rs)] = (train_acc,val_acc) if val_acc &gt; best_val: best_val = val_acc best_softmax = softmax################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val) 可以看出来感觉softmax比SVM的效果好一些？可视化最终的优化的weight123456789101112131415# Visualize the learned weights for each classw = best_softmax.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in range(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i])]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于标量，向量，矩阵求导]]></title>
    <url>%2F2019%2F03%2F30%2F%E5%85%B3%E4%BA%8E%E6%A0%87%E9%87%8F%E5%90%91%E9%87%8F%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[参考文章：https://blog.csdn.net/u010976453/article/details/54381248 关于layout在求导的时候有，因为分子和分母可能的维度不太一样，所以两种不同的布局，分别是分子布局和分母布局假设y（向量）对x（标量）求导： 分子布局，即和原来的y相同 分母布局，为分子布局的tranpose 对标量的导数scalar对scalar求导即最简单的求导 vector对scalar求导比如一个列向量y，对x求导，结果是y里面的每个值都对x求导 matrix对scalr求导矩阵里面的每个值都对x求导 对向量的导数scalar对vector 标量y和向量x，求出来的结果是y对每个x(x1,x2 ….xn)求导 结果为梯度向量，是标量y在空间Rn的梯度，空间以x为基 注意，x是列向量的话，最后求出来的是行的结果 vector对vectory = [y1,y2 …. ym]x = [x1,x2 …. xn]最后求出来的结果是一个m行n列的矩阵，jacobian矩阵 matrix对vector矩阵y =[[y11,y12…y1n],[y21,y22 …y2n],…[yn1,yn2 …ynn]]向量x = [x1,x2…xn]T最终的结果是每一行分别对这个x的向量求导，所以矩阵的列数和向量的行数应该先通 对于矩阵一般只考虑标量对矩阵(剩下的情况和上面类似)最终结果是这个标量对所有的矩阵内容求导，求出来的是梯度矩阵]]></content>
      <categories>
        <category>数学问题</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231N作业assignment1之SVM部分]]></title>
    <url>%2F2019%2F03%2F29%2FCS231Nassignment1SVM%2F</url>
    <content type="text"><![CDATA[Assignment from: http: // cs231n.github.io / assignments2018 / assignment1/ 目标： a fully - vectorized loss function for the SVM fully - vectorized expression for its analytic gradient use a validation set to tune the learning rate and regularization strength optimize the loss function with SGD visualize the final learned weights Set up部分1234567891011121314151617181920# Run some setup code for this notebook.from __future__ import print_functionimport randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt# This is a bit of magic to make matplotlib figures appear inline in the# notebook rather than in a new window.%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# Some more magic so that the notebook will reload external python modules;# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2 读取CIFAR-10的数据，预处理123456789101112131415161718# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)try: del X_train, y_train del X_test, y_test print('Clear previously loaded data.')except: passX_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print('Training data shape: ', X_train.shape)print('Training labels shape: ', y_train.shape)print('Test data shape: ', X_test.shape)print('Test labels shape: ', y_test.shape) 结果：1234Training data shape: (50000, 32, 32, 3)Training labels shape: (50000,)Test data shape: (10000, 32, 32, 3)Test labels shape: (10000,) 可视化dataset 从类型中1234567891011121314151617# Visualize some examples from the dataset.# We show a few examples of training images from each class.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']num_classes = len(classes)samples_per_class = 7for y, cls in enumerate(classes): idxs = np.flatnonzero(y_train == y) idxs = np.random.choice(idxs, samples_per_class, replace=False) for i, idx in enumerate(idxs): plt_idx = i * num_classes + y + 1 plt.subplot(samples_per_class, num_classes, plt_idx) plt.imshow(X_train[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls)plt.show() 1np.flatnonzero(y_train == y) 返回内容非0的index。这句是返回plane类别里面的（y_train == y）所有非0的内容。然后从这些里面随机选择7个内容，画出来。 结果如下： 进一步分为几部分123456789101112131415161718192021222324252627282930313233343536373839# Split the data into train, val, and test sets. In addition we will# create a small development set as a subset of the training data;# we can use this for development so our code runs faster.num_training = 49000num_validation = 1000num_test = 1000# 用这部分来优化代码num_dev = 500# Our validation set will be num_validation points from the original# training set.mask = range(num_training, num_training + num_validation)X_val = X_train[mask]y_val = y_train[mask]# Our training set will be the first num_train points from the original# training set.mask = range(num_training)X_train = X_train[mask]y_train = y_train[mask]# We will also make a development set, which is a small subset of# the training set.mask = np.random.choice(num_training, num_dev, replace=False)X_dev = X_train[mask]y_dev = y_train[mask]# We use the first num_test points of the original test set as our# test set.mask = range(num_test)X_test = X_test[mask]y_test = y_test[mask]print('Train data shape: ', X_train.shape)print('Train labels shape: ', y_train.shape)print('Validation data shape: ', X_val.shape)print('Validation labels shape: ', y_val.shape)print('Test data shape: ', X_test.shape)print('Test labels shape: ', y_test.shape) 12mask = range(num_test)X_test = X_test[mask] 感觉这是一种从一个整体中选取其中一部分的代码 将image拉成row1234567891011# Preprocessing: reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1))X_val = np.reshape(X_val, (X_val.shape[0], -1))X_test = np.reshape(X_test, (X_test.shape[0], -1))X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))# As a sanity check, print out the shapes of the dataprint('Training data shape: ', X_train.shape)print('Validation data shape: ', X_val.shape)print('Test data shape: ', X_test.shape)print('dev data shape: ', X_dev.shape) 当想把无论任何大小的东西拉成一整行的时候，用a.reshape(x, -1)。 X_train.shape[0]行，列数未知，但是拉平了 如果想拉成一整列的时候，用a.reshape(-1, x)。 列数为x，每列有多少东西未知 预处理部分：减去mean image 第一步，求出训练集的mean并且可视化 12345678# Preprocessing: subtract the mean image# first: compute the image mean based on the training datamean_image = np.mean(X_train, axis=0)print(mean_image[:10]) # print a few of the elementsplt.figure(figsize=(4, 4))plt.imshow(mean_image.reshape((32, 32, 3)).astype( 'uint8')) # visualize the mean imageplt.show() 第二步，从train和test里面减去平均数据 12345# second: subtract the mean image from train and test dataX_train -= mean_imageX_val -= mean_imageX_test -= mean_imageX_dev -= mean_image 第三步，把预处理好的所有图片的末尾（拉成行之后的最后）加了一个1（bias的dim） 12345678# third: append the bias dimension of ones (i.e. bias trick) so that our SVM# only has to worry about optimizing a single weight matrix W.X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape) np.hstack()，沿着水平方向把数组叠起来。于此相同，np.vstack()，是沿着垂直方向把数组叠起来。 SVM classifier1cs231n / classifiers / linear_svm.py. svm_loss_naive 有三个输入 X：一个有N个元素的minibatch，每个元素的内容是D(N, D) W: weights，(D, C), 图片的内容是D，一共C个class，所以用的时候跟普遍想法的W是tranpose的 y: 标签，大小(N,) 一共N张照片，每张照片有一个标签 最终结果 一个float的结果：loss W的gradient dW 注意，Wx求出来的就是不同分类的积分 dW的计算(https://blog.csdn.net/zt_1995/article/details/62227201) 形状很奇怪的1(x)指的是，当x为真的时候结果是1，当x为假的时候结果取0 第一个式子表示第i个被正确分类的梯度 有多少个Wj让这个边界值不被满足，就对损失起了多少贡献 乘以xi是因为xi包含了样本的全部特征，所以前面乘以一个系数1就可以了 符号是因为SGD采用负梯度运算 第二个式子表示不正确分类的梯度，只有在yi == j的时候才有贡献，所以没有求和。但是注意，在每张图里面，这个都会在j == yi的时候发生一次，所以每张图的j部分需要加上这个值 最终的结果需要，除以N 别忘了正则化！而且用2\lanmdaW来正则化的效果更好一些 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def svm_loss_naive(W, X, y, reg): """ Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] # 有多少需要训练的个数 num_train = X.shape[0] loss = 0.0 for i in range(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in range(num_classes): # 如果这个类型是正确的，那就不用管了 if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:, y[i]] += -X[i] dW[:, j] += X[i] # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. # reg是lanbda loss += reg * np.sum(W * W) dW += 2 * reg * W return loss, dW svm_loss_vectorized通过向量化来提高计算速度 计算loss部分 W是一个(D, C)的向量，X是(N, D)的，所以两者相乘可以得到一个(N, C)的矩阵，N为图片数量，C是每张图片对于不同分类的score 在score中取每一行的y中label部分就是这张图正确类型的评分 把整体的score矩阵的所有项减去正确评分的矩阵（应该可以广播但是我刚开始用repeat和reshape复制了一下），减去的结果就是svm中需要和0比的值（margin） 为了求loss，把小于0的项目和正确的项除去（都设置成0） 然后行求和，列求和，除以整体的个数，regularzation 计算dW部分 X.T点乘margin得到的就是最终的loss，所以需要把每个margin里面符合条件的数对了 所有比0大的时候都算1（根据导数的计算结果） 当应该判断正确的类型比0大的时候，这个东西会在每次计算导数的时候都算上一次，所以是行的合 最后乘完之后除以总的个数，再regularzation1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation. Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero ############################################################################# # TODO: # # Implement a vectorized version of the structured SVM loss, storing the # # result in loss. # ############################################################################# num_train = X.shape[0] num_classes = W.shape[1] scores = X.dot(W) # 这里是取第N行（图片行）的第C个（class列），得到的是（500，）的正确类的score的矩阵 correct_class_score = scores[np.arange(num_train), y] # correct_class_score = np.repeat(correct_class_score, num_classes) # correct_class_score = correct_class_score.reshape(num_train, num_classes) # DxC margin = scores - correct_class_score + 1.0 margin[np.arange(num_train), y] = 0.0 margin[margin &lt;= 0] = 0.0 loss += np.sum(np.sum(margin, axis=1)) / num_train # loss /= num_train loss += 0.5 * reg * np.sum(W * W) margin[margin &gt; 0] = 1.0 calculate_times = np.sum(margin, axis=1) margin[np.arange(num_train), y] = - calculate_times dW = np.dot(X.T, margin) / num_train dW += 2 * reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW 现在得到了dW和loss，使用SGD来减少loss训练 将整体分成不同的minibatch，使用np.random.choice，注意后面的replce可以选True，这样会重复选择元素但是结果速度好像是更快了 将minibatch的结果计算loss和gradient，然后grad * learning rate来update数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): """ Train this linear classifier using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label 0 &lt;= c &lt; C for C classes. - learning_rate: (float) learning rate for optimization. - reg: (float) regularization strength. - num_iters: (integer) number of steps to take when optimizing - batch_size: (integer) number of training examples to use at each step. - verbose: (boolean) If true, print progress during optimization. Outputs: A list containing the value of the loss function at each training iteration. """ num_train, dim = X.shape # assume y takes values 0...K-1 where K is number of classes num_classes = np.max(y) + 1 if self.W is None: # lazily initialize W self.W = 0.001 * np.random.randn(dim, num_classes) # Run stochastic gradient descent to optimize W loss_history = [] for it in range(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: # # Sample batch_size elements from the training data and their # # corresponding labels to use in this round of gradient descent. # # Store the data in X_batch and their corresponding labels in # # y_batch; after sampling X_batch should have shape (dim, batch_size) # # and y_batch should have shape (batch_size,) # # # # Hint: Use np.random.choice to generate indices. Sampling with # # replacement is faster than sampling without replacement. # ######################################################################### indices = np.random.choice(num_train, batch_size, replace=True) X_batch = X[indices] y_batch = y[indices] ######################################################################### # END OF YOUR CODE # ######################################################################### # evaluate loss and gradient loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) # perform parameter update ######################################################################### # TODO: # # Update the weights using the gradient and the learning rate. # ######################################################################### self.W += - learning_rate * grad ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print('iteration %d / %d: loss %f' % (it, num_iters, loss)) return loss_history 预测结果 已经有了前面的到的训练过的W（self.W） Wx算出来的就是分数 从每一行里面选择最大的分数就是预测的结果123456789101112131415161718192021222324252627def predict(self, X): """ Use the trained weights of this linear classifier to predict labels for data points. Inputs: - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D. Returns: - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional array of length N, and each element is an integer giving the predicted class. """ y_pred = np.zeros(X.shape[0]) ########################################################################### # TODO: # # Implement this method. Store the predicted labels in y_pred. # ########################################################################### scores = X.dot(self.W) y_pred = np.argmax(scores, axis=1) # print(labels.shape) # print(labels) ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred 交叉验证 在作业里，需要选择两个hyper的值，分别是学习率和regularzation的参数，没有采用交叉验证，但是采用了随机搜索，会比grid search更准确一些 采用不同的参数组合分别训练这个模型，然后得到各自在validation上面的准确率，这个得到准确率最大的组合的参数 注意，在验证的过程中应该选择iter的次数少一点，不然训练的时间会非常长 在这个代码里用了rand来得到0到1之间的随机数，这个数乘以hyper的范围的差，然后再加上下限，就是随机得到的最终结果 1234567891011121314rand_turple = np.random.rand(50,2)rand_turple[:,0] = rand_turple[:,0]*(learning_rates[1]-learning_rates[0]) + learning_rates[0]rand_turple[:,1] = rand_turple[:,1]*(regularization_strengths[1]-regularization_strengths[0])+regularization_strengths[0]for lr,rs in rand_turple: svm = LinearSVM() svm.train(X_train, y_train, learning_rate=lr, reg=rs,num_iters=1500, verbose=True) y_train_pred = svm.predict(X_train) train_acc = np.mean(y_train == y_train_pred) y_val_pred = svm.predict(X_train) val_acc = np.mean(y_train == y_val_pred) results[(lr,rs)] = (train_acc,val_acc) if (val_acc &gt; best_val): best_val = val_acc best_svm = svm 结果可视化123456789101112131415# Visualize the learned weights for each class.# Depending on your choice of learning rate and regularization strength, these may# or may not be nice to look at.w = best_svm.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in range(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i])]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>SGD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TypeError:'method' object is not subscriptable]]></title>
    <url>%2F2019%2F03%2F26%2Fmethod%E4%B8%8D%E6%98%AFsubscripatable%2F</url>
    <content type="text"><![CDATA[遭遇问题TypeError: ‘method’ object is not subscriptable是因为我本来写了一个class的method123def get_page(self, num):num = int(num)return self.pages[num] 但是在调用的时候我用了12get_page[i]get_page(i) #这样才是正确的 找到报错改括号就行了！]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[enumerate枚举]]></title>
    <url>%2F2019%2F03%2F25%2Fenumerate%E6%9E%9A%E4%B8%BE%2F</url>
    <content type="text"><![CDATA[enumerate()枚举对可迭代的数据进行标号并将其里面的数据和标号一并打印出来。1enumerate(iterable, start=0) iterable: 可迭代的数据，比如list start: 打印的初始值，默认从0开始打印 123test = [[11], [21], [31], [41]]for i, cnt in enumerate(test):print(i, cnt) 结果为12340 [11]1 [21]2 [31]3 [41]]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python的None和if的理解]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E7%9A%84None%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[python对变量None的判断None是一种数据类型！！！12&gt;&gt;&gt;type(None)&lt;class 'NoneType'&gt; 说明该值是一个空的对象，是Python里面的特殊的值，跟NULL不一样，跟0也不一样 123456a = Noneb = []if a is None or b is None:print("yahaha")else:print("wocao") 结果为“yahaha” 注意：在if的情况下，使用None有时候可以起到很好的作用1if a is None: 与这个差不多的用法是1if not a: 在python里面，None，空列表[]，字典{},tuple()，0等都会被转化成false，剩下的为true比如：12345a = Noneif a:print("yahaha")else:print("wocao") 这时候的输出是wocao，因为a被认为是false]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[np.delete删除数组内容]]></title>
    <url>%2F2019%2F03%2F25%2Fnp-delete%E5%88%A0%E9%99%A4%E6%95%B0%E7%BB%84%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[np.deletenumpy.delete(arr, obj, axis=None) 返回一个新的array，删除掉obj，沿着axis方向 axis : int, optional The axis along which to delete the subarray defined by obj. If axis is None, obj is applied to the flattened array.(如果不加上axis的话会自动把这个array拉平) axis = 0：删除数组的行 axis = 1: 删除数组的列 axis = none: 把整个数组平铺之后按索引删除 123456789101112import numpy as npids = [[3], [34], [5]]ids_o = [[3], [31]]remove_list = filter(lambda i: i not in ids, ids_o)# print(np.asarray(ids))for i in remove_list:index = np.where(np.asarray(ids_o) == i)[0]print("index = ", index)ids = np.delete(ids, index, axis = 0)print("new ids = \n", ids) 结果：1234index = [1]new ids = [[3][5]] 如果把上面改成1ids = np.delete(ids, 0, axis = 1) 即为删除数组的第0列，结果是 [ ] （因为只有一列） 如果改成1ids = np.delete(ids, index, axis = None) 结果为：12new ids = [3 5]]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[np.where查找索引]]></title>
    <url>%2F2019%2F03%2F25%2Fnp-where%E6%9F%A5%E6%89%BE%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[np.wherenp.where(condition, x, y)满足条件(condition)，输出x，不满足输出y。123np.where([[True,False], [True,True]], # 官网上的例子[[1,2], [3,4]],[[9,8], [7,6]]) 输出12array([[1, 8],[3, 4]]) 上面这个例子的条件为[[True,False], [True,False]]，分别对应最后输出结果的四个值。第一个值从[1,9]中选，因为条件为True，所以是选1。第二个值从[2,8]中选，因为条件为False，所以选8，后面以此类推这里的true指的就是选前面的，false就是指选后面的 1234567&gt;&gt;&gt; a = 10&gt;&gt;&gt; np.where([[a &gt; 5,a &lt; 5], [a == 10,a == 7]],[["chosen","not chosen"], ["chosen","not chosen"]],[["not chosen","chosen"], ["not chosen","chosen"]])array([['chosen', 'chosen'],['chosen', 'chosen']], dtype='&lt;U10') np.where(condition)只有条件 (condition)，没有x和y，则输出满足条件 (即非0) 元素的坐标（注意这里返回的是坐标）12345&gt;&gt;&gt; a = np.array([2,4,6,8,10])&gt;&gt;&gt; np.where(a &gt; 5) # 返回索引(array([2, 3, 4]),) &gt;&gt;&gt; a[np.where(a &gt; 5)] # 等价于 a[a&gt;5]array([ 6, 8, 10]) 123456789101112131415161718&gt;&gt;&gt; a = np.arange(27).reshape(3,3,3)&gt;&gt;&gt; aarray([[[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8]],[[ 9, 10, 11],[12, 13, 14],[15, 16, 17]],[[18, 19, 20],[21, 22, 23],[24, 25, 26]]])&gt;&gt;&gt; np.where(a &gt; 5)(array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]),array([2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2]),array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])) 注意这里的最终结果的坐标是要竖着看的，即（0，2，0），（0，2，1）…. 这个方法只能用在array上面，如果需要list的话需要np.asarray 12345678910import numpy as npids = [[3], [34], [5]]ids_o = [[3]]remove_list = filter(lambda i: i not in ids_o, ids)print(np.asarray(ids))for i in remove_list:index = np.where(np.asarray(ids) == i)print(index) 结果123456[[ 3][34][ 5]](array([1]), array([0]))(array([2]), array([0]))[Finished in 0.2s]]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python的filter函数]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E7%9A%84filter%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[python filterfilter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。 该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。 返回值是fliter的类型1remove_list = filter(lambda i: i not in ids_o,ids_u) 对于不在ids_o里面的i，是不是在ids_u里面，如果是的话就需要remove这部分东西]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tuple,array,list的大小问题]]></title>
    <url>%2F2019%2F03%2F22%2Ftuple-array-list%E7%9A%84%E5%A4%A7%E5%B0%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[每次在使用这群乱七八糟的数据结构的时候我都不明白到底用哪个函数求长度，而且各个结构的表示方法每次都让我感觉很迷茫，所以有了这篇文章。 好像只有array可以用shape来求！其他的都没有shape，array的shape可能是多维的。 数组array 数组的表示方法为最外面是括号，里面是方括号，不同的方括号代表不同的维度，np操作的都是array的部分 如果是一维数组，显示出来的size应该是(1,)这个样子的 size方法1a.size 1np.size(a) len不可以得到整个的大小，但是可以得到数组的行数，相当于a.shape[0]1len(a) 1a.shape[看看求的是第几维] 列表 列表最外面是方括号，不是圆括号！ 不可以直接用 a.size 求，’list’ object has no attribute ‘size’ 1np.size(List) 1len(List) 元组 元组的最外面是圆括号 不可以通过 t.size 来访问 可以通过 Tuple[]直接访问元素 1np.size(Tuple) 1len(Tuple) 字典 外面是大括号，里面是value-key的配对 size不可以用，np.size无法获得真实的大小 1len(Dict)]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>data structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在python中调用另外的文件]]></title>
    <url>%2F2019%2F03%2F22%2F%E5%9C%A8python%E4%B8%AD%E8%B0%83%E7%94%A8%E5%8F%A6%E5%A4%96%E7%9A%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[关于如何在python中调用其他的文件在cpp里面是使用头文件来导入的，但是提到python突然没想起来是怎么导入的。假设有文件a.py和b.py 在同一目录下12import aa.func() 或者引用模块中的函数123from a import funcfunc() ` 注意：前面一种方法导入的时候需要加上模块的名称限定，但是后面的导入就不用。如果怕麻烦可以导入的时候使用1from a import * 在不同目录下sys.path获取指定模块搜索路径的字符串集合，可以将写好的模块放在得到的某个路径下，就可以在程序中import时正确找到1234import syssys.path.append('a所在的路径')import aa.func() sys是什么 sys是python程序用来请求解释器行为的interface，比如调试，实时运行环境等 sys.argv 从外部向程序内部传递参数12345#!/usr/bin/env pythonimport sysprint sys.argv[0]print sys.argv[1] 运行：123# python sys.py argv1sys.pyargv1 sys.exit() 需要中途退出的时候可以调用，可以返回参数（0是正常退出，其他是异常）12345678910111213141516#!/usr/bin/env pythonimport sysdef exitfunc(value): print value sys.exit(0)print "hello"try: sys.exit(1)except SystemExit,value: exitfunc(value)print "come?" 123# python exit.pyhello1]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于hexo和gitpage的博客搭建以及设置]]></title>
    <url>%2F2019%2F03%2F20%2F%E5%85%B3%E4%BA%8Ehexo%E5%92%8Cgitpage%E7%9A%84%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E4%BB%A5%E5%8F%8A%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在commit了40多次之后终于把自己的博客搭好了，中间画遇到了一些奇怪的问题记录一下 github部分 在一些地方看到的说网站的名字必须和github的名字一样，不知道是不是必须的但是还是这么设置了 网站需要选择在master hexo部分基本功能：生成网页1hexo g 传到github上面1hexo d 生成新的md1hexo new &lt;title&gt; 需要把生成的全部清除1hexo clean 添加主题 把相应的主题clone下来，然后修改博客根目录的 _config.yml 文件 遇到404或者不显示模板的时候基本就是没套对 主题内容在主题的config修改这部分遇到的主要问题是两个：根目录config忘记添加一部分123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true hexo的url和root部分设置不对 github的deploy 地址应该是clone时候的网址 url部分应该是https://bigphess.github.io/，root部分是/ md文件增加图片在config里面设置，生成新的文章的时候就会生成对应的文件夹1post_asset_folder: true 然后把相应的图片放在文件夹里，引用的时候直接md格式引用：1![图片的名字](相对路径)]]></content>
      <categories>
        <category>博客相关</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[StanfordCS231N笔记]]></title>
    <url>%2F2019%2F03%2F20%2FStanfordCS231N%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Introduction Image Classification pipelinechallenges 图片是由无数数字块组成的 视角的转变，亮度的变化，变形都会产生非常大的变化 viewpoint illumination deformation occlusion background clutter intraclass variation image classifer input:image output: class_label data-driven approach 其他方法不行 attempts: 边缘检测，纹理等等（但是太过具体） 以数据为导向的方法 def train(image, label) def predict(model, test_image) KNNNN 对于每一个测试的data，在数据库里面找到离他最近的图片（选择一共找多少张，这么多张里面投票） 定义距离（hyperparameter） 曼哈顿距离 L1: 两张图相减求绝对值，然后把整张照片求和 欧几里得距离 L2: 距离的平方和开方 实现 training：记住每个图片的内容和label image：N✖D，每行是一张图片（拉成一行），一共N张 label：1-d数组，sizeN predict：计算距离找到最小的角标（np.argmin) 速度：linearly to size of dataset 缺点： 预测的时间太长了（expensive） 但是我们希望训练的时间长但是测试的时间短（CNN） KNN 找到最近的K个，投票 当K增加的时候，整个图片的边缘变得平滑了 K的数量也是一个hyperparameter 需要选择的hyper（并不能很好的找到最优解） K 用什么distance 如何选择最好的参数 总不能尝试所有的参数吧2333 不能使用test data，请在训练的时候忘记自己拥有它 把train data fold成不同的部分，把其中的一部分当成测试数据（validation data），然后测试训练的结果寻找hyper 交叉验证（cross-validation），循环当validation fold然后average result 但是根本不用呢 在test time的performance太差了 两个图片之间的距离太不直观了，你根本不知道图片间的距离会怎么变 linear classificationparametric approach 输入：32x32x3的图片，array of numbers 0,1,…3072 f(x,W) = Wx + b （在线性分类的情况下） （10x1） x: image （3072x1 -&gt; 拉直了） W: parameters，weights （10x3027） b： bias （10x1），不是这个函数的参数，只是用来决定比如猫的数量特别多，偏向猫的bias可能就比较大 输出：10个数字，表示每个class的scores 注意 W是把不同分类的classifer拼在了一起（乐高一样），每一行都是一个不同的class的分类器，点乘这个图片上面的像素，加上bias就是这个图片最终的得分 resize所有的图片到一个大小（目前） 实际上每个class的score就是图片里面每个点的加权求和，可以想象成在数每个不同地方的点的颜色。如果把W矩阵还原，还原出来的就是这个class的感觉上的颜色 可以想象在一个巨高d的space里面，用线性分类 hard part 都用灰度图会有问题 相似的texture（？ loss function optimizationtodo： 定义一个loss function来定义这个score的好坏 找到一个efficiently way去找到minimize 这个loss SVM loss定义 假设如果只有三个种类，一张图片对三个class分别会有不同的score。每张图片都可以计算出一个对应的loss SVM loss Li = sum max（0，sj - si + 1） si: 想要计算这个的loss function 的class的评分（也就是label标注的class的评分） sj: 这张图对于所有其他种类（除了i）的评分 Li: 最终这张图片的loss 1: 是一个safety margin（也是一个hyper parameter）。可以选择其他正数，但是选0会出问题 Li的每一项都在0和差值之间找最大值，然后把每一项的加起来求和 如何理解这个式子：既然对于不同class的评分越高就是越可能，那么评分是负数的话就说明不可能，这样就直接用0把这种可能性抹去了。如果其他种类在正的方面评分越高，说明这个种类跑偏了，loss越大 ###注意点 在上面这张图里，因为车的评分已经是最高了，计算出来的loss就是0 最后再把所有类型的loss求和，除以种类得到最终的loss 用的是求和而不是mean也是取决于自己的决定 也有的SVM里面用的是max之后平方，但是不平方的用的更多一点，也是一个hyper parameter scale 最小：0 最大：infinite bug 在实际应用里面没有那么好的效果 W不是唯一的，比如把这个W加倍，如果loss是0的时候是一样的 -&gt; 需要得到唯一的W weight regularization（解决上面这个问题） 在之前的loss的基础上加上了 \lambda R(W) \lambda是一个hyper parameter，是取决于自己的选择的 R是一个regularization函数，这个函数的作用是抵抗之前的loss。因为之前的loss是从训练集上得到的，比较吻合训练集，所以需要一个比较特别的W来和之前的fight，这样的话结果可能会在实际使用的时候更好一些 主要分类 L2 regularization：W里面的所有项平方然后求和（最常见） L1 regularization：W里面所有项绝对值然后求和 -&gt; 在一些其他地方使用 elastic net（L1+L2）：所有项平方乘参数加绝对值求和 max norm regularization -&gt; 后面讲 dropout 理解L2 比如X是[1,1,1,1],两个W分别是[1,0,0,0]和[0.25,0.25,0.25,0.25] 这样乘出来的最终结果都是一样的，都是1。 但是如果加上了L2的regularization之后就发现第二种方法的loss更少一点。因为他用到了更多的维数，在实际应用之中效果更好。 softmax（用起来更好）（multinomial logistic regression）定义 scores：unnormalized log probabilities of the class 需要把score先exp回来(这样所有的数都变成正数了) 再normalize（除以所有exp之后的的和） 最终，对于正确class的最终处理完的score来说，max这个log或者min（loss function）- log会得到最终最好的结果 最终处理完的score就是每个类型推测出来的占比可能性（和为1） 这里求完-log（p）其实就是信息熵，代表对不确定度的度量 直接比较可能性和log之后比较可能性在本质上是没有区别的 但是数学上一般log之后的数据会看起来好一些实际操作如下 一些问题 极值 Li最小值：0 -&gt; 如果正确类型的可能性是1，求出来的最终值就是0 Li最大值：infinite，可能性非常低非常接近于0 当W的初始化很小，所有score都接近于0： score求exp之后都是1，normalize之后是1/num（class），最后再求log 可以用于开头的检验 SVM和softmax 如果输入是[10,-100,-100]，在这个范围里微小变化，第一个是正确的class 对于SVM来说，后面两个负值都非常小了，根本不会去管后面的两个东西，-100和-200没啥区别 对于softmax来说，后面的-100还是-200还是会对loss最终的值产生影响，softmax希望所有的值都在正确的class上面，后面啥都没有。所以更具有robustness。 SVM会有一个你需要的区域，剩下的根本不考虑；而softmax会考虑所有的区域 上方区域总结 x：训练集里面的数据，放在图片里就是把一个图片拉成一个1xN的向量 y：训练集的标签，用来和最终的结果比对 W: weights，需要优化的部分 L：loss，用来权衡W优化结果的好坏 基本过程 Wx+b得到目前的分类器的score（score function） y是目前分类应该有的结果（label） R（W）得到regularzation的值 分类器得到score，y知道正确的分类，通过softmax或者SVM得到这个分类器目前的loss，再加上R（W）的部分增加robustness最终得到整个分类器的loss optimization lossfollow the slope 通过计算gradient来找到最低点 最基础的想法：（从数学上入手） 因为梯度是lim f(x+,h)-f(x)/h 把W上面的每一个点都加上一个0.00001（接近于0）然后再求上面的式子，就能得到第一次操作的梯度 silly 每一步都需要每一个维度都算一下，在CNN里面参数高达百万个，计算太慢了 因为用的0.00001，其实并不准确 感谢牛顿莱布尼兹发明了微积分 -&gt; 如何具体计算在下一节课 把loss的gradient改成了一个式子 快速，准确，然是容易发生bug（error-prone） practice需要进行gradient check 在写代码的时候用的肯定都是analytic gradient 但是需要在应用之前用numerical gradient检查一下，确保两者的结果是一样的，为了保证代码里面写的积分是正确的 gradient descent mini-batch 在实际应用的时候，不会把整个的训练集都拿来优化W，而是会把一部分拿出来（sample examples） 一小点一小点的拿结果不会非常准确，但是可以step很多次，在实际应用里面一般都不会用整个training set，不是很现实而且效果不是很好。 选择的数量上 32/64/256，这个不是一个很重要的hyperparameter，主要是根据GPU的性能来决定的 最终结果的loss是会下降的，虽然noise很多但是最终会go dowm learning rate 图片中使用linear classifier因为图片像素太多了，不可能对每个像素都用线性分类，所以一般会先提取一些特征然后得到最终的分类结果 color histogram 先得到一张图片的颜色特征分布 然后把整个特征分布拽成一个长的vector进行分类 HOG/SIFT 找到边缘特征，在图片的哪个部分有那种样子的edge bag of words 先把图片里面的一些特征当作一个vocabulary，然后放进一个词典里面 找到词典里每个词出现的频率然后拽成vector 线性分类 总结一般都是先进行特征提取然后再进行线性分类 深度学习特征都是自己提取 Backpropagation &amp; neural network目的：求出来loss function的gradient backpropagation最右边的点因为是df/df所以结果就是1 forward pass：知道开始然后一直顺到结束 在一个node上面，收到了x和y的input，对他们进行f操作，得到最终的结果z z再往后操作得到最后的loss（不知道什么操作） backward pass：从后到前，通过链式法则倒回来 虽然不知道loss对x或者y的gradient，但是可以求出来dz/dx和dz/dy（只和这个点有关） 可以得到dL/dz，然后乘以local gradient local gradient 每一个node上面的gradient往前推的时候，都可以通过链式法则（chain rule）变成这个点输入的gradient和这个点到上一个点的gradient的乘积。 算local的时候，乘的参数是输入进去的参数啊。比如dL/dx = dL/dz（这个带这个点back回来的数字） * dz/dx （这个里面的x带这个点输入进来x的值） 想不明白的时候把不同的点假设成不同名字然后求导！ 在这个网络里面，如果gate是加法（x + y）的话不是求偏导，如果求x的导数的话y并不是参数而是常数，所以求出来的结果是1，所以加法的gate就是直接把这个值相等的分开 加gate是一个gradient distributor，当一个gradient进来的时候会被相同的分开成了两份 也可以把一些gate组成一个大的gate，比如sigmoid 注意，求出来的gradient如果是正的，说明这个点对最终的loss有positive的作用 patterns add：gradient distributor max：router 假设f是max（x，y） local gradient对最大的那个就是1，对其他的都是0 因为如果没能通过max的gate的话根本对后门的loss没有影响。back的时候走最大的点就可以了，其他的都不用管了 multiply：switcher，真，两极反转 当往回的时候，两个点指向一个点，gradient需要相加（如下图） Implementationpsuedocode graph or net object forward: 把input pass进这个gate里面（必须在代码里面记住input） 把整个computational的garph往前推动 最后一个gate会return这个网络的loss backward 输入dz，然后乘不同的x和y 不同的gate分别是不同的文件（API），每个文件里面包括初始化，forward和backward 每次update的时候都需要进行forward和backward，forward得到gradient，backward再回来求最终的loss vectorized 在实际的计算中x，y，z都是矩阵，dz/dx是jacobian矩阵（全部都由偏导组成的矩阵） 比如一个max的门，如果输入是1x4096，输出也是1x4096，但是求偏导出来的矩阵是4096x4096（太大了），矩阵中间只有对角线部分的是需要考虑的（还会有很多0） 然后如果用了minibatch的100，得到的结果就是409600了，更可怕了 所以在每次API的时候，肯定不能写出来所有的链式法则，只用其中的一部分 作业的重点就是如何让这个东西计算出来效率高 neural network两层的NN 输入是图片一共的坐标数量 先通过第一层（max）得到100的中间层（hidden layer）-&gt; 100是hyperparameter，自己定的，但是越多越好吧 然后通过W2得到最终的分类结果（分10类） 其实具体里面是什么东西真的是不知道的？ 神经元 每个神经元的输入是Wx+b，然后经过激活函数 输出 激活函数 activation function sigmoid tanh ReLU 层状 -&gt; 可以更加efficient Neural network 2（training part1）前方提示： 小的dataset也可以有结果 电脑的性能有限 回顾一下历史 perceptron -&gt; 激活函数：0或者1，不能back madaline··· activation function（一个hyerparameter）sigmoid 特点： 把所有的数值都压到了0到1之间 曾经非常受欢迎，因为satrating的效果比较好 问题： 在saturate的情况下（非常接近0或者1），会杀死gradent -&gt; 看函数的图就能感觉出来-10做哟的导数就是0了，back回来没有意义 output不是以0为中心的（预处理的时候希望是0中心的） 不是0中心的问题：如果所有输入的x都是positive的话，得到的gradient要不都是positive要不都是negative 最后走出来的路径都是zig zag的 exp（）在计算上比较expensive tanh 把数字从-1到1之间分布，是一个以0为中心的sigmoid（0-centered），所以sigmoid的缺点（saturated的点会kill gradient）的缺点还在 ReLU 输入是正数的时候直接pass这个值，输入是负数的时候直接kill 可能的优点：（实际应用的时候效果非常好但是具体解释起来也没有那么知道为什么） 不会saturate（不会消失gradient） 计算效率高 更容易相交 问题 不是0-centered 如果x小于0（没有激活） -&gt; kill gradient） 死的时候会死一大片 -&gt; 所以一般的时候会把relu初始化的时候加上一个slightly positive bias 注意learning rate，选不好容易死 leaky ReLU 在小于0的时候会有一个微小的值，所以不会die 在使用的时候converges的速度比sigmoid和tanh快很多 加上了一个参数，可以在back的时候学到，这个值可以确定他是不是ReLU或者其他的 Maxout neuron 把ReLU和leaky ReLU组合了起来，有两个参数。算出来两个分别的值然后取其中大的那个 不会发生saturate或者die的问题 问题在于参数需要计算两次 步骤： 预处理数据 -&gt; 选择architecturedata preprocessingML 处理数据的时候首先需要0-center -&gt; 减去平均值（不是特别需要normalize，ML需要） PCA，Whitening，其实都在DL里不怎么常用 实际应用里：只需要center 比如一张图是32x32x3的 减去mean image（32x32x3） 减去per-channel mean （每个channel的mean，一共是三个数字） weight initialization（重要）请不要这么做：set所有w都是0，得到的结果就是每个神经元的功能都是一样的 small random numbers 0.01* np.random.randn(D,H) 问题： 在比较小的net里可以使用 在layer之间会发生non-homogeneous distribution of activation的问题 所有的activations会变成0 在back的时候所有的gradient都会变成0 如果把0。01变成了1，这时候发现所有的neurons全都是1或者-1 -&gt; gradient也全都是0，死亡 其他的一些论文也讨论过其他方法 Xavier 2010 除以input的sqrt ReLU， non-liear，会breaking。每回relu都会杀掉一半的东西，set到0 He 2015 把input除以2以后sqrt了 在实践中很有用 batch normalization -&gt; 实际中解决w初始化的方法 核心思想：x越来越接近0的原因是因为越乘越小（或者越大），这个时候我们就希望可以normalize这个x的input。因为gaussian的normaliztion是可以积分的，所以可以放回到back里面，在整个的网络里面插入一些normalize的部分就可以了 插在FC或者CNN之后，然后放在激活函数之前 优点 提高net里面的gradient flow 允许更高的学习率 减少对初始化参数的影响 form of regularization -&gt; 可能可以减少dropout的需求 babysitting &amp; learning process检查loss算的对不对 初始化这个net，去掉regularization，检查最后返回的loss 因为什么都没做呢，所以loss应该是最终知道的值（10 class是2。3） 再加上regularization，结果应该小小的变化 尝试训练 overfit一个非常小的dataset，关掉reg，得到非常小的loss和很高的accuracy 一个可能性：建议以一个小的reg开始，找到让loss变小的learning rate（如果不变小可能是rate太小了） cost：NaN，可能是learning rate高了 建议范围： 1e-3 ~ 1e-5 hyper optimization交叉验证 找到准确率高的部分，使用其中的hyper 最好set到log的space 再调整parameter，找到更准确的值 如果结果特别好可能也不对，可能是已经到了boundary了 参数的选择sample randomly的结果更好，不要固定一个选另一个，可能一个参数比另外一个重要很多 如果训练和验证之间的gap太大，说明overfitting，需要增加reg的力度。如果太小可能需要增加model的容量 ratio between the values and updates: ~ 0.0002 / 0.02 = 0.01 (about okay) 需要选择的hyper net architecture learning rate. decay schedule and update type regularization(L2/Dropout) ##总体summary training Neural Net2parameter updateSGD 以前是直接用gradient来update，现在希望变得复杂一点 -&gt; SGD太慢了 为什么SGD太慢： 如果在一个loss的分布上，一个维度特别密集，另一个维度特别稀疏，直接用gradient改变就会在一个方向跑大了 最后就会形成那种zag的形状 momentum update 在计算的时候引入了速度v = mu v - learning_rate dx （v初始化为0） 假设路线就是一个球在loss的圆弧里面运动，mu是～0.5，0.9，0.99（只使用一个值，single number，hyper） 形态，从初始点开始走一个大的圆弧，会跑过了，但是会再快速的converge回去 优点 引入了速度，可以在比较shallow的方向上速度逐渐增加 在比较深的维度上面，就像球在圆弧里面来回滑动 理解 是对这个update一点物理上比较直观的理解（其实名字叫做动量） 可以理解为这个东西是在一个平原上跑的一个球，我们需要求的w是这个球的速度，得到的dw是这个球的加速度，而这个球的初速度是0 可以理解为这个球找最低点的时候，除了每步按dw update，还在上面加上了前面速度的影响，也就是加上了惯性！123# Momentum updatev = mu * v - learning_rate * dx # integrate velocityx += v # integrate position nesterov momentum update 在上面的方法之后 look a head 了一步，得到的是两个向量之间的差 在实际走的过程当中，弧度会比monnument的更大一些，跑过的会更小一些 理解 Nesterov Momentum(NAG) 在原来的基础上：真实移动方向 = 速度的影响（momentum）+ 梯度的影响 （gradient） 现在：既然我们已经知道了要往前走到动量的影响的位置，那么我根据那个位置的梯度再进行update，岂不是跑的更快！ 总的来说就是考虑到了前面的坡度（二阶导数），如果前面的坡度缓的话我就再跑快点，如果陡的话就跑慢点123v_prev = v # back this upv = mu * v - learning_rate * dx # velocity update stays the samex += -mu * v_prev + (1 + mu) * v # position update changes form adaGrad （parameter-adaptive） 首先定义了一个cache，这个cache是gradient的平方的和，只是positive，和parameter的维度是一样的 然后把SGD的学习率（全局的learning rate）scale了一个这个数 -&gt; 这样得到的是不同参数的学习率 “ Added element-wise scaling of the gradient based on the historical sum of squares in each dimension” 结果：在越密集的维度上，update的步伐越小，越稀疏的上面update越大（因为平缓的地方历史gradient的平方和更小，所以update会更大） 问题 step size：时间越长learning rate会最终变到0，然后就停止学习 RMSProp（上面一个的变形） 把cache的定义改变了，增加了一个decay rate（hyper） adaGrad会计算的是所有梯度的平方的和，而这个计算的是gradient对应的平均值，这样的话learning rate的下降会更慢 依然能保持各个维度上面的平衡，但是不会让learning rate变到0 adam -&gt; 另一种自适应学习率的算法 beta都是hyper 结合了上面的两种方法 利用梯度的一阶矩和二阶矩估计动态调整每个参数的学习率 -&gt; 每次迭代学习率会有一个范围，让参数比较平稳 对梯度的一阶和二阶估计（期望的近似） 实际使用 默认用adam 刚开始使用高的learning rate -&gt; 这样进展会非常快 decay over time -&gt; 在进行到一定程度的时候会没有办法更细致的逼近minimum step decay: 比如过一些epoch之后就把lr减少到一半 exponential decay 1/t decay secend order optimization method（ml） 在计算的时候不仅需要gradient，还需要hessian来告诉你曲面的curve程度，以此来确定如何前进（牛顿method） 速度更快，hyper更少 但是在deep nets里面不太能使用，因为参数太多惹 BFGS（approximate inverse Hessian with rank 1 updates over time (O(n^2) each). L-BFGS work well in full batch mini-batch不是很适用 evaluation：model ensembles 可以不用训练很多个model，而是训练一个然后在其中选取不一样的check point track一个参数vector的running average可能会得到更好的效果 regularization（DROPOUT） 在forward的时候，随机的把一些neruon的值设置成0（比如杀掉一半） 为什么要使用： 为了求出来的结果更加的准确，每个特定的特征都不能完全依赖，因为这个feature可能就被drop掉了 计算一个大的net的其中一小部分，被drop掉的部分在back的时候也不会再计算了，就彻底关掉了。相当于在net里面取了一部分sample test time 在测试的时候希望可以把所有的neuron都打开（就进行一次） scale！！！！ 需要注意的问题： 计算训练时候的期望，就发现dropout之后的期望是测试的实际值的1/2（因为drop了一半） 因为以前net没见过这么大的output，会直接死掉，所以需要把测试时候的结果再缩小一半（或者drop的比例，* p） 最终结果：测试时候的输出 = 训练时候的期望输出 另一种方法 inverted dropout 在train的时候 / p 在测试的时候就不用改变了 gradient checking（并没有讲）CNN开始啦卷积层（核心部分） 对一张图片操作： 拥有一张图像32x32x3 拥有卷积核5x5x3（这两个东西必须维度一样） -&gt; 奇数尺寸的效果更好 kernal做卷积（所有的channel），得到一个28x28x1的activaton map 再对这张图片使用下一个不同的卷积核（卷积核的数量是一个hyper） 这样一个32x32x3变成了一个28x28x6（6是选择的hyper的数量） 当把这些层可视化了之后，发现越深图片的feature越高级（从上一级的特征得到的新的特征） 大致布局 卷积层 RELU层 -&gt; 黑白化 pooling层 最后加上fc层 具体计算stride 每次卷积核移动的时候的步长 注意在不同图片大小，不同卷积核大小和不同步长可能不匹配 （图片 - 卷积核）/步长 + 1 是不是整数，结果是输出图片的尺寸 padding 可以在图片周围一圈加上一圈0，这样图片卷积之后的大小就不变了 0-padding的大小和卷积核的大小有关，大小是（卷积核 -1）/2 如果不进行padding，图片会越来越小 参数数量 对一个卷积核：卷积核的大小 * 深度 + 1 （加一是加了一个bias） 一层的参数： 卷积核数量 * 一个卷积核 四个hyper： K：filter的数量，2的指数 -&gt; 计算效率高 S：步长 F：卷积核大小 P：0-padding 1x1的卷积 1x1的卷积层（stride也是1）会有比较好的效果 比如输入是56x56x64，filter是32个1x1x64。因为数据是有深度的，1x1的时候是有意义的（在二维上面没有意义） 现在处理的东西都是方形的从神经元的角度来看CNN 可以把filter认为成一个固定位置的神经元，这个神经元只看到了图片上面的一小部分，没有和全部的图片相连，然后进行了wx+b的运算 当slide这个filter的时候，weight是不变的，可以假设成一圈共享参数的神经元 对同一张图片的不同的filter可以认为成他们是在三维上面排列的一组神经元，每一层神经元都和这一层共享参数（不希望全部都是全联接，因为浪费了很多参数） pooling 在卷积的时候是不会改变图片的大小的 改变图片大小的操作在pooling layer里面实现 长宽缩短，深度不变 max pooling 2x2pool，stride2 -&gt; 每4个格子里面选择一个最大的表示 两个参数 pooling size F 2，3 stride S 2，2 不会改变图片的深度 fully connected 就跟普通的神经网络一样，所有神经元之间都会连接 把最后的图片变成一个列，放进去开始计算 实际应用LeNet-5AlexNet 两天不同的线，因为当时的GPU的效果不够 优点： 第一次使用ReLU 把data normalization了，但是现在看其实并不需要 data augumenation -&gt; 有用！ dropout 0.5，最后几层 ZFNet 在第一层上比alex的stride短，因为alex的步长4跳过了太多图片信息，这里改成了步长2 fliter的数量更多 VGGNet 只有3x3 s1 p1的卷积核，和2x2 s2的max pooling 结果还特别好 图像的尺寸越来越小，但是深度越来越高 需要的计算量：93MB/image（forward） -&gt; 200m/image(所有的计算加起来) 大部分的memory都在前期的层里，大部分的参数都在最后的全链接层里面（最后的计算量太大，后面有更好的方法） VGG也有位置确定，他比overfeat的层数更深 GoogLeNet 是一个一个的小结构组成出来了 参数的数量非常少 5million，取消了fc层 使用了average pool，把7x7x1024变成了1x1x1024 :把每个activate map取平均值 用VGG的人更多因为VGG的结构比较好想2333 ResNet t5 error降到了3.多 平常的加深层数训练集和测试集上边的准确率变化结果不统一，但是res做到了统一 虽然层数特比多，但是速度还是快 -&gt; 加入了skip的部分，把输入跳过了卷积又加了回去，这样back的时候就会分流 top-5 error 在看结果的时候不光看准确率，还会看分类器认为的前5个可能性（可能有几千个分类），如果这5个可能性都不对的话就是求出来的就是top-5 error spatial localization and detection这章的主要内容是识别出来这个东西之后用框框框出来 分类+定位：Localization as Regression 实际上就跟regression差不多 neurral net的输出是bounding box（4个数字），左上角的坐标和长宽 实际的图片标注的内容也有左上角坐标和长宽，求出这两个部分的L2 distance作为loss 步骤 训练（下载）一个分类的model 在net里面加上fc的regression head 用SGD和L2loss训练regression head部分 test的时候分类和regression都用 类别 平常的分类：最终的数量和class的数量相同 一个box里面会有4个数字，一共Cx4个数字 加在什么地方 conv layer之后 fc之后 多个目标的检测（Aside） 知道准确的检测目标的数量k，那么最终的分类数量就是4 * k 应用：人的动作检测 -&gt; 得到关节的位置 分类 + 定位：sliding window：overfeat 核心idea：在检测的时候直接process图片，但是对一张图片在不同的地方进行多次操作 操作步骤： 首先对图片进行conv和pooling，然后对得到的结果进行两个不同的fc， 得到的是1000个的分类种类 另一个的到的是1000x4的bounding box坐标 在一张大的图片上，在不同区域找到需要寻找的东西（比如分成四部分，这四部分是有重叠的，不是pooling那个样子） 得到每个部分对于这个分类的得分，以及相应部分对应的bounding box 最后用没怎么讲的办法merge了这些框，得到了最终结果 进一步优化 因为要对这个图片的不同crop做cnn，计算量会非常大 在fc层其实只是一个向量1x4096，把这个玩意拉成了一个cnn，4096x1x1，然后直接conv1x1的卷积核 现在net里面就只有conv和pooling了，所以就可以处理不同尺寸的图片了（不同尺寸的方形） 而且在处理不同区域的时候是参数是share的 目标检测 主要不同：不能确定图片里面物体的数量 思路： 尝试所有可能的window然后用classifcation找到需要的部分 问题：需要很多次分类 历史解决方法：用非常快的分类器，尝试所有（linear classifier） 更想用的方法：用cnn，只测试tiny subsets of possible locations region proposals 输入一张图片，输出所有可能有物体的区域 不在意到底是什么类型 不在意精确度 但是速度很快 selective search 从一个pixel开始，如果相邻的pixel有一样的颜色或者texture，merge 形成连接区域，再连接不同区域，这个区域也可以再打散 还有很多其他方法：edge boxes（推荐） RCNN（region based CNN） 从输入图片里面用region proposal的方法得到一系列的boundings（不同的位置和scale） 对每个区域crrop和wrap这个区域到fixed size cnn分类，regression head &amp; classifcation head 过程 下载model fine-tune for detection：改变分类的种类等 extract features 为每个class训练一个SVM（看这个区域是否包括寻找的东西） box regression：对每个种类，训练一个linear regression来纠正位置的偏差（太左太右，空隙太多）（dx，dy，dw，dh） datast PASCAL VOC 比较小 ImageNet 不是事很好操作，但是一张图一半只有一个东西 MS-COCO 一张图多个内容 fast RCNN （提速） 在测试时的速度比较慢 -&gt; 一张图里，在不同的proposals之间share conv的计算 训练时不是一起训练的，训练的pipeline也很复杂 -&gt; 把整个系统端对端对的训练一次 ROI pooling 在用的时候希望感兴趣区域的分辨率比较高，fc层希望更低的conv feature 在conv feature map上面投影高分辨率的region proposal 把这个区域分成小格，然后对每个格子进行max pooling(back的时候也是这么回来) 训练8倍！测试146倍！结果更准确！ faster RCNN（再提速） 之前的测试速度计算都没有算region proposal的时间，所以把这个问题也交给conv去干 在最后一层conv后面加入region proposal net 在feature map上面的移动实际就是卷积 训练一个小的网络判断是不是一个物体并分类，以及regression框的位置 在每个位置使用了N anchor boxes，不同的anchor有score来判断他是否属于一个object，在不同的形状上有不同的可能性（？ 后续的paper里面可以一口气train了 yolo 把detection变成了regression的问题 分成不同的小块，在每个块里面都加入 visualization, deep dream, neural style可视化：观察神经网络如何工作 可视化不同位置 可视化activation的神经元 -&gt; 大量的图片扔进神经元里面，找出来一个神经元最感兴趣的部分 可视化fliter -&gt; 只能在第一层进行（其他的层可以但是意义不大） 但是啥算法都会得出来长得差不多的 可视化特征（全联接层的特征向量） -&gt; t-SNE：Embed high-dimensional points so that locally, pairwise distances are conserved，特征相似的东西会聚类 对图片进行遮罩，可以看出来遮住不同地方这张图片被识别出来的概率 deep conv和optimazation的可视化工具：http://yosinski.com/deepvis deconv实现问题1:如何计算任意一个神经元的梯度（代码实现） 找到想要的神经元，forward的时候就停在这里 然后进行back，把所有其他的神经元的都设置成0，只把感兴趣的神经元设置成1，然后计算back出来的结果 最后的结果看起来并不是很好理解，所以改变了back的方法，得到更好的结果（“guided back”） guided back的计算方法 在普通的计算中，back的时候使用relu，会把所有负值都转化成0 在guide的计算里，在激活之后的东西back回去的时候，如果input的东西是负数的话，也会把这个东西kill成0，也就是说一个是block back的时候的gradient的值，另一个还会附加block输入进来的值 发生了什么：把输入进去ReLU的负的影响也取消掉了。如果不取消的话，这些正负就会互相fight，呈现更奇怪的图片。但是去掉负的之后变得就更清晰了。 deconv：直接无视掉relu的存在了 第二个问题：图像优化 how to find an image maximize some class score，但是整个网络不变 输入一张全0的图片 在back的时候把score设置成[0,0,0,1,0,0,…]，只有感兴趣的是1 back回去，找到对图片会产生什么影响 不停的重复这个步骤，更新的是图片不是weight 效果 找到可以让一个类型分数最高的图片（图片是根据网络生成的） 可视化data的梯度 -&gt; 得到了一个类似热量图的东西，这样对黑色的部分改变对这个东西的分类没有很大的影响 上面的步骤可以对任何的神经元进行（生成一张图片） 更好的regular 忽视了惩罚，只max神经元 但是更新之后blur了一下图像，这样可以阻止图片进行高频率积累 第三个问题，CNN的code包含多少信息 是否可以通过net还原出来原来的图片（涉及到隐私泄露的问题） 越往后的时候预测的准确度越低 deep dream 一个非常简单的过程，只有几百行代码，就是optimazation image 每次调用make_step图片都会发生微小的改变 把网络forward到一个位置 把gradient设置成activation设置成一样的 再往回传回去 可以强调对图片贡献最多的部分，不管激活了什么，都会把这个激活加强 deepart 把目标的content传进CNN 把style contet也传进CNN 把目标的loss和style的loss匹配，然后得到相应的opt image 是否可以用生成的图片去fool CNN 把图片的gradient设置成其他的东西，本来希望可以得到混合的结果，但是实际上图片的distort根本看起来不变 有些图片人类看起来差不多，但是gradient（或者HOG）之类的可能彻底是其他的东西 原因： 图片有很高维度的空间 实际训练的图像有一小部分被约束，放了一个线性分类之类只调整了其中的一小部分 在线性分类里，如果在每个维度上面都改变了一点点，实际上的置信区间会发生特别大的改变（大规模的点积运算）.下图只加进去了一点点的金鱼的噪音，分类就变成了100%的金鱼 这个现象不仅仅发生在图片里面，也发生在其他的地方 RNN（recurrent neural network）普通的nets：大小都是固定的 one to oneRNN：可以有灵活的对应结果 一系列的词来描述这张图 machine translation：seq of words -&gt; seq of words frame level的视频classification RNN是什么 可以在任何时间接受一个input（vector），然后对于不同的state产生不同的预测结果，然后需要在一些时间中预测出来vector。只需要特定的情况，其他的情况虽然有但是没有记录下来 过去的状态 + 新的input + 参数w -&gt; 预测出来新的state 注意：同样的function里面的weight是固定的，在不同时间使用但是weight是一样的 比如例子：https://gist.github.com/karpathy/d4dee566867f8291f086 输入一个字母的序列h e l o 预测下面的字母是什么，训练的模型是hello 把每个字母分别feed进去，顺着这个字母顺序来优化参数的序列，因为知道下一个的结果是什么了，就可以朝着这个目标来努力 竟然可以生成句子数学公式甚至代码 在图片中开始使用 从一张图得到一系列的文字 两部分组成 CNN：把test图片输入到CNN，一直到最后的fc，但是然后不进行分类，输入RNN RNN：RNN不仅是现在的输入，还会加入了CNN里面出来的输出。然后得到的结果（得到了没准一个词）进入下一个循环（就跟生成语义的时候一样） 直到在RNN里面找到的token，然后结束RNN LSTM long short term memory（大概是个生物里面的东西） RNN有好多层，每层还有很多个参数来决定这层往哪走 有两个输入x和h，组合到w上面，然后不同的东西乘不同的激活函数 x来自below h是从上一回来的 基于gate和function（forget gate）的类型，会更新c的值（反正都是参数的） 进行这些奇怪的操作的原因就是找到一个平衡和更好的结果 比较 每次普通的RNN都要经过f gate，会彻底改变。back的时候gradient会消失或者爆炸 LSTM里面用加法跳过了这个门，有一定的影响但是没有彻底改变，gradient的消失问题会被控制住（因为只用了加，不会die） gradient clipping可以控制住gradient爆炸 作业相关内容安装anaconda！！！ conda activate cs231npython3 -m IPython notebook 打开！！assignment1knn 两次循环计算距离 不需要一个像素一个像素的计算，用X直接表示i对应的那行的像素值的和，直接做差（每一项之间，平方（每一个，求和（所有项），开方。会快很多！！！！ 12#dists是一个500x5000的矩阵（测试数量和训练数量）dists[i,j] = np.sqrt(np.sum(np.square(X[i] - self.X_train[j]))) 初始化数组的方法是 np.array([[],[]]) 如果一个像素一个像素的循环结果简直太可怕了，害怕]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
</search>
