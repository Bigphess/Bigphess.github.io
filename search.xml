<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[十大经典排序]]></title>
    <url>%2F2019%2F09%2F03%2F%E5%8D%81%E5%A4%A7%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[参考出自：五分钟学算法 时间复杂度 最好情况 最坏情况 空间复杂度: 需不需要开辟新的空间 排序方式：in-place还是不是in-place 排序稳定性: 也就是之前顺序的东西在排序之后是否还顺序 冒泡排序 前面一个数和后面一个比，如果前面的数比后面的大(或者小)，就交换他们两个 这样交换一次之后的最后一个数就是所有数字里面的最大数 也就相当于最大的数像冒泡一样冒出来了，第一轮过后，第二轮可以直接冒倒数第二个数，也就是说每一轮的内循环次数可以逐渐减少 需要重复n次（但是每次对越来越少的数字进行上述操作），以确保所有的交换都已经完毕了。 增加了一个flag的判断，如果在一轮里面没有任何交换产生，那就说明所有的元素都已经排序完毕了（因为如果还有数字往上冒必然会在前面有交换 这样如果是正序排列的话，可以直接跳出循环，不用进行比较，时间复杂度是n12345678910111213def bubbleSort(num): n = len(num) for j in range(1, n): flag = True #如果在这轮没有任何交换，就可以说明排序已经完成了 for i in range(n - j): if num[i] &gt; num[i + 1]: temp = num[i] num[i] = num[i + 1] num[i + 1] = temp flag = False if flag is True: break return num 时间复杂度： 当正序排列的时候，因为可以直接跳出循环，所以只需要遍历一次所有的数保证他们排列正常，所以时间复杂度是n 当倒序排列的时候，需要把所有都移动一遍，所以时间复杂度是 n^2 平均的复杂度是n方 空间复杂度： 因为交换只需要一个额外的temp来储存临时变量，空间复杂度是1 in-place：是的 稳定性：稳定，因为一次换过来的东西就不动了，在判断大小的时候也是判断的大于而不是大于等于，也就是说前面换到后面的大数永远会在后面的数的前面 比如 [0,8,1,8,2]。首先会把第一个8换到第二个8的前面，因为判断条件没有等于，所以越不过去。 选择排序 从所有元素中找到最小（或者最大）元素，然后放到数组的第一个（也就是和数组的第一个交换位置），然后这个就算是固定住了 然后从第二个到最后一个中选择现在最小的，和数组的第二个交换位置，前两个就固定住了 以此类推 1234567891011121314def SelectSort(nums): n = len(nums) for i in range(n): Min = float("inf") index = None for j in range(i, n): if nums[j] &lt; Min: Min = nums[j] index = j if index != i: #这样可以减少一些根本不用交换的情况，下一个位置上本来就是最小的 temp = nums[i] nums[i] = nums[index] nums[index] = temp return nums 时间复杂度： 最好情况和最坏情况都不能避免走两个循环，都是n^2 空间复杂度:1,来储存temp的值和暂时的最大值的变量 in-place：是的，换位置就可以，不需要单拿出来 稳定： 不稳定，因为在交换位置的时候，并不知道现在的东西被交换到哪里去了 比如：[3,3,0,9]，当0和3交换位置的时候，把第一个3交换到第二个3后面去了]]></content>
      <categories>
        <category>算法</category>
        <category>排序</category>
      </categories>
      <tags>
        <tag>经典排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unity显示相机画面并在opencv处理]]></title>
    <url>%2F2019%2F08%2F22%2Funity%E6%98%BE%E7%A4%BA%E7%9B%B8%E6%9C%BA%E7%94%BB%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[打开相机显示方法首先需要明确，目前看到的显示图片有两种方法，一种是渲染到实际的gameobject上面去，另一种是draw到GUI上面去。关于gameobject在上一个显示图片里面有了大致的了解。这里主要说的内容包括 相机显示的必备步骤 关于GUI 关于renderer 相机内容显示 在unity中，有一个专门的类叫做WebCamTexture，我们需要为读取进来的相机texture创建一个新的对象。一般来说，需要三个部分，相机纹理，相机名字（string）以及相机是否打开（bool） start部分 在这部分，我们首先需要把创建的cameraTexture实例化 其次，我们需要调用StartCoroutine函数，在其中调用测试函数来确定相机是否打开 StartCoroutine 在一般的执行里面，unity是逐帧运行的，所以当操作花费时间的时候，帧率下降，就会发生卡顿 这部分开始了一个协程（Coroutine），使用yield，可以在任何部分暂停这个Coroutine的执行。如果被成功暂停，它会在下一帧恢复正常。所以这个方法在多帧运行之中非常好用。 unity会假装开辟一个新线程来执行，但是不会影响主线程的持续效果 参考 从这里的功能来说，yield会检查用户有没有授权，如果没有授权的话，运行被终止，跳到下一帧。如果授权成功了的话，运行成功，相机打开。 IEnumerator 在StartCoroutine调用的是一个IEnumerator函数，他通过yield一个bool来决定是不是继续运行这个函数 授权 在test里面需要考虑有没有用户的授权 有授权的情况下，需要把目前的WebCamTexture（注意这里不是建立的实例）的device的数据传递给WebCamDevice，包括类型，名字等等。 然后需要把包括这个相机名字，size和fps的信息传给之前cameraTexture的实例 以上都设定好之后，WebCamTexture.Play会激活这个相机，让他开始工作 然后再讲相机的texture渲染到object的表面上12345678910yield return Application.RequestUserAuthorization(UserAuthorization.WebCam); if(Application.HasUserAuthorization(UserAuthorization.WebCam)) &#123; WebCamDevice[] devices = WebCamTexture.devices; cameraName = devices[0].name; cameraTexture = new WebCamTexture(cameraName, 1024, 768, 15); cameraTexture.Play(); isOpen = true; renderer.material.mainTexture = cameraTexture; &#125; GUI GUI主要是提供了图形化的窗口，实际上显示的元素是直接显示在game画面上的，也就是说这部分是和实际相机拍摄到的画面独立的。无论相机如何移动，物体如何改变，最终GUI的画面都会显示到同样的地方 MonoBehaviour.OnGUI() 注意OnGUI函数并不需要我们自己去调用，不需要再update里面调用！ OnGUI是API里面自带的函数，我们需要在这个函数中渲染和处理GUI的event 在实际应用中，OnGUI可能每一个frame被call很多次，每次event（例如鼠标操作，键盘等等）发生的时候都会call这个函数 例如下面官方的例子，每次鼠标点击的时候，就会print出相应的话来12345678910111213using UnityEngine;using System.Collections;public class ExampleClass : MonoBehaviour&#123; void OnGUI() &#123; if (GUI.Button(new Rect(10, 10, 150, 100), "I am a button")) &#123; print("You clicked the button!"); &#125; &#125;&#125; GUI.DrawTexture 在GUI的实现中，我们需要将相机读取到的部分draw的GUI的上面，所以调用了这个函数 需要确定的参数包括：位置，需要渲染的texture，缩放比例等等 实现 代码部分参考1234567891011121314151617181920212223242526272829303132333435363738394041424344454647using System.Collections;using System.Collections.Generic;using UnityEngine;public class showCamera : MonoBehaviour&#123; protected WebCamTexture cameraTexture; protected string cameraName = ""; protected bool isOpen = false; //protected MeshRenderer renderer; // Start is called before the first frame update void Start() &#123; //renderer = this.GetComponent&lt;MeshRenderer&gt;(); cameraTexture = new WebCamTexture(); StartCoroutine(Test()); &#125; // Update is called once per frame void Update() &#123; &#125; IEnumerator Test() &#123; yield return Application.RequestUserAuthorization(UserAuthorization.WebCam); if(Application.HasUserAuthorization(UserAuthorization.WebCam)) &#123; WebCamDevice[] devices = WebCamTexture.devices; cameraName = devices[0].name; cameraTexture = new WebCamTexture(cameraName, 1024, 768, 15); cameraTexture.Play(); isOpen = true; //renderer.material.mainTexture = cameraTexture; &#125; &#125; void OnGUI() &#123; if(isOpen) &#123; GUI.DrawTexture(new Rect(0, 0, 400, 300), cameraTexture, ScaleMode.ScaleToFit); &#125; &#125;&#125; 渲染到object上面 和之前的操作类似，需要 构建MeshRenderer的object 在start里面读取出物体的MeshRenderer（getcomponent） 最后打开相机后，把相机的内容渲染到MeshRenderer上面 12345678910111213141516171819202122232425262728293031323334353637383940414243using System.Collections;using System.Collections.Generic;using UnityEngine;public class showCamera : MonoBehaviour&#123; protected WebCamTexture cameraTexture; protected string cameraName = ""; protected bool isOpen = false; protected MeshRenderer renderer; // Start is called before the first frame update void Start() &#123; renderer = this.GetComponent&lt;MeshRenderer&gt;(); //cameraTexture = new WebCamTexture(); StartCoroutine(Test()); &#125; IEnumerator Test() &#123; yield return Application.RequestUserAuthorization(UserAuthorization.WebCam); if(Application.HasUserAuthorization(UserAuthorization.WebCam)) &#123; WebCamDevice[] devices = WebCamTexture.devices; cameraName = devices[0].name; cameraTexture = new WebCamTexture(cameraName, 1024, 768, 15); cameraTexture.Play(); isOpen = true; renderer.material.mainTexture = cameraTexture; &#125; &#125; //void OnGUI() //&#123; // if(isOpen) // &#123; // GUI.DrawTexture(new Rect(0, 0, 400, 300), cameraTexture, ScaleMode.ScaleToFit); // &#125; //&#125;&#125; 将图片放入opencv来源 得到了web的texture之后，可以直接用openCV的部分把这个玩意转换成mat，然后处理 这里的问题是刚开始mat的大小莫名其妙的是16，所以需要加上一个判断条件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667using System.Collections;using System.Collections.Generic;using UnityEngine;using OpenCVForUnity.CoreModule;using OpenCVForUnity.UnityUtils;using OpenCVForUnity.ImgprocModule;public class showCamera : MonoBehaviour&#123; protected WebCamTexture cameraTexture; //protected string cameraName = ""; //protected bool isOpen = false; private Color32[] colors; private Mat rgbaMat; private Texture2D tex; // Start is called before the first frame update void Start() &#123; //renderer = this.GetComponent&lt;MeshRenderer&gt;(); //cameraTexture = new WebCamTexture(); //StartCoroutine(Test()); cameraTexture = new WebCamTexture(WebCamTexture.devices[0].name, 640, 480); cameraTexture.Play(); StartCoroutine(init()); &#125; private IEnumerator init() &#123; Debug.Log(cameraTexture.width); if (cameraTexture.width &lt;= 16) &#123; while(!cameraTexture.didUpdateThisFrame) &#123; yield return new WaitForEndOfFrame(); &#125; cameraTexture.Pause(); colors = cameraTexture.GetPixels32(); cameraTexture.Stop(); yield return new WaitForEndOfFrame(); cameraTexture.Play(); tex = new Texture2D(cameraTexture.width, cameraTexture.height, TextureFormat.RGBA32, false); rgbaMat = new Mat(cameraTexture.height, cameraTexture.width, CvType.CV_8UC4); GetComponent&lt;Renderer&gt;().material.mainTexture = tex; &#125; &#125; private void Update() &#123; Debug.Log(cameraTexture.width); if (cameraTexture.didUpdateThisFrame &amp;&amp; rgbaMat != null) &#123; Utils.webCamTextureToMat(cameraTexture, rgbaMat); //Imgproc.cvtColor(rgbaMat, rgbaMat, Imgproc.COLOR_RGB2HSV); Utils.matToTexture2D(rgbaMat, tex); tex.Apply(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Unity</category>
        <category>OpenCV</category>
      </categories>
      <tags>
        <tag>Unity</tag>
        <tag>WebCamera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程珠玑ProgrammingPearls]]></title>
    <url>%2F2019%2F08%2F20%2F%E7%BC%96%E7%A8%8B%E7%8F%A0%E7%8E%91ProgrammingPearls%2F</url>
    <content type="text"><![CDATA[第一章问题 开始的问题是如何对文件进行排序 -&gt; merge sort 整合问题之后，问题变成了需要对7位数字进行排序，这样的话需要的时间就远小于merge sort了 另一种方法 如果在每个byte里面存一个数字，那么1MB可以存143000左右的号码（e6/7) 但是如果把每个数字表示成一个32位的int（也就是说每7位数存成一个32位的整数，那么这7位数就占4个byte），那么可以存250000左右的号码 从这个角度考虑，快排的速度比merge快 实现 从上面的问题分析来看，用bitmap或者bit vector来表示数据很常见。 比如，可以用一个20bit的string来表示{1,2,3,5,8,13} 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 在这个里面，出现在集合里的数字就表示为1，没有出现的就表示为0 在实际解决问题过程中，7位数字可以表示成一个小于千万的数。如果用一个千万的二进制串来表示，那么如果整个文件里面有现在的号码的时候，这个位才被表示为1，否则就被表示为0 分为三个阶段 关闭所有的位，即千万个位全都是0 从输入文件里面导入所有的数字，比如2897的话，就是b[2897]=1 全部输入完毕之后，再根据现有存在的数字就可以直接排序，输出了 处理原则总原则：在开始处理问题之前分析问题，才能让问题更好解决 确定正确的问题 -&gt; 最重要的一点 选择了bitmap的数据结构：选择这个数据结构是根据电话号码不会重复这个特殊条件得到的 multi-pass 时间和空间的trade off 简单设计 第二章 算法问题1： 如果有最多40亿个排好序的32位浮点数，其中有遗漏的数据，如何找到遗漏的数据。考虑内存足够的时候和内存不够的时候的情况 如果内存足够，可以像第一章说的一样，用位图来表示这些数据，然后看哪些没有 如果内存不够？ 二分查找 必须定义范围，范围里面每个数字的表示方法和探测方法 比如如果把这些数据分成两部分，比如1-10里面取中位数，因为缺数据的原因，总是会有一边的个数少，那么缺的数据就肯定在少的这边 问题2： 将具有n个元素的向量x左旋i个位置，时间上与n成正比，有十几字节的额外空间 直接方法 储存到额外数组 -&gt; 太浪费空间了 定义一个函数将数组每次移动一个位置 -&gt; 太浪费时间了（虽然时间上和n成正比） 另一个思考 把一个数组分成不同的组，每次把对应组的内容转移，直到所有内容都转移成功。比如把x[0]挪出去，x[3]诺到x[0],x[6]挪到3，然后0挪到6。然后再挪x[1]和他的对应的内容们 另一种思考方法：旋转x实际就是把ab转换成ba 如果a是前i个元素，a比b短，把b分割成b1和b2，让b2和a的长度一样 那么可以先交换a和b2 -&gt; b2 a b1 然后再集中精力交换b1和b2里的元素，也就是说这可以是一个recursive的表示方法 转置：这个看起来在写Leetcode的时候非常好用啊！！！但是注意在python里面reverse要自己写 如果把问题看成转置，实际上数组ab可以 先转置a：aTb 再转置b：aTbT 再转置整个数组：（aTbT）T = ba 也就是说，如果abcdefg想让他旋转三位，实际上可以做到的是 reverse（0，2）：cbadefg reverse（3，6）：cbagfed reverse（0，6）：defgabc 问题3 找到一个单词的变位词：比如pots和stop 解决方法 注意，找到所有的可能性是很愚蠢的，比如一个22个字母的词，所有的变位22！大概是10e21，时间爆炸了 核心思想 -&gt; 排序，排序之后的变位词就都一样了 leetcode 242,49 这个题的核心思路就是，每个单词按字母顺序排序之后的答案就是这个单词的key，如果两个单词的key一样的话这两个单词就是变位词，如果不一样的话就是新的词 在python里面直接用字典可以很好的储存变位词 第三章 数据结构数据结构的意义就是让代码可以更加简短整洁 -&gt; 比如用数组代替循环 原则：能用小程序解决的就不要用大程序 把重复性代码改写 封装复杂的结构 尽可能使用高级工具 让数据去构造程序 第四章 正确编写程序写一个完全正确的binary search吧！ 注意点 求中位数的时候是前后相加，除以二 注意跳出循环的条件是 start&gt; end 为了满足上面的循环条件，需要每次判断完mid之后，把start或者end移动一位，不然会陷入死循环 后面的9，11，14会用到程序的验证技术12345678910111213141516class Solution: def search(self, nums: List[int], target: int) -&gt; int: start = 0 end = len(nums) - 1 mid = (start + end) // 2 while start &lt;= end: if target == nums[mid]: return mid elif target &gt; nums[mid]: start = mid + 1 mid = (end + start) // 2 elif target &lt; nums[mid]: end = mid - 1 mid = (start + end) // 2 return -1 第五章 次要问题 虽然每次写完了程序，大家基本都会选择把它直接插入系统，然后强烈的希望他能运行 （哭了，写的也太真实了） 测试用例 设置极小的测试用例（比如0个，1个，2个元素等等） 设置可以自动生成的测试用例 计时，测试不同方法的时间 调试 -&gt; 引发bug是会有实际的逻辑原因的，调试的时候需要关注这些问题 第六章 性能透视在后面的三章会说到提高运行效率的三个方法，在第六章主要讲的是各个层次如何组合为一个整体 案例 一个模拟天体间受力关系，来模拟运动的程序，通过对程序的改进，在n=1000的程度下，把速度从一年提升到了不到一天，速度提升系数约为400 改进方法 算法和数据结构，把时间复杂度n2 -&gt; nlogn（二叉树） 算法优化，优化了两个粒子靠的很近的情况 数据结构重组，减少了局部计算的次数 代码优化：64位浮点数改为32位，计算时间减半。用汇编重写了某个函数，给缓慢的函数加速 硬件，提升了硬件 算法加速不一定是独立于硬件的，比如在超级计算机上，树形结构对时间的影响就很小 设计层次 问题的定义 系统结构 -&gt; 第七章，封底估计 算法和数据结构 -&gt; 2,8章 代码优化 -&gt; 9 系统软件 硬件：比如实现某个功能的专门硬件 原则 如果需要少量加速，研究最好的层次。虽然修改算法是非常常见的答案，但是在研究之前最好考虑一下所有可能的层次。比如硬件？这种的，最好能找到一个得到最大加速，又所需精力最少的层次 如果需要大量加速，需要研究多个层次。就像上面的案例一样 封底计算在处理问题之前，需要对这个问题的规模有一个大概的估计，才能更好的处理问题 帮助封底计算 两个不同方面的答案对比 量纲检测 经验法则 实践 性能估计 + 安全系数]]></content>
      <categories>
        <category>算法</category>
        <category>编程珠玑</category>
      </categories>
      <tags>
        <tag>Programming Pearls</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unity与openCV显示图片]]></title>
    <url>%2F2019%2F08%2F20%2Funity%E4%B8%8EopenCV%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[关于opencv库 unity带了opencv库，但是这个库是based on java的基础上的，也就是说我继用过opencv的c++和python之后这回要用java了 OpenCV for Unity文档 注意：需要在载入opecv的asset之后，把StreamingAssets文件夹移动到Asset里面，需要操作tool--set plugin settings opencv自带了很多example，把这些加入到building setting里面就可以用了 显示图片首先明确一点，unity里面显示图片没有imshow这种东西，需要把图片的mat转化成texture格式，然后把这个texture加到object上面 在这里我建立了一个plane的object，并且调整了相机的角度，用来显示图片 这部分在start里面进行，也就是现在的图片是静态显示 var var是用来推断这个变量类型，因为var之后直接创建了变量，所以可以推断出来。但是java还是静态语言 这种样子是不行的var foo; foo = &quot;foo&quot;; 在for循环里可以 imread 基础功能和以前一样，读取一张图片，存为mat格式 路径中使用了Application.streamingAssetsPath，也就是上文中说到需要移动到asset文件夹里的opencv自带的文件夹。 Imgcodecs和Imgproc等都是以前没有接触过的库，如果需要图片正常显示，需要把格式从BGR改成RGB Texture2D 这个是处理物体表面纹理的一个class，构建新的的时候需要确定这个texture的大小 在这里需要创建一个新的texture2D，才能在之后把mat转到texture里面 Utils.matToTexture2D(dst, tex) 用于mat和纹理的转换，同样也有texture转到mat的方法 GetComponent() 得到这个gameObject的一个部分，尖括号里面的名字取决于现在这个object里面有什么 这里用的是plane，里面自带renderer的属性，并且renderer里面带有material，用来修改构成这个object的材料 总结 用unity显示图片的中心思想就是这个图片变成了object上面的texture，这个图片不能脱离object而独立存在，所以需要首先为这个图片构建object 最终结果如下 123456789101112131415161718192021222324252627using System.Collections;using System.Collections.Generic;using UnityEngine;using OpenCVForUnity;using UnityEngine.UI;using OpenCVForUnity.ImgcodecsModule;using OpenCVForUnity.ImgprocModule;using OpenCVForUnity.CoreModule;public class remove : MonoBehaviour&#123; void Start() &#123; var dst = Imgcodecs.imread(Application.streamingAssetsPath + "/image.JPG"); Imgproc.cvtColor(dst, dst, Imgproc.COLOR_BGR2RGB); //Debug.Log(dst.channels()); Texture2D tex = new Texture2D(dst.width(), dst.height(), TextureFormat.RGBA32, false); OpenCVForUnity.UnityUtils.Utils.matToTexture2D(dst, tex); gameObject.GetComponent&lt;Renderer&gt;().material.mainTexture = tex; &#125;&#125;]]></content>
      <categories>
        <category>Unity</category>
        <category>OpenCV</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>Unity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OmniProcamUnity文件总结]]></title>
    <url>%2F2019%2F08%2F19%2FOmniProcamUnity%E6%96%87%E4%BB%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[文件构成 Lib libOmniProCam libOmniProCamCalibration OmniProcam Projection Camera ProjectionTarget RenderTexture Shader DebugUI LibOmniProCamManager 其中，在lib里面有calibration用的接口，在里面设定好了投影仪需要的参数数量和相机需要的参数数量。在lib里面直接放了老师写好的dll，所以其实cs文件是在访问这些dll的内容 Camera 目的：初始化相机 变量 创建一个class：cameraInitializer，继承了Monobehaviour MonoBehaviour是每一个unity的script的基础类 public的gameobject，用来决定setup哪个相机，这个相机直接从unity的UI里面拽进去 protected的camera类，targetcamera projected可以在这个class以及所有继承这个class的里面被访问，但是private只能在这个class中被访问 Camera是unity里面一个表示相机的类 创建了两个float的数组，大小和之前设置好的相机参数，和投影仪参数一样 初始化三个4x4的矩阵，分别是相机的intrinsic，extrinsic以及相机的投影矩阵 Matrix4x4可以表示transformation的矩阵，包括平移旋转等等 函数 void Awake() 功能 在开始之前初始化variable或者game state，在整个stript里面只被call一次 在所有的object初始化结束之后 通常在start之前被call 效果 初始化了calibration 设定好了目标的相机，并且把这个相机setup protected void setupCamera(Camera targetCamera, int cameraIndex) 直接call了dll里面写好的功能（但是这里得到的是projector的，为什么？），得到了各种参数 关于GChandle 然后把distortion和都记录下来了，直接把translation和rotation（相机的外矩阵）赋值给了目标相机、这样目标相机就能直接移动到应该到的位置上了 另外两个函数可以返回这里目前没有用到的distortion k（4个）和c（2个） device]]></content>
      <categories>
        <category>研究室</category>
        <category>OmniProcamV2</category>
      </categories>
      <tags>
        <tag>OmniProcam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Levenshtein Distance的具体分析]]></title>
    <url>%2F2019%2F08%2F08%2FLevenshtein%2F</url>
    <content type="text"><![CDATA[编辑距离 编辑距离是衡量字符串相似度的距离 主要应用比如衡量DNA的相似性，衡量什么地方断字，文件的差异等等 基本操作 对于字符串里面的两个字母，有三种操作方法能让他们改变 插入一个新的字母 删除一个已经存在的字母 把现有的字母替换成其他字母 对于两个string，有四种基本的操作 第一种，不变 （比如ruopeng和fag） 原本的操作 ruopeng[0,6] -&gt; fag[0,2] 变换之后 ruopen[0,5] -&gt; fa[0,1] 因为最后一位相同，这个问题可以拆分成最后一个字母，和不包括最后一个字母的substring，这个问题就可以拆分了 第二种，替代 replace （比如 peng 和 zhou） 原本操作 peng[0,3] -&gt; zhou[0,3] 变换之后 pen[0,2] -&gt; zho[0,2] + replace(g -&gt; u) 其中，替换的操作是一步，替换之后最后一位的字母相同，这个问题就可以拆分成更小的问题了 第三种，插入 insert 原本操作 peng[0,3] -&gt; zhou[0,3] 变换之后 peng[0,3] -&gt; zho[0,2] + insert(u) 首先，直接把这个问题里面的zhou拆分成了更小的问题zho，然后再进行一个插入操作（一步），使zho重新变成了u 第四种，删除 deletion 原本操作 peng[0,3] -&gt; zhou[0,3] 变换之后 pen[0,2] -&gt; zhou[0,3] + delete(g) 同时也可以尝试尝试删除掉前一个string里面的最后一个字母g，不再管里面的g，把这个字符串尝试变成pen来进行比较 核心思想 首先，我们针对这两个string制作一个table 对于这个表来说，每一个位置都相当于这个位置对应的两个substring的相似程度，比如E和F对应的就是 ”RUOPE“ 和 ”F“两个substring对应的相似度 在每个单词开始之前，还有一个空白符号，对应的substring就是空白。比如空白符号这一列对应的就是”“分别和”F“，”FA“，”FAN“等元素的比较 对于每个表格里面的位置，从(1,1)开始，这个位置和周围位置的关系如下 上面一格是插入，指的是在FANGZHOU这个string的substring里面插入 左边一格是删除，指的是把RUOPENG这个单词删除一个字母 左上角一格是替代，指的是把这两个单词的字母分别退一位。如果当前位的字母相同，当前位的值等于退一位之后的值，如果当前不同，是退一位的值+替代花费的操作（也就是1） 表格初始化 首先需要确定这个表格的边缘情况，也就是横坐标和纵坐标分别是0的部分，这个部分不能用上面总结出来的公式表示 因为这部分其实就是把不同的substring和空白符号比较，那么需要的最小操作就等于当前字母的位数 填表 从当前的初始状况出发，逐渐把这个表填完，每一个新的格子的值都取决于上一个格子的值 insert = M[i-1][j]+1 delete = M[i][j-1] + 1 replace = M[i-1][j-1] + 1 dont change = M[i-1][j-1] （两个字母匹配） 如果两个字母不匹配的时候，now = min(insert,delete,replace) 最终结果如下，红色部分为字母相同部分 python代码实现12345678910111213141516171819202122def EditDistance(a, b): m = len(a) n = len(b) # 构建表格，注意需要比长度大一格，储存空字符串 M = [[0 for i in range(m + 1)] for j in range(n + 1)] # 初始化表格，substring分别和空字符串比较 for i in range(1, n + 1): M[i][0] = i for i in range(1, m + 1): M[0][i] = i # DP for i in range(1, n + 1): for j in range(1, m + 1): # 判断是否相同，注意这里要减一 if b[i - 1] == a[j - 1]: M[i][j] = M[i - 1][j - 1] else: insert = M[i - 1][j] + 1 delete = M[i][j - 1] + 1 replace = M[i - 1][j - 1] + 1 M[i][j] = min(insert, delete, replace) return M[n][m] 注意分清矩阵的行和列，在这个实现里a是行，b是列 考虑到最前面的空字符串 注意从string里面读取的时候要记得减1 注意构建矩阵的时候行数和列数要加一 右下角的结果就是两个完整字符串对比的结果 时间复杂度 这个方法需要遍历左右的substring的组合，并且把所有的结果都存在一个矩阵里 如果两个string的长度分别是m和n，那么时间复杂度O(mn),空间复杂度O(mn) 最后，RUOPENG在8月8号这天祝距离为8的FANGZHOU，八(7+1)夕快乐]]></content>
      <categories>
        <category>算法</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>编辑距离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法图解笔记]]></title>
    <url>%2F2019%2F08%2F06%2F%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[第一章 算法简介二分查找 必须是有序的数组 对于n个元素的列表，使用二分查找最多需要 log2 n步，用普通的查找最多需要n步1234567891011121314def binary_search(l, item): low = 0 high = len(l) - 1 while low &lt;= high: mid = int((low + high) / 2) # 这里需要转成int，不然没办法做index if l[mid] == item: return mid elif l[mid] &lt; item: low = mid + 1 else: high = mid - 1 return None 运行时间 大O表示法表示了操作数，表示的是这个算法的增量 大O表示了在最糟情况下的运行时间 -&gt; 但是也是需要讨论平均时间的 常见的大O时间 logN 对数时间 Logarithmic N 线性时间 Linear N * logN 包括快速排序等，速度较快 N ^ 2 选择排序等，速度较慢 N！ Factorial time 非常慢 旅行商问题 travelling salesman problem, TSP 一个搞不好可以用n！来解决的问题 你要去五个不同的地方，需要规划怎么样路线最短，最开始的思路就是把每种可能性都列出来，然后对每种路线进行计算。这样的话五个地方就是120种，地方越多越呈阶乘增长 第二章 选择排序数组和链表 数组需要的空间是固定的（也就是说必须连在一起），所以如果后面加进去了其他东西的话就不行了，需要转移位置，或者预留空间 链表的每一个位置都会有到下一个位置的指针，所以不需要移动元素。 但是链表的问题在于，如果想直接找后面的东西的时候需要一个接着一个读取 需要读取整个数据的时候链表效率很高，需要跳跃的时候链表效率很低。而数组在读取随机元素的时候效率很高 数组插入 / 删除线性，读取常数。 链表读取线性，插入 / 删除常数。 在实际中，因为数组支持随机访问（但是链表只支持顺序访问），所以数组的适用范围大一些 选择排序 实现方法：每次都从所有的里面选出最大 / 最小，然后放在最开头 时间复杂度 n ^ 21234567891011121314def select_sort(l): newArr = [] for j in range(len(l)): smallest = float('inf') index = None for i, item in enumerate(l): if item &lt; smallest: smallest = item index = i newArr.append(l.pop(index)) # 注意这里需要把l的大小改变了 # 但是是在带l的loop外面变得所以没有关系 return newArr 第三章 递归 recursion何为递归 函数自己调用自己 递归和循环的作用效果是相同的，没有性能上的优势，但是可以让方案更加清晰 base case：告诉函数什么时候不再调用自己，停止循环 recursive case：函数调用自己 栈 stack 先进后出的数据结构，push（压入）和pop（读取和删除） 在调用另一个函数的时候，当前函数暂停并且处于未完成状态，函数的所有变量都储存在内存里 使用递归的一个问题：如果调用栈的时候很长，会占据大量内存 这种时候需要重新编写代码使用循环 或者使用尾递归（并不是所有语言都支持） 第四章 快速排序（分而治之 divide and conquer）分治 核心：把一个问题分成子问题，再把子问题分成更小的子问题，最后的子问题可以直接求解，这样的话原问题的解就是子问题的解的合并 比如把n规模的问题分成k份，然后在k个问题里面分别再分开 特征 问题缩小规模可以轻松解决 可以分解成若干个小问题 分解出的子问题可以再合并（如果不满足这条，应该考虑贪心或者DP） 分解出的各个子问题是独立的（不满足应该考虑DP） 不用循环而用递归：函数式编程里面没有循环（Haskell） 1234567891011121314def RecursiveSum(l): if l == []: return 0 else: return l[0] + RecursiveSum(l[1:]) # 这里不能改变l本身的大小def MaxNum(l): if len(l) == 2: return l[0] if l[0] &gt; l[1] else l[1] submax = MaxNum(l[1:]) return l[0] if l[0] &gt; submax else submax 注意： 需要找好基本条件，如果找最大值的base就是还剩下两个值 注意return的内容 快速排序 比选择排序速度快很多 base条件，一个元素或者空的数组就不需要排序了 需要设定一个基本值（比如取第一个值，根据这个值把原数组分成两部分），这样这三个大块就分类完成了。然后对于每个小块，再继续分组 比基准小的子数组 基准 比基准大的子数组1234567891011121314def QuickSort(l): if len(l) &lt; 2: return l else: piv = l[0] # 这里是快读得到比他大和比他小的写法，主要从l[1:]开始 less_part = [i for i in l[1:] if i &lt; piv] more_part = [i for i in l[1:] if i &gt; piv] return QuickSort(less_part) + [piv] + QuickSort(more_part) # 注意这里piv是个int，所以连接的时候需要改成list 时间复杂度 快速排序的速度取决于选择的piv的值的大小，也就是说如果完全排好的情况下，复杂度是n2 合并排序的速度是 nlogn，快速排序的平均速度是 nlogn，最佳情况是 logn 快速排序需要logn层，每层需要把n个元素全都遍历一次，所以最终的结果是nlogn 在计算复杂度的时候，复杂度是操作的次数，而这个次数需要乘一个每次操作的常量，在快速排序的时候常量更小，而且快速排序没那么容易遇到最糟情况 第五章 散列表（hash表）hash函数 最终的目的是查找的时候时间复杂度是 1 函数构造：把输入映射到一个数字 无论你什么时候输入，输出的数字是一致的 将不同的输入映射到不同的数字 知道整个存储的范围有多大，不会返回超过这个大小的数字 应用：缓存 网站的缓存数据就存在hash表里面，这样访问速度更快，网站本身需要做的工作更少 访问一个网页 -&gt; 是否在缓存里 -&gt; 有的话调用缓存 -&gt; 没有的话存进缓存 冲突 虽然假设的时候认为每个东西都被映射到不同地方，其实会产生冲突 这种情况下要在hash后面加上list hash函数需要把内容比较平均分分配 如果储存的链表很长，那么性能就会急剧下降 性能 最佳性能，1 最糟性能，插入，删除，查询全都是n 装填因子： 装填的元素数 / 元素总数 一旦超过0.7就需要调整hash的长度了 第六章 BFS：最短路径问题BFS 用于图的查找算法，可以解决两种问题 从A出发有前往B的路径吗 从A出发到B的路径最短是什么 实现图 -&gt; hash表，需要将node映射到所有的邻居 在python里面使用deque创建双端队列 算法实现（广度搜索）： 创建一个队列，储存用于查找的人 首先把初始化的人载入队列 从队列里弹出一个人，查找他是不是（查过之后标记成已检查，列表记录），不是的话把这个人的相邻加入队列（一直重复） 标记成已检查非常重要，因为不标记的话可能会陷入无限循环 如果最后队列空了还没找到，那就是没有 123456789101112131415def BFS(name, graph): search_queue = deque() search_queue += graph[name] searched = [name] # 用来储存已经探索过的人数 while search_queue: person = search_queue.popleft() if person not in searched: if person[-1] == "m": # 只是一个判断是不是这个人的办法 return person else: search_queue += graph[person] searched.append(person) return None 更新版本，不但可以搜索还可以计算长度12345678910111213141516def BFS(name, graph): search_queue = deque() search_queue.append(name) searched = [name] # 用来储存已经探索过的人数 distance = &#123;name: 0&#125; while search_queue: current = search_queue.popleft() for person in graph[current]: if person not in searched: searched.append(person) distance[person] = distance[current] + 1 if person[-1] == "m": return person, distance[person] else: search_queue.append(person) return None, None 注意点： 增加了distance这个dict来储存开始点到这个点的距离 在初始化的时候只在已搜索队列里添加了第一个点的信息，在后面的循环里才添加后面的点 对于每个从queue里面拿出来的点，如果不在已经查找的点里就一定需要加进去，并且计算距离，距离即是和上一点的距离+1 计算过距离之后再判断是不是要找的点 运行时间 需要沿着每条边前进，所以在边上的运行时间是 O(E) 把每个人加到queue里面也需要时间，每个人的时间是常数，所以人数的时间是 O(V) 总的运行时间 E + V 拓扑排序 如果任务A依赖于任务B，那么任务A就必须排在B的后面，这种就是拓扑排序 第七章 狄克斯特拉算法 之前的图找的是最短路径，现在需要给图加权，找加权之后的最短路径 加权之后的最短不一定是边数最短 负的权重不顶用，因为负权重不能确定没有比目前消耗更小的 书里写的错的地方：可以有环，没环都是树了！有向无环图可以直接拓扑排序 核心思想：找到到这个点消耗最少的路径，并且确保没有路径比这个小了 算法流程 找出消耗最低的点 更新这个点相邻点的开销 重复这个过程，直到对所有点都做了（即A点最小之后更新B点，然后更新C点，以此类推） 计算最终流程 具体实现 创建一个表格 包含了每一项和每一项的具体开销，目前不知道的开销标记成inf 需要包含每个点的父节点，才能保证最后可以计算流程 不停的更新这个表，从开销最低的一直更新到开销最高的，如果更新了开销的话同样需要更新父节点 在确定路径的时候，从结尾的地方开始找，然后一路找到开头 一个问题：关于图在python里面的表示 实际就是一个dict叠dict，如果直接访问G[a][b]就可以直接得到这两个点的距离 如果没有加权的话，可以直接dict叠list，因为不需要记录距离了 注意点 最好是最开始指定了第一个消耗最低点，然后再循环里面最后找消耗最低点 初始化的时候注意：cost里面初始为0，father里面去掉这个点，Node（cost最低点）初始化成这个点，循环的条件是Node不是空123456789101112131415161718192021222324252627282930313233343536373839404142def dijkstra(graph, src, target): # 需要创建一个cost表和一个父节点表 cost = &#123;&#125; father = &#123;&#125; visited = [] for i in graph.keys(): # 访问这个图里面所有的key，就是所有的店 cost[i] = float('inf') father[i] = None cost[src] = 0 # 起始点的cost是0 father.pop(src) # 起始点不需要父节点 Node = src # 最小开销点还存在的时候更新所有的点 while Node is not None: for near in graph[Node]: new_distance = cost[Node] + graph[Node][near] if new_distance &lt; cost[near]: cost[near] = new_distance father[near] = Node visited.append(Node) # 找到最小开销的点(放在循环里面好一点) Node = None smallest = float('inf') for i in cost: if i not in visited: if cost[i] &lt; smallest: smallest = cost[i] Node = i # 跳出循环，得到结果 N = &#123; "start": &#123;"a": 2, "b": 6&#125;, "a": &#123;"fin": 1&#125;, "b": &#123;"a": 3, "fin": 5&#125;, "fin": &#123;&#125;&#125;# print(N["start"]["a"])dijkstra(N, "start", "fin") 第八章 贪心算法 处理npc问题，即没有快速算法的解法的问题 对NPC问题找到近似解 例子问题 课表问题： 希望尽可能多的课在这个屋子里面上，但是上课时间冲突，如何排课 先找到这个教室里最早开始的课，然后找到这节课结束之后最早开始的课 装东西 背着包去装东西，容量有限，如何装到最大价值的 从最大的开始装，但是得到的不是最优解，最优解应该动态规划 核心思想：每一步都用局部最优解，得到的结果就是全局最优解。即使得不到最优解，也可以得到近似最优解 只能用贪心的问题 集合覆盖问题，比如想在全部区域广播，但是每家电台只覆盖特定区域，如果才能花费最小的计划 列出所有的可能性的话，复杂度是 2^n 近似算法：从覆盖最多地方的电台开始找，一直到覆盖所有地方 贪心算法的复杂度是 n^2，n是广播台的数量 因为每次都需要遍历一次找出最需要的那个（n)，这样的操作需要进行最多n次（万一每次都覆盖不上） NPC问题：从旅行商问题详解 旅行商问题和求路径问题的区别在于，旅行商问题需要访问一遍所有的地方，找到最短路线 旅行商问题需要查看所有的可能的路线 从简单问题出发 两个地方的时候，可能的路线有两条（一来一回） 三个地方的时候 6条 需要计算出所有的解才有可能从中选出最短的路线 -&gt; NPC问题 近似方法：每次都去离得最近的地方 识别问题 元素较少的时候速度快，随着元素增加非常慢 涉及所有组合 不能分成小问题，必须考虑所有情况 涉及序列（比如旅行商中的城市序列），集合（集合覆盖问题）且难以解决。或者可以转换成这种问题的 第九章 动态规划核心思想 把一个大的问题拆分成很多小问题的解的合并 最终的结果其实类似一个表，每次再得到新的结果的时候，其实是在比较 同等大小下上一个的值（即上一行的值，这个值已经是之前储存下来的最大的值了） 当前放新的东西的价值+放完这个东西之后剩下空间可以放的东西的最大价值（这个最大价值同样也被记录了） DP处理问题的时候只能整件的处理，也就是说分布应该是离散的而不是连续的 而且DP处理问题的时候各个子问题之间不能互相依赖，如果互相依赖了就很复杂了 最长公共子串 比如搜索引擎误输入，怎么判断相近词 每种动态规划都涉及网络，每个网格里面的值就是我需要优化的值 每个单元格里是什么 每个单元格是这个格子之前相同的字母数量 如何把这个问题分成子问题 如果相同，就是前一个的字母数+1，如果不同就是0 网络的坐标轴是什么 在这个问题中，是两个单词的各个字母 注意在这个问题里面，最终答案不是在最后出现的 最长公共字序列！！和子串不一样，对比的是序列而不是字母的个数 当字母不同的时候，选择上边或者左边最大的那个 应用 levenshtein distance（字符串的相似程度，也就是上面的公共子序列） 指出文件的差异，DNA的相似性，确定什么地方断字 核心思想：需要把两个str里面所有的substr的combination都计算一下 实现 初始化条件：插入 实际上来说一个格子有三种操作，删除，插入和置换，这个格子需要改变的操作是这三个操作的最小值 删除 = d[i - 1][j] + 1 插入 = d[i][j - 1] + 1 （相当于这个单词的前一个字母，加上一个插入，这两个没有本质区别） 替换 = d[i - 1][j - 1] + cost，其中这个cost，如果两个相同就是0，两个不同就是1 具体说明见levenshtein distance说明 第十章 K最近邻创建推荐系统 向这个人推荐电影，找到离这个人最近的五个用户 如何判断最近：特征抽取 第十一章 接下来如何树（数据库 + 高级数据结构） 二叉查找树：对于每一个节点，左节点都比他小，右节点都比他大 时间复杂度 logn 平衡问题 延伸： B树，红黑树，堆，延展树 反向索引 搜索引擎的工作原理 把网页创建hash表，key是单词，value是包含单词的页面 傅里叶变换 信号处理 并行算法（提高速度） 对速度的提升是非线性的 并行管理开销 负载均匀 MapReduce 分布式算法 把一台电脑上面处理的工作分布到多台电脑，处理大量数据 分布+归并 布隆过滤器 概率型数据结构：可能出现错报，但是不可能出现漏报 储存空间很少 HyperLogLog 类似于布隆过滤器的算法，不能给出准确的答案但是占据的空间很小 SHA 另一种hash，安全散列算法，给定一个字符串返回hash 比较大型文件 检查密码 局部不敏感，修改其中一个字符，会改变很多 Diffie-Hellman 密钥交换 如何对消息加密，让只有收件人看得懂 线性规划 在给定的约束条件下最大限度的改善指定的指标]]></content>
      <categories>
        <category>算法</category>
        <category>算法图解</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SVD]]></title>
    <url>%2F2019%2F07%2F16%2FSVD%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[fusion360]]></title>
    <url>%2F2019%2F06%2F26%2Ffusion360%2F</url>
    <content type="text"><![CDATA[给予特征 拉伸指令 修饰外形 修改边缘 圆角 倒角 外观处理 选择贴图 右键选择外观 可以选择加到面上面或者加到所有东西上面 render（3D模型2D化） 从model改成渲染 点那个台灯，修改图片的场景设置，比如高宽比 所有东西都弄好了之后直接渲染，渲染到自己满意的地方 画草图 可以把参考的图放在平面上来，这样画起来比较轻松（插入） 需要校准加进去图片的比例 修建偏移之类的对标CAD 画曲线 直接关键点画曲线，可以再调整 几何关系靠计算来 咖啡杯 杯身 旋转体成型 杯盖 新建新的零部件 修改 – 合并（相交运算 草图 – 投影，可以把其他的东西投影到现在的平面]]></content>
      <categories>
        <category>CAD</category>
        <category>Fusion360</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[libProCam笔记]]></title>
    <url>%2F2019%2F06%2F25%2FlibProCam%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Binarizer.cpp这个部分是来计算图像的binary的，包括 计算这个图片的threshold 做背景substract Cpp相关 构造函数 构造一个类的时候使用的函数，这个函数的名字和类名相同，默认是没有参数的，可以自己加上参数，这样创建对象的时候就需要给参数 对标python里面的init 析构函数 名字和构造函数完全相同，但是在前面增加了一个波浪线，没有返回值也没有参数，只是用来在关闭程序的时候释放资源 虚函数 借助指针来达到多态的效果 定义一个函数是虚函数，不代表这个函数不被实现，而是代表基类的指针可以调用子类的这个函数 如果定义为纯虚函数，才说明不会实现 比如下面的例子里面，B是子类，A是基类，创建的是A的指针，但是调用的却是B的函数，这说明这个函数的调用不是在编译的时候被确定的，而是在运行的时候被确定的12345678910111213141516171819202122class A&#123;public: virtual void foo() &#123; cout&lt;&lt;"A::foo() is called"&lt;&lt;endl; &#125;&#125;;class B:public A&#123;public: void foo() &#123; cout&lt;&lt;"B::foo() is called"&lt;&lt;endl; &#125;&#125;;int main(void)&#123; A *a = new B(); a-&gt;foo(); // 在这里，a虽然是指向A的指针，但是被调用的函数(foo)却是B的! return 0;&#125; 纯虚函数，在函数的定义后面加上 =0 经常会在定义基类的时候用纯虚函数，因为基类可能有很多的派生，但是基类本身生成的对象可能是不合理的 比如动物可以派生狮子老虎，但是动物本身不是很合理 在子类里面必须重新声明这个函数，也就是说在子类的时候必须提供一个这个函数的实现，但是基类的作者不知道你怎么实现它]]></content>
      <categories>
        <category>研究室</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些OpenCV里面之前没用到的函数]]></title>
    <url>%2F2019%2F06%2F20%2F%E4%B8%80%E4%BA%9BOpenCV%E9%87%8C%E9%9D%A2%E4%B9%8B%E5%89%8D%E6%B2%A1%E7%94%A8%E5%88%B0%E7%9A%84%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[看了别人写的代码，感觉好多openCV里面的基础功能我都不知道，当时还是在自己写的，比如降噪啊，减去背景啊等等的 关于载入模型看了一个Github上面的项目，是自己训练了一个五个手势的识别，然后用这些手势来控制自己屋子里面的灯光变化，用keras训练的所以先载入了这个的模型，然后有两个不同的预测的函数 第一个函数用了model.predict_classes，这个函数预测出来的直接是类别，打印出来的就是类别的编号 第二个用的是model.predict，这个预测出来的是一个数字，不能直接用，还需要把这个数值argmax(predict_test,axis=1)才可以用（也就是找到最大的） createBackgroundSubtractorMOG2()retval = cv.createBackgroundSubtractorMOG2( [, history[, varThreshold[, detectShadows]]] ) 创建了一个MOG2的background substructor 参数分别是 history的长度 计算的是曼哈顿距离，这个的阈值 是否检测影子，如果检测的话速度回慢一点 创建完的模型有很多功能 这里用到了一个apply，就是对一张图进行这个操作，来计算出这张图片的foreground 这里有一个参数叫learning rate，指的是你的这个模型会不会随着时间而改变，0的话就是不变，1的话就是完全由上一帧形成，然后负数的话会自动选择一个 bitwise_and()dst = cv.bitwise_and( src1, src2[, dst[, mask]] ) 这里计算了每一位的与（and）计算，加上了一个可选的参数mask，这样可以直接计算出来前景去掉背景 videoCapture.set()dst = cv.bitwise_and( src1, src2[, dst[, mask]] ) 这里他的代码没有用属性，直接设置了两个数字，每个数字会代表一个属性，数字的范围是0-18 也就是说她设置了10这个属性，然后把这个属性的大小设置成了200 cv2.bilateralFilter(frame, 5, 50, 100) 一个叫这个名字的滤镜，可以消除掉图片里面你不想要的噪音 效果比较好，可以在消除噪音的同时保证图像比较清晰，但是与此同时这个filter的速度会比大多数的慢一些 图像翻转 flip 两个参数，翻转的图像以及翻转参数 0是竖直翻转，1是水平翻转，小于0的时候是旋转180度（先竖直再水平翻转） inRangedst = cv.inRange( src, lowerb, upperb[, dst] ) 查看是否有数字在这个范围里面，三个channel都在的话就输出1，不在的话就输出0 可以给不同的channel赋不同的值 calcHisthist = cv.calcHist( images, channels, mask, histSize, ranges[, hist[, accumulate]] ) 计算一系列输入的histogram 原来有这个函数，把这个输入normalize到255个channel上面就能得到，这样就可以得到这张图片里面的分布了 而且考虑了上面的mask的情况 calcBackProject（存疑！）dst = cv.calcBackProject( images, channels, hist, ranges, scale[, dst] ) 计算一个hist的back projection CamShift()]]></content>
      <categories>
        <category>图像处理</category>
        <category>OpenCV</category>
      </categories>
      <tags>
        <tag>OpenCv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文Touch180]]></title>
    <url>%2F2019%2F06%2F17%2F%E8%AE%BA%E6%96%87Touch180%2F</url>
    <content type="text"><![CDATA[Touch180: Finger Identification on Mobile Touchscreen using Fisheye Camera and Convolutional Neural Networkabstract 鱼眼+深度学习，证明检测手指的准确性和鲁棒性 生成了一个dataset 训练了一个CNN，确定手指touch的位置 intro 可以通过给不同的手指不同的职责来增强对触摸屏的使用 以前的分别手指的论文 wearable device -&gt; 比较贵 确定触摸的区域 -&gt; 必须多个手指触控 还有一个方法是在手上戴上了一个有颜色的戒指（诶这个方法用来实现现在的device的画画功能感觉怎么样） 用了深度相机的 -&gt; 不适合mobile device 其他用CNN的没有使用鱼眼相机 dataset 直接用不同的frame来做的dataset 用五个二进制的数字来表示labeling net]]></content>
      <categories>
        <category>Papers</category>
        <category>鱼眼手势识别</category>
      </categories>
      <tags>
        <tag>fisheye</tag>
        <tag>hand</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unity入门制作space shooter]]></title>
    <url>%2F2019%2F06%2F13%2Funity%E5%85%A5%E9%97%A8spaceshooter%2F</url>
    <content type="text"><![CDATA[目标 这个游戏是和雷电类似的游戏 需要设定碰撞等等游戏逻辑，然后设计音乐，图等等东西 setup，player，cameraset up 需要确定好应用的开发平台file-build setting 注意这个游戏想开发网页版的，但是现在已经不支持web player，建议使用web GL 可以在project setting-player里面改变一些游戏的设置（这里改变了宽度和高度） 在右上角的layout可以改变窗口的布局 放置游戏的object 直接从model里面拖进来放进scene或者hierarchy里面 点击F可以锁定这个东西（最佳视角） 需要设置这个object在原点 有一个mesh filiter来决定这个东西用的什么模型 一个mesh renderer来渲染这个模型 这个里面有两个不同的材料 因为需要物体之间的碰撞，所以需要增加physics--rigidbody 加入了这个东西之后，物体就有了物理上的特征 下面由于需要碰撞，所以需要定义这个物体的体积 增加一个physics -- capsule collision 相当于把这个物体的周围加上了一个cage，来确定他的体积 需要在这里面定义碰撞的方向（因为这里面是沿着z运动的，所以碰撞在z轴上面） 可以改一改视角，然后直接拉bar来调节这个consule的大小 关于coliider 除了这里面用到的胶囊型的以外，还有square的sphere的，confound的（混合）？，在能解决问题的时候尽量用上面的基础图形 除此之外，还有一个mesh coillder，用来专门契合这个物体的体积的，但是是最后的选择，选择的时候最好也选择一些比较简单的形状(如图所示，新的碰撞范围会完整契合这个物体) 在coillder里面可以自己设定这个对应的mesh的种类，即使和使用的模型不同也可以（也就是说可以建立一个相对于这个模型简单的模型，然后建立碰撞） 为了选中的这个东西可以被识别为碰撞物体，选择is trigger 为了增加一点神奇的新功能，在预设好的prefab（一个可以重复克隆的对象）里面找到一个引擎的动画，拖到player的子目录下面，这个东西就自动加进去了 相机和light相机 因为一开始相机会初始化在物体之后的位置，所以现在在game界面只能看到这个物体的屁股部分，需要设置相机 点击main camera，这时候右下角会出现相机位置的缩略图 位置 -&gt; 调节到物体上方，并且旋转90度 设置相机的种类 如果选择perspective的相机，可以通过调节FOV来决定看到的是多大 这里选择orthographic（正投影，主视图那种感觉）的相机，然后直接调节看到的size就可以了 因为希望object保持在原点上，所以在这里移动camera 可以在game画面里面直接调节object的位置等等东西 下面把在camera里面把背景改成黑色（soild color） 但是这时候object的颜色并没有变黑，这是因为打开了ambiant light 在window--rendering--lighting setting里面，把source改成color就可以改变环境光的颜色 环境光：ambient light 这个光没有方向，所以如果都加上了的话可能会很奇怪 光 创建一个有方向的light -&gt; main light 应该是最亮的光 希望可以看到飞机的颜色但是不希望太亮 调整这个光的角度以及强度 建立第二个光，来照亮不怎么亮的部分 -&gt; fill light 还是调整角度，照亮第一个光照不到的部分 为了不让这个光那么强，把这个光的强度适当调小 为了适应太空的冷色调，把光源颜色改成蓝绿色 最后增加第三个光，把暗部勾边（相当于素描里面的在边上的高亮部分） -&gt; rim light 为了不照亮东西的上层，在x轴把角度调成复数 为了勾边，颜色是白色的 降低亮度 最后建立一个新的空的object，位置reset好，来整理上面的三个光 注意，因为这里的光是directional light，所以光照的效果和光源的位置无关，只和光源的角度有关 背景 新创建一个quad，调整位置和角度让相机能看到他 remove掉碰撞的部分，因为背景不需要 加上texture -&gt; 可以直接把这张图片拽到对应的位置上面 这样会创建一个新的material加到这个mesh的renderer上面去 缩放这个方块，让这个图片可以完全显示（高宽比必须保持！） 这时候照在player上面的光也可以照在背景上面，但是不是很希望这种结果 一种方法是把background完全独立一个layer出来 另一种方法是改变这个texture的shader 这里改成完全的testure，这样就不会受到光的影响了 最后把船从陷入的background里拽出来 移动player 首先需要在player底下建立新的scripts fixedupdate -&gt; 确定物体现在的位置 首先得到垂直和平行的输入（键盘或者鼠标输入） 物体的移动是一个三维的数组，分别是xyz轴，这里y轴的移动是0.0f，其他两个分别是平行和垂直的移动 因为已经用了rigid body，可以从里面给物体一个速度GetComponent&lt;Rigidbody&gt;().velocity -&gt; 这时候移动的非常慢，因为input接收的只是0和1，所以每秒移动一个unit 这时候加上了一个新的speed参数，每次在计算速度的时候乘上这个参数就可以了 注意，因为这里的speed是个public的值，所以可以直接在unity的UI里面进行操作 这时候出现了一个问题，player会跑出屏幕 增加一个判断条件，移动这个东西如果到了边界，那么把position从先reset到边界上 因为这样在更新下一帧之前都不会出界 使用了mathf里面的一个函数clamp，来设定x和z的范围 这个函数超过了下界就会一直显示下界，超过了上届就会一直显示上届 为了不让设定的xmax，xmin，zmax，zmin在UI里面太占地，所以建立了一个新的class来装这些东西（注意新的class不需要继承），然后在需要用的时候建立新的对象，call 为了能在UI里显示，需要在新创建的class上面加上[System.Serializable] 这些边缘设置什么值可以直接把东西拖到边缘然后看看是什么值就可以了 tilt或者bank：让飞船到左右移动的时候可以旋转一下 增加了旋转功能Quaternion.Euler shots希望把这部分的逻辑和这部分的图像分隔开：这样换模型的时候就很方便了 首先创建一个新的quan，在这个上面加上texture 创建一个新的material，然后再这个里面选择一个texture 然后再把新的material加到对应的东西里面 希望这个东西黑色的部分消失，shader选择mobile--particle--addtive（mobile比较有效率） 增加这个东西的碰撞，去掉VFX的碰撞 然后加上这个光的运动逻辑 transform.forward()向前运动，乘上速度 把这个东西设定成一个prefab（直接拖到prefab的文件夹里面） shoot shots 已经把需要射出去的光线设定成了一个prefab，这时候只要在每次点鼠标的时候把这个东西射出去就可以了 创建一个新的空的object作为player的子目录来定义发出去激光的位置（shotSpawn） 在player中新增一个功能，public的对象是shot和shotSpawn，这两个东西都可以直接从UI里面拖进来，然后设定这个子弹发射的逻辑，每次按下鼠标都并且在0.25秒外会发射子弹 按下鼠标后把showSpawn的transform赋值给shot，这样就知道shot的位置了 问题：现在游戏进行过程中会往root里面增加很多prefab boundary，hazards，enemyboundary在画面里面消失的子弹也删掉 -&gt; 建立一个box 打开trigger 给这个box里面用一个函数，判断是否撞上这个边框，撞上了的话就删除撞上的物体OnTriggerExit 1234567public class DestroyByBoundry : MonoBehaviour&#123; void OnTriggerExit(Collider other) &#123; Destroy(other.gameObject); &#125;&#125; hazrads会撞上player的陨石 首先建立一个新的陨石对象，这个对象应该可以被撞击的，加上碰撞和刚体 加上这个陨石的自动旋转，使用随机的数字，并且把angular drag改成0 加上激光和陨石碰撞之后，两个东西同时消失的效果 单纯的加碰撞效果会发现因为边界引发的陨石消失bug 因为陨石先和边界碰撞了，然后两个一起消失了 需要把Boundary加上一个新的tag1234if (other.tag == "Boundary") &#123; return; &#125; 爆炸！就是艺术！ 在contact destroy的文件里面增加新的爆炸效果 新建一个explosion的gameobject，给这个对象赋值位置Instantiate(explosion, transform.position, transform.rotation); 给player一个新的tag，然后在碰撞里面判断是陨石撞player还是陨石撞激光，赋值不同的爆炸效果 把mover增加到陨石里面，速度设置为负值，这样陨石就能往自己身上掉了 game controller 创建整个游戏的逻辑 设置好的陨石创建成新的prefab 创建一个新的对象作为控制器，加入一个新的对象hazard，初始化好掉下来的敌人的位置，让敌人可以从随机的位置往下掉 每次掉落之间隔着时间（这部分和C++有些不一样），把所有的掉落放在一个while里面 去掉剩余的爆炸效果 score，audio，building加声音 首先把陨石爆炸的背景音加到陨石的prefab里面 背景音乐加到game controller，武器的音乐加到player 武器的音乐需要在每次发射的时候触发 调整各个音量的大小 计算score 新建一个text，设定好这个text的字体效果之类的 然后把这个text reference到gamecontroller里面，然后把最开始的分数初始化，并且写出来更新score的代码 把这个score reference陨石里面 增加文字效果 需要增加重新开始游戏和游戏结束的text 在游戏逻辑里面加上的是这个玩意怎么更新和计算，需要两个flag判断有没有游戏结束 需要在碰撞里面把player弄死 增加最后的效果 增加陨石 复制之前的asteroid并且给他们不同的模型和碰撞 把之前的对象改成一个数组就可以handle很多个对象了，随机选择 增加移动的小星星（项目里面自带的） 让背景重复起来 把背景设置在一个范围内，一旦超过了这个范围就重新开始]]></content>
      <categories>
        <category>Unity</category>
        <category>入门</category>
      </categories>
      <tags>
        <tag>Unity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS17的配置问题]]></title>
    <url>%2F2019%2F06%2F07%2FVS17%E7%9A%84%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[无法打开源文件问题 直接从老师那里得到的项目，第一次运行直接出错，查了一下主要需要注意下面几个 装VS的时候有没有装标准库之类的 尝试使用window的其他版本的SDK 右键项目，属性里面 上面两个问题都可以在VS的installer里面找到相关的安装 多个main的问题 这回的文件里面有两个cpp都带着main，如果想要运行的话需要把一个cpp右键移除出项目再运行另一个]]></content>
      <categories>
        <category>IDE</category>
        <category>Visual Studio</category>
      </categories>
      <tags>
        <tag>VS17</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的复制和多维数组]]></title>
    <url>%2F2019%2F06%2F05%2Fpython%E7%9A%84%E5%A4%8D%E5%88%B6%E5%92%8C%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[最近写面试题的时候遇到了自己都想不到的奇怪小错误 多维数组在创建多维数组的时候，本来应该是用嵌套的for循环来生成[[0 for i in range(m)] for j in range(n)](因为一般网测不能调用numpy，不然就直接用numpy搞了) 但是最近想要偷懒的时候尝试用 [[0] * n] * m 来创建，结果疯狂遭遇bug。 最终原因是发现这样创建出来的数组，每一行都是第一行的引用，所以每次操作大家都会一起变 (但是这种方法可以创建一维的) list的复制我一直以为a=b就是list的复制了，但是并不是这样的！！这样的话a是一个关于b的reference，并不是复制b，改变的时候是会一起改变的 下面几种方法可以用： a = list(b) a = b[:] a = b * 1 a = copy.copy(b) #需要import copy]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
        <category>list</category>
      </categories>
      <tags>
        <tag>list复制</tag>
        <tag>多维数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于字符串匹配算法KMP和BM]]></title>
    <url>%2F2019%2F06%2F05%2F%E5%85%B3%E4%BA%8E%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95KMP%E5%92%8CBM%2F</url>
    <content type="text"><![CDATA[今天在leetcode终于刷到了string的题，是在一大串字符里面匹配相应的字符。托python的福，居然被我用暴力破解解决了，虽然结果不是很优雅。123456789class Solution: def strStr(self, haystack: str, needle: str) -&gt; int: if needle == "": return 0 for i, ch in enumerate(haystack): if ch == needle[0]: if needle == haystack[i:i+len(needle)]: return i return -1 KMP 首先，把这个需要检测的string（a）的第一个和目标string（t）的第一个进行对比，如果不匹配，后移一位 直到找到第一个相同的字母，然后把a和b同时后移一位 还是相同，继续后移 不同（这部分是这个代码的精髓） 一般的思路是把b全都后移一位，但是这样其实消耗挺大的 KMP的思路是，既然b的前n位都已经比较过了，那就不要放弃这个信息，不要移动回之前比较过的n位了，继续后移移动到全新的位置 需要移动的位数 = 已经匹配到的字数 - 对应最后一位匹配上的东西的匹配值（由partial match table得出） 如何产生这张表比如一个单词 bread 前缀：b,br,bre,brea 后缀：read,ead,ad,d部分匹配值就是前缀和后缀最长的共有元素长度 比如ABCDABD这个string A前后缀都是空的，长度0 AB，前缀A，后缀B，共有长度0 ABC，[A, AB]，后缀为[BC, C]，共有元素的长度0 ABCD，[A,AB,ABC],[B,BC,BCD] -&gt; 0 ABCDA, [A,AB,ABC,ABCD],[BCDA,CDA,DA,A] -&gt; 有一个共有元素A，长度为1 ABCDAB,[A, AB, ABC, ABCD, ABCDA],[BCDAB, CDAB, DAB, AB, B] -&gt; 共有元素AB，长度为2 “ABCDABD”的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。（难点：怎么得到上面的这个长度） BM 首先将a和b的头对其开始，然后从尾部开始比较。因为如果尾部不匹配的话，这整个一串都不匹配了。知道了这个不匹配的字符之后，这个字符就被称为坏字符 如果这个坏字符包括在单词里面，则需要把这两个对其 后移位数 = 坏字符的位置 - 搜索词中的上一次出现位置 如果不包含在搜索词里面，那么上一次的位置是-1 最后一位匹配上了，那么就顺着b往前捋 在a里面可以和b后面匹配上的都是good suffix（比如example的e，le，ple等等） 好后缀的后移：后移位数 = 好后缀的位置（最后一个字符为准） - 搜索词中的上一次出现位置 如果没有出现过是 -1 如果有多个好后缀，那么除了最长的好后缀，其他的上一次出现位置必须在头部（b的头部） 在上面两个规则里面，选择移动的最大值 上面两个规则只和搜索词有关，和原来的字符串没关系，所以可以提前生成坏字符和好后缀表，直接比较移动位数]]></content>
      <categories>
        <category>算法</category>
        <category>字符串处理</category>
      </categories>
      <tags>
        <tag>KMP</tag>
        <tag>BM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于python的threading]]></title>
    <url>%2F2019%2F06%2F03%2F%E5%85%B3%E4%BA%8Epython%E7%9A%84threading%2F</url>
    <content type="text"><![CDATA[多线程多线程类似于同时执行多个任务 可以把占用时间长的程序放到后台去处理 可以使用户界面更加吸引人（比如点击按钮，会出现进度条） 处理速度更快]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于动态规划(dynamic programmin)]]></title>
    <url>%2F2019%2F05%2F24%2F%E5%85%B3%E4%BA%8E%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92DP%2F</url>
    <content type="text"><![CDATA[入门ref：https://www.zhihu.com/question/23995189 例子在身上带着不同数额的钞票，目标是凑出来某个金额w，使用尽量少的钞票 如果用贪心算法，实际上就是尽快让w变得更小，有更大面值的就用更大面值的钞票 但是如果换了一组其他的钞票面值，可能就会出现问题（比如 1，5，11凑15） 因为在贪心算法里面，需要先把15降成4，再把4降下来，但是降4的成本很高，需要4张1 在考虑的时候鼠目寸光，只考虑了眼前的情况，没有考虑后续的发展 如果开始列举，其实这个问题就会变成接下来需要凑出来n，需要f(n)张钞票 这时候，凑15其实就变成了三个情况 f(4) + 1 f(10) + 1 f(14) +1 可以发现实际上f(15)只和这三个值有关系，也就是只和n-1，n-5，n-11有关系 f(n) = min(f(n-1),f(n-5),f(n-11))+1 这是一个可以迭代的式子对不对！ 并不关心到底是怎么凑出来的，反正只关心f(w)的值 在代码实现上面，只需从小到大对比所有的cost就可以了，也就是对比新的方案的cost是不是会比以前的方案小。注意在求的时候可能会需要i-1/-5/-11的值，所以要把从头到尾的值都记录下来 比如要求凑15块钱，会先考虑15比1大，那么把1块拿出来，看看取14块钱的时候需要的步骤是多少，然后把5块拿出来，看看比拿1块差多少，最后拿11，看看和之前的cost差多少 区别 dp和贪心算法的区别就在于，dp会分别算出不同策略的代价，而贪心算法包含着冗余的信息（到底怎么使用） 所以就是求出来fn -&gt; 得到求fn需要的fc -&gt; 求fc，不停的循环 也就是把一个问题拆成了不同的子问题 概念 后无效性 一旦fn确定，我们就不需要知道怎么得到的fn了，只在后面直接用就可以了 最优子结构 在得到fn的时候本身得到的就是最优的fn了，所以在用的时候才可以放心的用 一旦问题可以拆成子问题，并且满足上面的两个概念，就可以用dp解了 为什么快 dp和贪心都是在空间里寻找最优解，但是dp在找解的时候已经找到了子问题的最优解，也就是说他已经把子问题里面不可能的状态排除掉了 算法设计 把现在面对的局面看做x 对于x，需要求得答案是fx，目标是求出来fT，找出x和哪些局面p有关，写出一个状态专业方程，来求fp到fx的关系 也就是考虑现在我是谁，和我从哪里来（或者我到哪里去）]]></content>
      <categories>
        <category>算法</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231nAssignment3StyleTransfer]]></title>
    <url>%2F2019%2F05%2F22%2FCS231nAssignment3StyleTransfer%2F</url>
    <content type="text"><![CDATA[target 现在有两张图片，需要产生一些新的图片是一张图片的内容但是是另一张图片的style 首先我们希望可以构建一个loss function，可以连接style和每个不同的image，然后在每个图片的pixel上面降低gradient 在这个里面用squeezeNet（在ImageNet上面pretrain的）来提取图片的feature 预先设定好的函数 因为在这部分直接处理的是jpeg的图片而不是cifar-10的图片了，所以在这部分需要对出片进行预处理 同时需要设定一个dtype = torch.FloatTensor 来设计是用CPU跑还是用GPU跑（GPU的里面会带cuda） CNN = torchvision.models.squeezenet1_1(pretrained=True).features提取squeezenet的model，并且设定CNN的type等于上面设定好的dtype 因为不需要再进行训练了，需要把cnn里面的所有自动计算grad的功能关掉 提取特征 输入 x，一个tensor，大小是(N,C,H,W),里面是一个minibatch的数据 cnn，刚才载入好的model 输出 features，一个list，features[i]的大小是(N,C_i,H_i,W_i) 在不同层得到的feature会有不同的channel的数量以及H和W的大小 实现： 在具体的代码实现里面，直接用value得到每一层之后的结果，下一层的输入就是上一层得到的结果123456789def extract_features(x, cnn): features = [] prev_feat = x for i, module in enumerate(cnn._modules.values()): next_feat = module(prev_feat) features.append(next_feat) prev_feat = next_feat return features 计算lossloss一共由三个部分组成，分别是：图片content的loss + style的loss + total var loss 我们这个东西的目的是用一张图片的内容和另一个图片的style 当内容偏离了content图片的content，style偏离了stype图片的时候就需要penalize（处罚） 为了实现这个功能，我们需要用hybrid的loss，并且不是在weights上面调参，而是在每张图片的pixel上面调整 content loss 这个函数衡量生成的图片的feature map和原来作为content的图片偏离多少 我们只关心这个network里面的一层的表示，这一层会有自己特定的channel数量以及filter的大小 我们需要把这个feature map reshape，把所有的空间位置组合到同一个维度上面 但是在实际的实现上面，我们不需要再reshape了，因为大小可以直接对应处理了 123456789101112131415161718def content_loss(content_weight, content_current, content_original): """ Compute the content loss for style transfer. Inputs: - content_weight: Scalar giving the weighting for the content loss. - content_current: features of the current image; this is a PyTorch Tensor of shape (1, C_l, H_l, W_l). - content_target: features of the content image, Tensor with shape (1, C_l, H_l, W_l). Returns: - scalar content loss """ # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** return content_weight * torch.sum((content_original - content_current)**2) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** style loss对于一个给定的层layer，定义loss 计算Gram Mat，G，表示不同filter的相关性。这个矩阵是个协方差矩阵，我们希望形成的图片的activation 统计和style图片的可以match，计算这两个的协方差就是一个办法（并且经过验证效果比较好） 给定一个feature map，G矩阵的形状应该是（Cl，Cl）。Cl是这一层的filter的数量。里面的元素应该等于两个filter的乘积 把生成图片的G和style图片的G做差，平方和就是一层的loss 所有层的loss加在一起就是总共的loss G Mat implement view(),形成一个内容相同但是大小不同的tensor .matmul 两个tensor相乘 .permute 给tensor里面的维度换位12345678910111213141516171819202122232425262728293031def gram_matrix(features, normalize=True): """ Compute the Gram matrix from features. Inputs: - features: PyTorch Tensor of shape (N, C, H, W) giving features for a batch of N images. - normalize: optional, whether to normalize the Gram matrix If True, divide the Gram matrix by the number of neurons (H * W * C) Returns: - gram: PyTorch Tensor of shape (N, C, C) giving the (optionally normalized) Gram matrices for the N input images. """ # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** N,C,H,W = features.size() # N,C,M features = features.view(N,C,H*W) # N,C,M x N,M,C -&gt; N,C,C gram = features.matmul(features.permute(0,2,1)) if normalize==True: gram /= (H*W*C) return gram # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** loss implement 输入 feats：现在图片的每一层的feature，从上面的提取特征函数得到 style_layers：indices style_targets：和上面的长度相同，计算的是第i层原图片得到的G Mat style_weights：scalar 在计算的时候只需要考虑每一层里面计算出来的现在的G Mat（注意索引不是i）和原图片的G，和上面一样的计算就可以了 123456789101112131415161718192021222324252627282930313233# Now put it together in the style_loss function...def style_loss(feats, style_layers, style_targets, style_weights): """ Computes the style loss at a set of layers. Inputs: - feats: list of the features at every layer of the current image, as produced by the extract_features function. - style_layers: List of layer indices into feats giving the layers to include in the style loss. - style_targets: List of the same length as style_layers, where style_targets[i] is a PyTorch Tensor giving the Gram matrix of the source style image computed at layer style_layers[i]. - style_weights: List of the same length as style_layers, where style_weights[i] is a scalar giving the weight for the style loss at layer style_layers[i]. Returns: - style_loss: A PyTorch Tensor holding a scalar giving the style loss. """ # Hint: you can do this with one for loop over the style layers, and should # not be very much code (~5 lines). You will need to use your gram_matrix function. # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** loss = torch.tensor(0.).type(dtype) for i in range(len(style_layers)): G_Mat = gram_matrix(feats[style_layers[i]]) loss_layer = style_weights[i] * torch.sum((style_targets[i] - G_Mat)**2) loss += loss_layer return loss # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** total-variation reg 为了让图片显示的内容更加平滑，加入了这个惩罚部分 计算的方法可以是计算每个像素和它相邻像素的差的平方和（相邻像素分别包括垂直和水平） 需要让结果vec化，直接用-1把矩阵错位一个 123456789101112131415161718192021def tv_loss(img, tv_weight): """ Compute total variation loss. Inputs: - img: PyTorch Variable of shape (1, 3, H, W) holding an input image. - tv_weight: Scalar giving the weight w_t to use for the TV loss. Returns: - loss: PyTorch Variable holding a scalar giving the total variation loss for img weighted by tv_weight. """ # Your implementation should be vectorized and not require any loops! # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** H_var = torch.sum((img[:,:,1:,:] - img[:,:,:-1,:])**2) W_var = torch.sum((img[:,:,:,1:] - img[:,:,:,:-1])**2) return (H_var + W_var) * tv_weight # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** 已经写好了转化style的函数 首先提取content和style图片的特征 然后初始化需要生成的图片，这张图片上面需要打开grad 设置好hyper，设定好optimizer 然后在一定的范围里，用cnn提取现在图片的特征 用现在的特征计算loss，然后改变现在的图片]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>style transfer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231nassignment3Vis]]></title>
    <url>%2F2019%2F05%2F21%2FCS231nassignment3Vis%2F</url>
    <content type="text"><![CDATA[Network Visualization (PyTorch) 在这部分用了一个已经在ImageNet上面pretrain过的CNN 用这个CNN来定义一个loss function，然后用这个loss来测量现在的不高兴程度 back的时候计算这个loss对于每个像素的gradient 保持这个model不变，但是在图片上面展示出来gradients的下降，形成让loss最小的图片 这个作业一共分成三个部分： saliency map：一个比较快的方法来展示这个图片哪个部分影响了net分类的决定 fooling image：扰乱一个图片，让他看起来跟人似的，但是会被误分类 class visualization：形成可以得到最大分类得分的图片 注意这里需要先激活conda，不然在jupter里面torch会报错 事先处理 事先定义了函数preprocess的部分，因为pretrain的时候也是提前进行好了预处理 需要下载下来预处理的模型，这里用的是SqueezeNet，因为这样可以直接在CPU上面形成图片 读取一部分ImageNet里面的图片看一看是什么样子的 saliency maps saliency告诉我们每个pixel对分类得分的影响 为了计算这个东西，我们需要计算没有正则化之前的score对于正确分类的gradient（具体到每个pixel） 比如图片的大小是3xHxW，那么得到的gradient的形状也应该是3xHxW 表示的就是这个pixel改变的话对于整个结果改变的影响 为了计算，我们取每个gradient的绝对值，然后取三个channel里面的最大值，最后得到的大小是HxW gather method 就像在assignment1里面选择一个矩阵里面的最大值一样，gather这个方法就是在s.gather(1, y.view(-1, 1)).squeeze()一个N，C的矩阵s里面选择对应的y那个的值然后形成一个行的数组 compute_saliency_map 输入： X:输入的图片 (N,3,H,W) y:label (N,) model:预训练好的模型 输出： saliency，大小是（N，H，W） 注意，因为torch这个对象自己本来就已经带着grad了，所以直接求出来就可以了，但是注意需要定义一下backward之后的大小应该是多少 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def compute_saliency_maps(X, y, model): """ Compute a class saliency map using the model for images X and labels y. Input: - X: Input images; Tensor of shape (N, 3, H, W) - y: Labels for X; LongTensor of shape (N,) - model: A pretrained CNN that will be used to compute the saliency map. Returns: - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input images. """ # Make sure the model is in "test" mode model.eval() # Make input tensor require gradient X.requires_grad_() saliency = None ############################################################################## # TODO: Implement this function. Perform a forward and backward pass through # # the model to compute the gradient of the correct class score with respect # # to each input image. You first want to compute the loss over the correct # # scores (we'll combine losses across a batch by summing), and then compute # # the gradients with a backward pass. # ############################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** #forward #NxC scores = model(X) #N correct_scores = scores.gather(1,y.view(-1,1)).squeeze() #backward correct_scores.backward(torch.ones(correct_scores.size())) saliency = X.grad saliency = saliency.abs() saliency,_ = torch.max(saliency, dim = 1) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return saliency fooling images 可以生成fooling image，给一个image和一个目标class，我们让gradient一直升高，去让目标的score最大，一直到最后的分类是目标的分类 输入 X (1,3,224,224) target_y 在0-1000的范围里面 model 预训练的CNN 输出： x_fooling TODO When computing an update step, first normalize the gradient:# dX = learning_rate * g / ||g||_2 需要自己写一个训练的部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def make_fooling_image(X, target_y, model): """ Generate a fooling image that is close to X, but that the model classifies as target_y. Inputs: - X: Input image; Tensor of shape (1, 3, 224, 224) - target_y: An integer in the range [0, 1000) - model: A pretrained CNN Returns: - X_fooling: An image that is close to X, but that is classifed as target_y by the model. """ # Initialize our fooling image to the input image, and make it require gradient X_fooling = X.clone() X_fooling = X_fooling.requires_grad_() learning_rate = 1 ############################################################################## # TODO: Generate a fooling image X_fooling that the model will classify as # # the class target_y. You should perform gradient ascent on the score of the # # target class, stopping when the model is fooled. # # When computing an update step, first normalize the gradient: # # dX = learning_rate * g / ||g||_2 # # # # You should write a training loop. # # # # HINT: For most examples, you should be able to generate a fooling image # # in fewer than 100 iterations of gradient ascent. # # You can print your progress over iterations to check your algorithm. # ############################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** for i in range(100): scores = model(X_fooling) index = torch.argmax(scores,dim = 1) if index[0] == target_y: break target_score = scores[0,target_y] target_score.backward() grad = X_fooling.grad.data X_fooling.data += learning_rate * (grad/grad.norm()) X_fooling.grad.zero_() # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return X_fooling class visualization 从一个随机的noise开始然后往目标的class上面增加gradient 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980def create_class_visualization(target_y, model, dtype, **kwargs): """ Generate an image to maximize the score of target_y under a pretrained model. Inputs: - target_y: Integer in the range [0, 1000) giving the index of the class - model: A pretrained CNN that will be used to generate the image - dtype: Torch datatype to use for computations Keyword arguments: - l2_reg: Strength of L2 regularization on the image - learning_rate: How big of a step to take - num_iterations: How many iterations to use - blur_every: How often to blur the image as an implicit regularizer - max_jitter: How much to gjitter the image as an implicit regularizer - show_every: How often to show the intermediate result """ model.type(dtype) l2_reg = kwargs.pop('l2_reg', 1e-3) learning_rate = kwargs.pop('learning_rate', 25) num_iterations = kwargs.pop('num_iterations', 100) blur_every = kwargs.pop('blur_every', 10) max_jitter = kwargs.pop('max_jitter', 16) show_every = kwargs.pop('show_every', 25) # Randomly initialize the image as a PyTorch Tensor, and make it requires gradient. img = torch.randn(1, 3, 224, 224).mul_(1.0).type(dtype).requires_grad_() for t in range(num_iterations): # Randomly jitter the image a bit; this gives slightly nicer results ox, oy = random.randint(0, max_jitter), random.randint(0, max_jitter) img.data.copy_(jitter(img.data, ox, oy)) ######################################################################## # TODO: Use the model to compute the gradient of the score for the # # class target_y with respect to the pixels of the image, and make a # # gradient step on the image using the learning rate. Don't forget the # # L2 regularization term! # # Be very careful about the signs of elements in your code. # ######################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** scores = model(img) target_score = scores[0,target_y] target_score.backward() grad = img.grad.data grad -= 2*l2_reg * img.data img.data += learning_rate * (grad/grad.norm()) img.grad.zero_() # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ######################################################################## # END OF YOUR CODE # ######################################################################## # Undo the random jitter img.data.copy_(jitter(img.data, -ox, -oy)) # As regularizer, clamp and periodically blur the image for c in range(3): lo = float(-SQUEEZENET_MEAN[c] / SQUEEZENET_STD[c]) hi = float((1.0 - SQUEEZENET_MEAN[c]) / SQUEEZENET_STD[c]) img.data[:, c].clamp_(min=lo, max=hi) if t % blur_every == 0: blur_image(img.data, sigma=0.5) # Periodically show the image if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1: plt.imshow(deprocess(img.data.clone().cpu())) class_name = class_names[target_y] plt.title('%s\nIteration %d / %d' % (class_name, t + 1, num_iterations)) plt.gcf().set_size_inches(4, 4) plt.axis('off') plt.show() return deprocess(img.data.cpu())]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231nassignment3LSTM]]></title>
    <url>%2F2019%2F05%2F17%2FCS231nassignment3LSTM%2F</url>
    <content type="text"><![CDATA[targetimplement LSTM update rule and use it for image captioning LSTM 在vanilla RNN里面，可以根据vanilla来计算长的sequence，但是同样会因为不停的乘矩阵导致gradient爆炸的问题，LSTM主要就是用了一个其他的update rule解决了这个问题 和vanilla RNN差不多，现在这个step的x，前一个hidden state。LSTM保持着H-d的cell state，所以也会从前一个接收到前一个的cell state。 LSTM会学一个input-to-hidden的矩阵 4HxD， hidden-to-hidden的矩阵 4HxH， bias 4H 在每一部都会先计算被激活之后的函数（4H），然后把这个结果a分成四个部分，每个部分的大小是H 根据这四个部分计算input gate，forget gate，output gate，block gate 前三个都用sigmoid激活，最后一个用tanh激活 然后用上面的四个参数计算下一个cell state和hidden state step forward 输入的大小是D，hidden的大小是H，minibatch的大小是N input x (N,D) prev_h (N,H) prev_c (N,H) Wx input 2 hidden (D,4H) Wh hidden 2 hidden (H,4H) bias, (4H) output next_h (N,H) next_c (N,H) cache 按照之前给的公式直接计算就行了，其实就是把原来求出来的值分成了四个部分，分别求出来了四个新的值，用这四个新的值的公式可以得到下一个状态的c和h step backward input dnext_h (N,H) 都是上面一个回来的 dnext_c (N,H) cache output dx(N,D) dprev_h (N,H) dprev_c (N,H) dWx (D,4H) dWh (H,4H) db (4H) 按着正方向计算的顺序back回去就可以了，注意这里有个问题就是因为next_c被用来计算next h了，所以dnext_c需要再求一下关于next h的导数，并且把求出来的新的值加在以前的东西上面 后面的矩阵计算尺寸 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b): """ Forward pass for a single timestep of an LSTM. The input data has dimension D, the hidden state has dimension H, and we use a minibatch size of N. Note that a sigmoid() function has already been provided for you in this file. Inputs: - x: Input data, of shape (N, D) - prev_h: Previous hidden state, of shape (N, H) - prev_c: previous cell state, of shape (N, H) - Wx: Input-to-hidden weights, of shape (D, 4H) - Wh: Hidden-to-hidden weights, of shape (H, 4H) - b: Biases, of shape (4H,) Returns a tuple of: - next_h: Next hidden state, of shape (N, H) - next_c: Next cell state, of shape (N, H) - cache: Tuple of values needed for backward pass. """ next_h, next_c, cache = None, None, None ############################################################################# # TODO: Implement the forward pass for a single timestep of an LSTM. # # You may want to use the numerically stable sigmoid implementation above. # ############################################################################# # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** N, H = prev_h.shape # -&gt; N,4H a = x.dot(Wx) + prev_h.dot(Wh) + b a_i = a[:, :H] a_f = a[:, H:2 * H] a_o = a[:, 2 * H:3 * H] a_g = a[:, 3 * H:] i = sigmoid(a_i) f = sigmoid(a_f) o = sigmoid(a_o) g = np.tanh(a_g) next_c = f * prev_c + i * g next_h = o * np.tanh(next_c) cache = (x, prev_h, prev_c, Wx, Wh, a, i, f, o, g, next_c, next_h) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return next_h, next_c, cachedef lstm_step_backward(dnext_h, dnext_c, cache): """ Backward pass for a single timestep of an LSTM. Inputs: - dnext_h: Gradients of next hidden state, of shape (N, H) - dnext_c: Gradients of next cell state, of shape (N, H) - cache: Values from the forward pass Returns a tuple of: - dx: Gradient of input data, of shape (N, D) - dprev_h: Gradient of previous hidden state, of shape (N, H) - dprev_c: Gradient of previous cell state, of shape (N, H) - dWx: Gradient of input-to-hidden weights, of shape (D, 4H) - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H) - db: Gradient of biases, of shape (4H,) """ dx, dprev_h, dprev_c, dWx, dWh, db = None, None, None, None, None, None ############################################################################# # TODO: Implement the backward pass for a single timestep of an LSTM. # # # # HINT: For sigmoid and tanh you can compute local derivatives in terms of # # the output value from the nonlinearity. # ############################################################################# # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** N, H = dnext_h.shape x, prev_h, prev_c, Wx, Wh, a, i, f, o, g, next_c, next_h = cache # dnext_h/do -&gt; do do = np.tanh(next_c) * dnext_h # dnext_h/dnext_c dnext_c = o * (1 - np.tanh(next_c) ** 2) * dnext_h + dnext_c # dnext_c/df -&gt; df df = prev_c * dnext_c # dnext_c/dprev_c dprev_c = f * dnext_c # dnext_c/di di = g * dnext_c # dnext_c/dg dg = i * dnext_c da = np.zeros((N, 4 * H)) # sigmoid i da[:, :H] = i * (1 - i) * di da[:, H:2 * H] = f * (1 - f) * df da[:, 2 * H:3 * H] = o * (1 - o) * do da[:, 3 * H:] = (1 - g * g) * dg # a = x.dot(Wx) + prev_h.dot(Wh) + b # N,4H D,4H dx = da.dot(Wx.T) # N,D N,4H dWx = x.T.dot(da) # N,4H H,4H dprev_h = da.dot(Wh.T) # N,H N,4H dWh = prev_h.T.dot(da) # da N,4H db = np.sum(da, axis=0) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return dx, dprev_h, dprev_c, dWx, dWh, db forward 输入了一大串data，假设输入的data包括了T个vector，每个的dim是D，用的hidden的大小是H，在N的minibatch上面进行，返回对于所有time step的hidden state 初始化的cell是0，不会return cell state，只是LSTM自己的变量 输入 x (N,T,D) h0, (N,H) Wx (D,4H) Wh (H,4H) b (4H) out h (N,T,D) cache 注意h是需要初始化为0的，每次for里面拿出来的是h里面的一部分来赋值 backward 和之前的差不多，注意W和b都是要积累的，之前都是要初始化的 而且back的时候要用reversed的顺序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104def lstm_forward(x, h0, Wx, Wh, b): """ Forward pass for an LSTM over an entire sequence of data. We assume an input sequence composed of T vectors, each of dimension D. The LSTM uses a hidden size of H, and we work over a minibatch containing N sequences. After running the LSTM forward, we return the hidden states for all timesteps. Note that the initial cell state is passed as input, but the initial cell state is set to zero. Also note that the cell state is not returned; it is an internal variable to the LSTM and is not accessed from outside. Inputs: - x: Input data of shape (N, T, D) - h0: Initial hidden state of shape (N, H) - Wx: Weights for input-to-hidden connections, of shape (D, 4H) - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H) - b: Biases of shape (4H,) Returns a tuple of: - h: Hidden states for all timesteps of all sequences, of shape (N, T, H) - cache: Values needed for the backward pass. """ h, cache = None, None ############################################################################# # TODO: Implement the forward pass for an LSTM over an entire timeseries. # # You should use the lstm_step_forward function that you just defined. # ############################################################################# # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** N, T, D = x.shape N, H = h0.shape prev_h = h0 prev_c = np.zeros((N, H)) cache = &#123;&#125; h = np.zeros((N, T, H)) for step in range(T): prev_h, prev_c, cache_step = lstm_step_forward( x[:, step, :], prev_h, prev_c, Wx, Wh, b) h[:, step, :] = prev_h cache[step] = cache_step # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return h, cachedef lstm_backward(dh, cache): """ Backward pass for an LSTM over an entire sequence of data.] Inputs: - dh: Upstream gradients of hidden states, of shape (N, T, H) - cache: Values from the forward pass Returns a tuple of: - dx: Gradient of input data of shape (N, T, D) - dh0: Gradient of initial hidden state of shape (N, H) - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H) - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H) - db: Gradient of biases, of shape (4H,) """ dx, dh0, dWx, dWh, db = None, None, None, None, None ############################################################################# # TODO: Implement the backward pass for an LSTM over an entire timeseries. # # You should use the lstm_step_backward function that you just defined. # ############################################################################# # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** x = cache[0][0] # 注意这是一个step里面的x，大小是N，D N, D = x.shape _, T, H = dh.shape dx = np.zeros((N, T, D)) dprev_h = np.zeros((N, H)) dprev_c = np.zeros((N, H)) dh0 = np.zeros((N, H)) dWx = np.zeros((D, 4 * H)) dWh = np.zeros((H, 4 * H)) db = np.zeros(4 * H) for step in reversed(range(T)): dnext_h = dh[:, step, :] + dprev_h dnext_c = dprev_c dx[:, step, :], dprev_h, dprev_c, dWx_temp, dWh_temp, db_temp = lstm_step_backward( dnext_h, dnext_c, cache[step]) dWx += dWx_temp dWh += dWh_temp db += db_temp dh0 = dprev_h # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return dx, dh0, dWx, dWh, db 剩下的和RNN部分没有什么区别了，主要就是把代码的选项里面加上lstm的部分]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>word captioning</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于github删除和ignore]]></title>
    <url>%2F2019%2F05%2F16%2F%E5%85%B3%E4%BA%8Egithub%E5%88%A0%E9%99%A4%E5%92%8Cignore%2F</url>
    <content type="text"><![CDATA[removeadd过的文件如果想要都撤销了修改git rm -r --cached .（不小心add之后关上了的情况）如果status之后就发现不对，可以用git reset HEAD &lt;file&gt;，后面加点就是撤销全部的 如果已经push了，可以还原版本12345git revert HEAD 撤销前一次 commit git revert HEAD^ 撤销前前一次 commit git revert commit-id (撤销指定的版本，撤销也会作为一次提交进行保存） (ref：https://blog.csdn.net/kongbaidepao/article/details/52253774) ignore有一些比较大的文件想要忽略掉的，需要在根目录建立一个.gitignore，里面直接放需要的路径就可以了 不要往里面传很大的数据鸭会爆炸的TAT]]></content>
      <categories>
        <category>Github</category>
      </categories>
      <tags>
        <tag>gitignore</tag>
        <tag>remove</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment3RNN]]></title>
    <url>%2F2019%2F05%2F10%2FCS231Nassignment3RNN%2F</url>
    <content type="text"><![CDATA[assignment3 targetIn this exercise you will implement a vanilla recurrent neural networks and use them it to train a model that can generate novel captions for images. Microsoft COCO 在这次的作业里用的是Microsoft的coco dataset，已经是一个很常用的给文字配上说明文（captioning）的dataset了，有80000个训练和40000个val，每个图片包含一个五个字的注释 在这个作业里已经preprocess了data，每个图片已经从VGG-16（ImageNet pretrain）layer 7提取了feature，存在了train2014_vgg16_fc7.h5和val2014_vgg16_fc7.h5 为了减少处理的时间和内存，feature的特征从4096降到了512 真实的图片太大了，所以把图片的url存在了txt里面，这样在vis的时候可以直接下载这些图片（必须联网） 直接处理string的效率太低了，所以在caption的一个encoded版本上面进行处理，这样可以把string表示成一串int。在dataset里面也有这两个之间转换的信息 -&gt; 在转换的时候也加了更多的tokens 事先看了一下图片和对应的语句 RNN 在这章要用rnn language model来进行image captioning cs231n/rnn_layers.py step forwardvanilla RNN的single timestep，用tanh来激活。输入data的大小是D，hidden layer的大小是H，minibatch的大小是N 输入 x(N,D) prev_h:前一个timestep的hidden (N,H) Wx:input- to- hidden connections (D,H) Wh:hidden-to-hidden connections (H,H) b:bias,(H,) 返回(tuple): next_h:下一个hidden state，(N,H) cache:back需要的数据 构成: RNN用的就是上一个的h，这一个的x同时乘以不同的参数，合在一起预测这一次的h 对于某个时间点上的输入，还需要上一个的state h，参数W，乘在一起得到新的state 这个参数的W无论在哪个步骤里面使用，一直都是一样的 1234567891011121314151617181920212223242526272829303132333435363738def rnn_step_forward(x, prev_h, Wx, Wh, b): """ Run the forward pass for a single timestep of a vanilla RNN that uses a tanh activation function. The input data has dimension D, the hidden state has dimension H, and we use a minibatch size of N. Inputs: - x: Input data for this timestep, of shape (N, D). - prev_h: Hidden state from previous timestep, of shape (N, H) - Wx: Weight matrix for input-to-hidden connections, of shape (D, H) - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H) - b: Biases of shape (H,) Returns a tuple of: - next_h: Next hidden state, of shape (N, H) - cache: Tuple of values needed for the backward pass. """ next_h, cache = None, None ############################################################################## # TODO: Implement a single forward step for the vanilla RNN. Store the next # # hidden state and any values you need for the backward pass in the next_h # # and cache variables respectively. # ############################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** x_1 = x.dot(Wx) h_1 = prev_h.dot(Wh) x_raw = x_1 + h_1 + b next_h = np.tanh(x_raw) cache = (x, prev_h, Wx, Wh, x_raw, next_h) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return next_h, cache step backward 输入： dnext，下一个state的loss的gradient，(N,H) cache 输出： dx:input的gradient，(N,D) dprev_h:前一个hidden state的gradient，(N,H) dWx:Wx的gradient,(D,H) dWh:Wh的gradient，(H,H) db:bias的gradient，(H,) 其实这个求起来gradient更简单了，因为每一个的导数都很好求，搞对了矩阵的形状就可以了 123456789101112131415161718192021222324252627282930313233343536373839404142434445def rnn_step_backward(dnext_h, cache): """ Backward pass for a single timestep of a vanilla RNN. Inputs: - dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H) - cache: Cache object from the forward pass Returns a tuple of: - dx: Gradients of input data, of shape (N, D) - dprev_h: Gradients of previous hidden state, of shape (N, H) - dWx: Gradients of input-to-hidden weights, of shape (D, H) - dWh: Gradients of hidden-to-hidden weights, of shape (H, H) - db: Gradients of bias vector, of shape (H,) """ dx, dprev_h, dWx, dWh, db = None, None, None, None, None ############################################################################## # TODO: Implement the backward pass for a single step of a vanilla RNN. # # # # HINT: For the tanh function, you can compute the local derivative in terms # # of the output value from tanh. # ############################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** x, prev_h, Wx, Wh, x_raw, next_h = cache # d(tanh) = 1 - tanh * tanh # NxH dx_raw = (1 - next_h * next_h) * dnext_h # H, db = np.sum(dx_raw, axis=0) # N,D .T x N,H -&gt; DxH dWx = x.T.dot(dx_raw) # N H x D,H dx = dx_raw.dot(Wx.T) # N,H .T x N,H dWh = prev_h.T.dot(dx_raw) # N,H dprev_h = dx_raw.dot(Wh.T) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return dx, dprev_h, dWx, dWh, db forward + backwoard 刚才只是实现了每一步的forward和backward，现在要实现整个的这个过程了 forward 假设输入的是一系列由T个vector组成的，每个的大小是D minibatch的大小是N，hidden的大小是H，返回整个timesetps里面的hidden state 输入 整个timestep里面的数据x(N,T,D) h0，初始化的hidden state(N,H) Wx (D,H) Wh (H,H) b (H,) 输出 h整个timestep里面的states(N,T,H) cache 实际上就是首先设置了最开始的输入h0，然后在时间循环T里面不停的调用上面已经写好的step的函数，更新prev_h，把不同的值存在cache里面 注意h需要初始化！！ backward 输入了dh和cache，需要输出所有东西的gradient 思路主要是每一个step里面是加的关系，所以对于dWx，dWh和db来说，需要在每次遍历里面加上之前的值，相当于每次都需要加上新的东西 back的时候需要next的时候来求现在的，然后在下一轮把next更新成现在的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899def rnn_forward(x, h0, Wx, Wh, b): """ Run a vanilla RNN forward on an entire sequence of data. We assume an input sequence composed of T vectors, each of dimension D. The RNN uses a hidden size of H, and we work over a minibatch containing N sequences. After running the RNN forward, we return the hidden states for all timesteps. Inputs: - x: Input data for the entire timeseries, of shape (N, T, D). - h0: Initial hidden state, of shape (N, H) - Wx: Weight matrix for input-to-hidden connections, of shape (D, H) - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H) - b: Biases of shape (H,) Returns a tuple of: - h: Hidden states for the entire timeseries, of shape (N, T, H). - cache: Values needed in the backward pass """ h, cache = None, None ############################################################################## # TODO: Implement forward pass for a vanilla RNN running on a sequence of # # input data. You should use the rnn_step_forward function that you defined # # above. You can use a for loop to help compute the forward pass. # ############################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** N, T, D = x.shape N, H = h0.shape h = np.zeros((N, T, H)) prev_h = h0 cache = &#123;&#125; for i in range(T): prev_h, cache_i = rnn_step_forward(x[:, i, :], prev_h, Wx, Wh, b) h[:, i, :] = prev_h cache[i] = cache_i # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return h, cachedef rnn_backward(dh, cache): """ Compute the backward pass for a vanilla RNN over an entire sequence of data. Inputs: - dh: Upstream gradients of all hidden states, of shape (N, T, H). NOTE: 'dh' contains the upstream gradients produced by the individual loss functions at each timestep, *not* the gradients being passed between timesteps (which you'll have to compute yourself by calling rnn_step_backward in a loop). Returns a tuple of: - dx: Gradient of inputs, of shape (N, T, D) - dh0: Gradient of initial hidden state, of shape (N, H) - dWx: Gradient of input-to-hidden weights, of shape (D, H) - dWh: Gradient of hidden-to-hidden weights, of shape (H, H) - db: Gradient of biases, of shape (H,) """ dx, dh0, dWx, dWh, db = None, None, None, None, None ############################################################################## # TODO: Implement the backward pass for a vanilla RNN running an entire # # sequence of data. You should use the rnn_step_backward function that you # # defined above. You can use a for loop to help compute the backward pass. # ############################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** N, T, H = dh.shape N, D = cache[0][0].shape dx = np.zeros((N, T, D)) dh0 = np.zeros((N, H)) dWx = np.zeros((D, H)) dWh = np.zeros((H, H)) db = np.zeros(H) dprev_h = np.zeros((N, H)) for i in reversed(range(T)): cache_i = cache[i] dnext_h = dh[:, i, :] + dprev_h dx[:, i, :], dprev_h, dWx_tmp, dWh_tmp, db_tmp = rnn_step_backward( dnext_h, cache_i) dWx += dWx_tmp dWh += dWh_tmp db += db_tmp dh0 = dprev_h # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return dx, dh0, dWx, dWh, db word embedding在深度学习的系统里面主要是用vector来表示单词的，字典里面的每一个都会关系到一个vector，然后这些vectors会和系统的其他部分一起学习在这部分需要把int表示的单词转化成vectors 理解 在一句话里面，一个单词就是一个维度，而word embedding的核心就是降维 把字组成段落，然后用段落来总结出来最后的核心内容 forward 一个minibatch的大小是N，长度是T，把每个单词给到一个大小是D的vector input x (N,T)一个N个数据，每个数据里面T个单词，T给出来的是单词的indice W (V,D)给所有word的vectors return out：(N,T,D)给所有单词一个D的vector cache backward back的时候不能back到word（因为是int），所以只需要得到embedding mat的gradient 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566def word_embedding_forward(x, W): """ Forward pass for word embeddings. We operate on minibatches of size N where each sequence has length T. We assume a vocabulary of V words, assigning each word to a vector of dimension D. Inputs: - x: Integer array of shape (N, T) giving indices of words. Each element idx of x muxt be in the range 0 &lt;= idx &lt; V. - W: Weight matrix of shape (V, D) giving word vectors for all words. Returns a tuple of: - out: Array of shape (N, T, D) giving word vectors for all input words. - cache: Values needed for the backward pass """ out, cache = None, None ############################################################################## # TODO: Implement the forward pass for word embeddings. # # # # HINT: This can be done in one line using NumPy's array indexing. # ############################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** out = W[x, :] cache = x, W # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return out, cachedef word_embedding_backward(dout, cache): """ Backward pass for word embeddings. We cannot back-propagate into the words since they are integers, so we only return gradient for the word embedding matrix. HINT: Look up the function np.add.at Inputs: - dout: Upstream gradients of shape (N, T, D) - cache: Values from the forward pass Returns: - dW: Gradient of word embedding matrix, of shape (V, D). """ dW = None ############################################################################## # TODO: Implement the backward pass for word embeddings. # # # # Note that words can appear more than once in a sequence. # # HINT: Look up the function np.add.at # ############################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** x, W = cache dW = np.zeros_like(W) np.add.at(dW, x, dout) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################## # END OF YOUR CODE # ############################################################################## return dW Temporal Affine layer 在每个timestep的时候，我们需要一个affine来把RNN的hidden vector转换成每个单词在vocabulary里面的scores（原来是根据这个来评分的，然后每次选出来一个合适的单词） 因为和之前做过的一样，所以直接提供了 Temporal SOftmax loss 在RNN的结构里面，每个timestep会生成一个对于vocabulary里面所有单词的score(矩阵) 在每步里面都知道ground truth，所以用softmax来计算每一步的loss和gradient，然后计算一个minibatch里面所有时间的平均loss 因为每个句子不一定一样长，所以在里面加上了NULL的token，让所有东西一边长，但是在计算loss的时候不希望计算这个NULL。所以还会接收一个mask来告诉这个函数哪个地方需要算哪个地方不需要算 RNN for image captioning 在cs231n/classifiers/rnn.py里面，现在只需要考虑vanialla RNN的问题 implement loss里面的forward和backward IO 输入 image features，大小是(N,D) captions：gorund truth，大小是(N,T)其中每个元素应该都在 0-V之间 输出 loss grads TODO affine trans，从图片的特征计算初始化的hidden state，输出的大小是 (N,H) -&gt; W_proj,b_proj -&gt; 这一步初始化的是h0，也就是最开始的状态 word embedding，把输入句子的int（表示在voca里面的位置）转化成vector，输出结果是(N,T,W) vanilla RNN（或者后面的LSTM）来计算中间的timestep里面hidden state的改变，输出结果(N,T,H) temporal affine来把每一步的结果转化成在vocabulary上面的score，(N,T,V) temporal softmax把score转化成loss，注意需要忽略mask上面没有的 在back的时候需要计算loss关于所有参数的gradient，存在上面的dict里面 实际上直接按照之前写好的一直操作就可以了！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105def loss(self, features, captions): """ Compute training-time loss for the RNN. We input image features and ground-truth captions for those images, and use an RNN (or LSTM) to compute loss and gradients on all parameters. Inputs: - features: Input image features, of shape (N, D) - captions: Ground-truth captions; an integer array of shape (N, T) where each element is in the range 0 &lt;= y[i, t] &lt; V Returns a tuple of: - loss: Scalar loss - grads: Dictionary of gradients parallel to self.params """ # Cut captions into two pieces: captions_in has everything but the last word # and will be input to the RNN; captions_out has everything but the first # word and this is what we will expect the RNN to generate. These are offset # by one relative to each other because the RNN should produce word (t+1) # after receiving word t. The first element of captions_in will be the START # token, and the first element of captions_out will be the first word. captions_in = captions[:, :-1] captions_out = captions[:, 1:] # You'll need this mask = (captions_out != self._null) # Weight and bias for the affine transform from image features to initial # hidden state W_proj, b_proj = self.params['W_proj'], self.params['b_proj'] # Word embedding matrix W_embed = self.params['W_embed'] # Input-to-hidden, hidden-to-hidden, and biases for the RNN Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b'] # Weight and bias for the hidden-to-vocab transformation. W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab'] loss, grads = 0.0, &#123;&#125; ############################################################################ # TODO: Implement the forward and backward passes for the CaptioningRNN. # # In the forward pass you will need to do the following: # # (1) Use an affine transformation to compute the initial hidden state # # from the image features. This should produce an array of shape (N, H)# # (2) Use a word embedding layer to transform the words in captions_in # # from indices to vectors, giving an array of shape (N, T, W). # # (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to # # process the sequence of input word vectors and produce hidden state # # vectors for all timesteps, producing an array of shape (N, T, H). # # (4) Use a (temporal) affine transformation to compute scores over the # # vocabulary at every timestep using the hidden states, giving an # # array of shape (N, T, V). # # (5) Use (temporal) softmax to compute loss using captions_out, ignoring # # the points where the output word is &lt;NULL&gt; using the mask above. # # # # In the backward pass you will need to compute the gradient of the loss # # with respect to all model parameters. Use the loss and grads variables # # defined above to store loss and gradients; grads[k] should give the # # gradients for self.params[k]. # # # # Note also that you are allowed to make use of functions from layers.py # # in your implementation, if needed. # ############################################################################ # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** # N,D x D,H -&gt; N,H hidden_init, hidden_init_cache = affine_forward( features, W_proj, b_proj) # N,T -&gt; N,T,D embeding, embeding_cache = word_embedding_forward(captions_in, W_embed) # RNN -&gt; N,T,H if self.cell_type == 'rnn': hidden_state, hidden_cache = rnn_forward( embeding, hidden_init, Wx, Wh, b) # N,T,H x H,V -&gt; N,T,V scores, score_cache = temporal_affine_forward( hidden_state, W_vocab, b_vocab) # N,T,V -&gt; loss loss, dloss = temporal_softmax_loss(scores, captions_out, mask) grads = &#123;&#125; # gradient in temporal affine daffine_x, grads['W_vocab'], grads['b_vocab'] = temporal_affine_backward( dloss, score_cache) if self.cell_type == 'rnn': drnn, dh_init, grads['Wx'], grads['Wh'], grads['b'] = rnn_backward( daffine_x, hidden_cache) grads['W_embed'] = word_embedding_backward(drnn, embeding_cache) dfeatures, grads['W_proj'], grads['b_proj'] = affine_backward( dh_init, hidden_init_cache) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################ # END OF YOUR CODE # ############################################################################ return loss, grads overfit small data 和之前一样，写了一个solver来计算，包括了训练model的所有需要的东西，在optim..py里面有很多不同的update的方法 可以接受train或者val的data和label，可以得到训练或者val的acc。在训练之后这个model里面会保存最好的参数，让val最低 在这一步里面，载入了50个coco的训练数据，然后对一个model进行训练，最后得到的loss会小于0.1 test-time sampling 和分类不同，RNN训练和测试得到的结果会非常不相同 训练的时候，我们把ground-truth放进RNN 测试的时候，我们会sample出来每个timestep的单词的分布，然后把这些分布再喂到下一个step里面 implement在每次step里面，我们把现在的单词embed，和前一个hidden state一起输入进RNN里面，得到下一个hidden state，然后得到vocabulary上面的score，选择最有可能的单词然后根据这个单词得到下一个单词 输入： features (N,D) 还没有进行projection的数据 max_length：最长的caption的长度 输出 captions (N,max_length)，里面放的是0-V的int，第一个应该是 TODO 需要把features初始化，然后第一个输入的单词应该是(最开始) 在之后的每一步里面 用已经学习好的参数，embed上一个单词 RNN step，从上一个hidden和现在的embed得到下一个hidden（需要call每一步的函数而不是完整的函数） 把下一个转化成score 在score里面选择最有可能的单词，写出来这个单词的index， 为了简单，在出现之前不用停止 注意： 应该用的是affine来计算score而不是temporal，因为要计算的只是现在这个范围里面的score，计算出来的大小应该是(N,V)，所以应该在每行找到最合适的 每一步计算出来的最大值应该记在相应step的列上 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485def sample(self, features, max_length=30): """ Run a test-time forward pass for the model, sampling captions for input feature vectors. At each timestep, we embed the current word, pass it and the previous hidden state to the RNN to get the next hidden state, use the hidden state to get scores for all vocab words, and choose the word with the highest score as the next word. The initial hidden state is computed by applying an affine transform to the input image features, and the initial word is the &lt;START&gt; token. For LSTMs you will also have to keep track of the cell state; in that case the initial cell state should be zero. Inputs: - features: Array of input image features of shape (N, D). - max_length: Maximum length T of generated captions. Returns: - captions: Array of shape (N, max_length) giving sampled captions, where each element is an integer in the range [0, V). The first element of captions should be the first sampled word, not the &lt;START&gt; token. """ N = features.shape[0] captions = self._null * np.ones((N, max_length), dtype=np.int32) # Unpack parameters W_proj, b_proj = self.params['W_proj'], self.params['b_proj'] W_embed = self.params['W_embed'] Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b'] W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab'] ########################################################################### # TODO: Implement test-time sampling for the model. You will need to # # initialize the hidden state of the RNN by applying the learned affine # # transform to the input image features. The first word that you feed to # # the RNN should be the &lt;START&gt; token; its value is stored in the # # variable self._start. At each timestep you will need to do to: # # (1) Embed the previous word using the learned word embeddings # # (2) Make an RNN step using the previous hidden state and the embedded # # current word to get the next hidden state. # # (3) Apply the learned affine transformation to the next hidden state to # # get scores for all words in the vocabulary # # (4) Select the word with the highest score as the next word, writing it # # (the word index) to the appropriate slot in the captions variable # # # # For simplicity, you do not need to stop generating after an &lt;END&gt; token # # is sampled, but you can if you want to. # # # # HINT: You will not be able to use the rnn_forward or lstm_forward # # functions; you'll need to call rnn_step_forward or lstm_step_forward in # # a loop. # # # # NOTE: we are still working over minibatches in this function. Also if # # you are using an LSTM, initialize the first cell state to zeros. # ########################################################################### # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** hidden_init, _ = affine_forward( features, W_proj, b_proj) start_word, _ = word_embedding_forward(self._start, W_embed) current_word = start_word next_state = hidden_init for step in range(max_length): prev_state = next_state if self.cell_type == 'rnn': next_state, _ = rnn_step_forward( current_word, prev_state, Wx, Wh, b) step_scores, _ = affine_forward( next_state, W_vocab, b_vocab) captions[:, step] = np.argmax(step_scores, axis=1) current_word, _ = word_embedding_forward( captions[:, step], W_embed) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ############################################################################ # END OF YOUR CODE # ############################################################################ return captions]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习OpenCV第19章，投影和3D视觉]]></title>
    <url>%2F2019%2F05%2F08%2FOpenCV19%2F</url>
    <content type="text"><![CDATA[Chapter 19首先会讨论从3D得到2D信息，然后再讨论从2D推断3D信息如果没有多张图片是很难得到靠谱的3D信息的： 通过stereo vision 通过motion Projections 得到了物体在三维里面的位置之后，因为我们在18章已经calibration了相机，所以可以得到这个点在图片里面的位置 提供了一个函数projectPoints来投影一系列的点，针对刚体 a list of loca‐ tions in the object’s own body-centered coordinate system 加上了平移和旋转，相机intrinsic和distortion 最后输出在画面上的店 Affine and Perspective Transformationsaffine是针对一个list或者一整张图片进行的，可以把一个point从图片的一个location移动到另一个location，perspective trans更多的是针对一个矩形图片 -&gt; related to prspective transformation 总结不同的函数 Bird’s-eye-view trans（p699） 在robtic巡航的时间，经常把排到的画面变成从上往下看的bird-view 需要相机的intrinsic和distortion （把棋盘放在地上进行calibration） 步骤 首先读取相机的参数和distortion model 找到地面上已知的店（比如chessboard），找到至少四个点 cv::getPerspectiveTransform()计算地面上已知点的homography H cv::warpPerspective()形成bird-eye-view three-dim pose estimation物体的三维pose可以从 一个相机：必须先考虑情况惹 多个相机捕捉：从多个不同图片来推断，这样即使是不知道的东西都可以操作 single camera 如果我们知道一个object，我们需要知道这个东西在他自己坐标系里面关键点的坐标 如果现在给了一个新的view point，可以根据关键点的位置来推断 cv::solvePnP() 用来计算一个know object的位置 从图片里面提取特征点，然后计算不同点的位置，这个问题的解是应该是唯一的 PNP问题不是每次都有唯一的解 如果没有足够的关键点，为了保险起见应该有足够的店 或者当物体离得特别远（这时候光线接近于平行了，就不好判断了） 总的来说，单目视觉和人自己的眼睛（单只）看东西的感觉差不多，不能获得精确的大小，还会产生一些错觉（比如把大楼的窗户设计的小来显得楼更高） Stereo Imaging在电脑中，通过计算在两张图里面都出现的点的位置来计算，这样就可以计算这个点的三维位置。虽然这样计算的计算量很大，但是可以通过一些方法来压缩搜寻的范围，从而得到相应的结果。主要分为4步： 在数学上remove掉相机lens的辐射和平移distortion -&gt; undistortion 调整相机之间的角度和距离 -&gt; rectification。这一步输出之后的两张图片应该是row-aligned的（frontal parallel） 找到左右两张图相同的feature -&gt; correspondence。这一步的输出是一个disparity map，输出的是两个图中相同特征点的x坐标方向上面的disparity 最后可以把disparity转换成triangulation，这一步叫做reprojection，这样输出的就是depth map了 triangulation（找到disparity和depth的关系） 整体概念如上图所示，在这张图里我们假设系统已经完全undistort，aligned（两张图片的行和行对上了）了，两个相机的平面完全相同，焦距也相同，并且两个相机的cx已经被calibrated好了（相同） 这时，这个物体点P的depth和disparity是成正比的，求出来的disparity是：xl - xr（xl和xr都是根据各自的相机中心的坐标）: T - (xl - xr)/Z - f = T/Z 这个关系虽然是正比但是不是线性的 当disparity接近0的时候，小的disparity的差异会引发非常大的depth的差异 当disparity非常大的时候，disparity的改变不会对depth引起太多的影响 最终，stereo的系统只在比较接近相机的部分有比较高的depth resolution（如下图所示） 上面的例子是二维转一维的，实际在OpenCV的系统里面是三维转二维的 在实际的应用里面相机不是那么理想的共线的，要尽量确保共线，才不会引起太多的distortion。最终的目的是通过math的计算让他共线，而不是在物理上共线 除此之外，还需要保证相机的拍摄是同步的，避免在拍摄的时候会有东西移动 Epipolar Geometry（简化双目模型）stereo image system的模型： 组合了两个pinhole model 加入了新的points epipoles 主要点： 对于每个相机都会有一个投影的中心O，并且有两个和这个相关的投影平面 在现实中的物体P会在两个投影平面上分别有投影pl和pr el或者er，定义是另一个相机的中心在这个投影平面上的投影，el和pl可以形成一条epipolar line 得到要义 每个三维的点，都会在每个相机上面得到一个epipolar的，这个点和pr/pl的交点就是epipolar line 一个图片里面的feature，在另一个图片里面必须在相对应的epipolar line上面（epipolar constraint） 上面那个定义意味着：可以把在图片上寻找特征从二维（图片）降低到一维（线） 并且图片的order会保存，比如一条线在两张图里面都是水平的 The Essential and Fundamental Matrices E Mat：包括了两个相机的translation和rotation F Mat：包括了E的信息，以及相机的intrinsic（在pixel的层面上关联两个相机） 二者的区别 E只知道两个相机的关系，不知道任何关于图片的信息，只在物体的层面上关联了两个相机 F关联了两个照片在各自图片坐标系里面的关系 E math + F mat（p713，还没有怎么看） 在左边的相机里，观察到的点是pl，在右边的相机观察到的点是pr pr = R（pl - T） cv::findFundamentalMatComputing Epipolar Lines(计算上面模型里面的那条线) 有了F Mat之后希望可以计算上面的epipolar line。每一个图片里面的line都会在另一张图片里有一个对应的line line用一个三个点的vector来表示 cv::computeCorrespondEpilines Stereo Calibration上面已经说了很多的理论知识了所以我们现在就开始calibration吧！ Stereo calibration是在空间上面计算两个相机的位置。相反，后面要说的rectification才是来保证两张图片行是共线的 Stereo calibration主要依靠的是找两个相机之间的T和R矩阵，这两个都可以用cv::stereoCalibrate()来计算 和单目相机的calibration有些相似，但是单目的相机要寻找一系列相机和chessboard之间的R和T 双目的calibration在寻找唯一一个能让左右相机匹配上的R和T 可以得到三个等式求解 因为图片的noise或者rounding error，每组得到的结果可能会有轻微的不同，最后会取中位数 calibration会把右边的相机放在和左边的相机相同的plane上面，这样这两个相机得到的图片就是parallel的，但是这时候还不是row-aligned的！！！ 可以直接通过用这一个函数计算相机的intrinsic，extrinsic和Stereo的参数，不用先进行calibration Stereo Rectification 如果两个图片aligned了，那么根据上面计算出来的disparity就可以很轻易的得到depth map了。但是在实际中只有相机没有这么容易做到 目标：我们需要reproject两个image plane，让他们在完全相同的plane里面，可以得到完美的aligned 我们希望在rectification之后图片的row aliged，这样stereo correspondence（在两个图片里找相同的点）就会变得更可信而且容易计算 在另一张照片里只找match一个点的row 这样的结果会有无限个待选 我们再人为的加上限制 结果会有八个term，四个给左边的相机，四个给右边的相机（两种计算这些参数的算法） 每个相机都会有distCoffs和旋转矩阵R，修正和未修正的相机矩阵（4个） 用上面这些东西，得到map来确定原图要怎么修改cv::initUndistortRectifyMap() Hartley’s algorithm + Bouguet’s algorithm（p730）Rectification mapStereo Correspondence 在两个图片里面match三维的点，只能在两张图片交叠的地方找到 两种不同的算法 block matching：快，效率高，基于“sum of absolute difference” (SAD） 只会找到高度符合的点（highly textured）-&gt; 户外 semi-global block matching (SGBM) ：精确度更高 matching is done at subpixel level using the Birchfield-Tomasi metric enforce a global smoothness constraint on the computed depth information that it approximates by considering many one-dimensional smoothness constraints through the region of interest Block matching三个步骤 prefiltering，normal图片的亮度，增强纹理 用SAD的窗口，搜索水平的epipolar line 在rectificatin之后，每行都是一个epipolar line，所以左边的图片肯定在右边的同一行里面有一个对应的部分 disparity会在一定的pixel范围里进行搜索，不同范围里的disparity代表的是不同的depth。但是超过了最大值的话就找不到depth了 -&gt; Each disparity limit defines a plane at a fixed depth from the cameras Postfiltering，减少比较差的结果 Semi-global block matchingcode example （p752）Structure from Motion 从移动中得到构造信息。但是在静止的情况下，一个相机移动得到的信息和两个相机得到的信息没有本质的区别 但是如果特别大的时候，就需要通过计算frame之间的关系得到最后的结果（SLAM？） 在附录 FitLine（直线拟合） 在三维的分析之中比较常用，所以在这里介绍]]></content>
      <categories>
        <category>图像处理</category>
        <category>OpenCV</category>
        <category>Projection</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Projection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode笔记]]></title>
    <url>%2F2019%2F05%2F07%2FLeetcode%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[进度： array部分差不多 string部分提高往后没有继续 math部分浅尝辄止 开始搞树的部分 1 twoSumGiven an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: Given nums = [2, 7, 11, 15], target = 9, Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. 1234567class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: for i, item in enumerate(nums): if target - item in nums and nums.index(target - item) != i: return [i, nums.index(target - item)] 总结： 刚开始用了直接for所有的元素的方法，忘记考虑当两个数字重复的时候需要怎么办，考虑了之后在非常大的数的情况下爆炸了 标准答案说到了hash表，但是其实在python实现里面本身就是个hash（不然怎么从索引得到结果），不需要考虑这个问题 然后考虑了把所有东西都放一个dict里面（毕竟hash？），但是遇到的问题是从value直接得到key会生一些问题。如果把数字作为key，索引作为value会发现数字有重复的，会覆盖key的值 这时候突然发现，如果用数字作为索引的话其实dict和list没有本质区别，在list里面操作就行了，而且list的.index()可以直接返回这个值得坐标（找到的是第一个值！！） 所以直接用enumerate把所有的index和item都列出来就可以解决了，神奇。 27 remove elementGiven an array nums and a value val, remove all instances of that value in-place and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. The order of elements can be changed. It doesn’t matter what you leave beyond the new length. Example 1: Given nums = [3, 2, 2, 3], val = 3, Your function should return length = 2, with the first two elements of nums being 2. It doesn’t matter what you leave beyond the returned length.Example 2: Given nums = [0, 1, 2, 2, 3, 0, 4, 2], val = 2, Your function should return length = 5, with the first five elements of nums containing 0, 1, 3, 0, and 4. Note that the order of those five elements can be arbitrary. It doesn’t matter what values are set beyond the returned length. 12345678910111213class Solution: def removeElement(self, nums: List[int], val: int) -&gt; int: remove_nums = 0 ori_length = len(nums) for i in range(len(nums)): if nums[i] == val: remove_nums += 1 nums[i] = float('inf') nums.sort() nums = nums[:ori_length - remove_nums] return len(nums) 总结： 这道题的重点是需要in - place的处理，空间复杂度要求很高（然而我的空间结果很垃圾）。一个重点就是返回的list不需要按照原来的顺序排列 从不需要原来的顺序得到的思路是：我把需要删除的东西的位置改成了inf，然后对所有部分进行排序，得到排序之后的结果再进行切片（这里刚开始的思路是删掉这个地方的东西然后再insert，后来发现直接替换就好了） 其实也可以直接用交换位置的方法，不用切片，因为题目只需要前面的这些元素符合要求就可以了，没有说后面的怎么样。 看了一些discussion都是memory只比5 % 的人少。。。但是差距都不大应该没问题！ 看到了一个超级牛逼简要写法：1234while val in nums: nums.remove(val)return len(nums) 80Given a sorted array nums, remove the duplicates in-place such that duplicates appeared at most twice and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Example 1: Given nums = [1, 1, 1, 2, 2, 3], Your function should return length = 5, with the first five elements of nums being 1, 1, 2, 2 and 3 respectively. It doesn’t matter what you leave beyond the returned length. 1234567891011121314151617181920212223242526class Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: i = 0 while(True): if i &gt;= len(nums) - 1: if len(nums) &gt; 2 and nums[i] == nums[i - 2]: nums = nums[:nums.index(nums[i]) + 2] break else: break if nums[i] == nums[i + 1]: i += 1 else: next_num = nums[i + 1] start_index = nums.index(nums[i]) next_index = nums.index(next_num) if next_index - start_index &gt; 2: for l in range(start_index + 2, next_index): nums[l] = float('inf') i = next_index while float('inf') in nums: nums.remove(float('inf')) return len(nums) 总结： 我深信我的方法虽然蠢但是没有问题，但是跑出来就是有问题，分明我return之前的数据还都是对的，但是return之后显示的东西就都有问题了 主要思路是这样的 因为in - place操作，所以就不能直接用remove去掉元素导致下标错乱 本来是想和上面的思路一样，换成inf，然后再把有inf的部分删除掉（参考了 # 27的简易解法） 怎么换成inf呢，我判断的方法是找到下一个值得index，然后计算这个index和上一个之间差多少个数，然后把富裕的数字都替换成inf 忽略的问题： 数数数错了很多问题 最开始没有考虑到什么停止 然后没有考虑到如果最后一个数字重复了两遍以上要怎么办的问题（这也是我用next_index的一个弊端） 然后看着大佬的代码哭出了声！！！12345678910class Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: i = 0 for n in nums: if i &lt; 2 or n != nums[i - 2]: nums[i] = n i += 1 return i 总结： 哇这个思路真的牛逼！ 中心思想就是让n来增加但是i不增加，这里已经说了不在意前面项之后list里面的内容，也就是说前n项之后的东西都不用管了。既然如此的话与其用inf来替换这个位置的数字，不如直接用后面的项填在相对应的位置上，只有填成功了才会增加i 这里需要先判断i的值是否小于2，然后再计算nums[i - 2]，否则会out of range i跑的速度没有超过n跑的速度所以没有关系 合理利用题里面的条件限制真的很重要！！ 189 Rotate arrayGiven an array, rotate the array to the right by k steps, where k is non - negative. Example 1: Input: [1, 2, 3, 4, 5, 6, 7] and k = 3Output: [5, 6, 7, 1, 2, 3, 4]Explanation:rotate 1 steps to the right: [7, 1, 2, 3, 4, 5, 6]rotate 2 steps to the right: [6, 7, 1, 2, 3, 4, 5]rotate 3 steps to the right: [5, 6, 7, 1, 2, 3, 4] Example 2: Input: [-1, -100, 3, 99] and k = 2Output: [3, 99, -1, -100]Explanation:rotate 1 steps to the right: [99, -1, -100, 3]rotate 2 steps to the right: [3, 99, -1, -100] Note: Try to come up as many solutions as you can, there are at least 3 different ways to solve this problem.Could you do it in-place with O(1) extra space? 思路 需不需要注意k = 0的时候 如果k的个数特别大需不需要简化一下 123456789101112class Solution: def rotate(self, nums: List[int], k: int) -&gt; None: """ Do not return anything, modify nums in-place instead. """ steps = k % len(nums) unchange_nums = nums[:len(nums) - steps] change_nums = nums[len(nums) - steps:] nums[:steps] = change_nums nums[steps:] = unchange_nums 总结： 居然第一种就这么写出来了，实际上就是把后面的数字移动到前面去 注意nums不能直接用change_nums + unchange_nums，大概是他认为这个不是in - place了吧 另一种方法：in-place123456789101112 k = k % len(nums) self.reverse_nums(nums, 0, len(nums) - 1) self.reverse_nums(nums, 0, k - 1) self.reverse_nums(nums, k, len(nums) - 1)def reverse_nums(self,nums,start,end): while start &lt; end: temp = nums[start] nums[start] = nums[end] nums[end] = temp start += 1 end -= 1 实际上，rotate的另外一种方法是先把整个list反向，然后把前面的k个反向，然后再把后面的(n-k)个反向（这里我是没想到的） 把一个数组反向的算法就是从两头向中间逼近着交换（我该好好去看看基础的算法了。。） 最后，还有一种方法是跳着设置值，也就是说k个之后的值就应该是现在这个位置的值 41 First Missing PositiveGiven an unsorted integer array, find the smallest missing positive integer. Example 1: Input: [1,2,0]Output: 3Example 2: Input: [3,4,-1,1]Output: 2Example 3: Input: [7,8,9,11,12]Output: 1Note: Your algorithm should run in O(n) time and uses constant extra space. 第一个思路：时间nlog(n)12345678class Solution: def firstMissingPositive(self, nums: List[int]) -&gt; int: nums.sort() target = 1 for n in nums: if n == target: target += 1 return target 这个思路整体建立在先排序的基础上，但是排序的时间复杂度本身就已经是nlog(n)了 排序 - 找到比0大的数字从这里开始 - 这个数字不符合的话找下一个 但是我在找比0大的数字的时候还想着把list切片，切片就又需要考虑0啊，1啊，缺多少个数字的问题，空的list。其实根本不用这么麻烦 本质上这个方法就是，找到miss的正数，那就从正数的第一个（1）开始找，如果找到了这个数就继续找下一个（target++），总是能找到的嘛，找到的就是缺的数字了 自己的方法12345678910111213141516class Solution: def firstMissingPositive(self, nums: List[int]) -&gt; int: if nums is None or len(nums) == 0: return 1 for i in range(len(nums)): target_num = i + 1 if nums[i] == target_num: if i == len(nums) - 1: return target_num + 1 else: continue if target_num in nums: temp = nums.index(target_num) nums[temp],nums[i] = nums[i], nums[temp] else: return target_num 桶排序：要把对应的数字放在对应的位置上 这道题里应该的样子就是nums[index] = index + 1 大佬的思路 -&gt; 首先判断边界条件！！(学到了学到了) 看过了上面的提示写出来的第二版 判断边界条件 判断这个数字是不是摆在了正确的位置 正确，判断是否是最后一个数字 是，输出的是最后一个数字+1 不是，这个位置的正确了，判断下一个位置 没有，判断nums里面还有没有应该摆在这个位置的数字 有，那就和这个位置交换 没有，那没有的数字就是缺少的数字了 因为每次都是把数字换到了正确的位置了，所以交换最多进行len(nums)次，时间复杂度是O(n) 123456789def firstMissingPositive(self, nums): for i in xrange(len(nums)): while 0 &lt;= nums[i]-1 &lt; len(nums) and nums[nums[i]-1] != nums[i]: tmp = nums[i]-1 nums[i], nums[tmp] = nums[tmp], nums[i] for i in xrange(len(nums)): if nums[i] != i+1: return i+1 return len(nums)+1 大佬的另一个方法，其实思路和上面的差不多，就是把数字换到正确的位置上，但是判断的条件和我的有一点不同，可能因为我的是基于python的功能 其中，换到正确位置的数字就是在1到len(nums)之间的数字。nums[i]-1是这个数字应该的坐标位置，如果应该的位置和现在的位置的数字不一样，那就交换这两个数字 注意这里需要用while换，要一直换到正确的位置才可以 这样的结果就是大家都按正确的填好了，最后不对的那个位置的index+1就是需要的结果 299You are playing the following Bulls and Cows game with your friend: You write down a number and ask your friend to guess what the number is. Each time your friend makes a guess, you provide a hint that indicates how many digits in said guess match your secret number exactly in both digit and position (called “bulls”) and how many digits match the secret number but locate in the wrong position (called “cows”). Your friend will use successive guesses and hints to eventually derive the secret number. Write a function to return a hint according to the secret number and friend’s guess, use A to indicate the bulls and B to indicate the cows. Please note that both secret number and friend’s guess may contain duplicate digits. Example 1: Input: secret = “1807”, guess = “7810” Output: “1A3B” Explanation: 1 bull and 3 cows. The bull is 8, the cows are 0, 1 and 7.Example 2: Input: secret = “1123”, guess = “0111” Output: “1A1B” Explanation: The 1st 1 in friend’s guess is a bull, the 2nd or 3rd 1 is a cow.Note: You may assume that the secret number and your friend’s guess only contain digits, and their lengths are always equal. 1234567class Solution: def getHint(self, secret: str, guess: str) -&gt; str: bull = sum(a == b for a,b in zip(secret,guess)) cow = 0 for x in set(guess): cow += min(secret.count(x),guess.count(x)) return str(bull) + "A" + str(cow-bull) + "B" 这里自己想了一些比较蠢的想法之后直接参考别人的了 其一是比对他们两个位置和数字都相同的东西，想要转换成dict来比较，但是后来发现string就可以直接index了不用这么麻烦 想过能不能按位做减法，未果 其二是在得到了bull之后把bull的部分从原来的里面剔除出去然后再比较相似的数字 遇到了主要问题是重复的数字怎么办以及如何剔除出去bull 主要思路是这样的： 其实cow的数量就是bull-cow都是的数量减去bull的数量，也就相当于维恩图里面，只有A的量是A的量 - 同时AB的量。这里是bull就相当于AB都有，两个里面所有重复的数量就相当于A的量 这样可以做减法就解决了上面的从bull得到cow的问题！！ 所以说看问题还是要看本质 面对重复的数字，居然可以直接把string转换成set 这里复习一下set好吗！！！这个集合居然可以没有重复的元素，平常我忽视你了呀小可爱，转化成set就不会重复了哦，震惊！！ 这样问题就变成了： 求bull：用zip把两个东西一一对应的打包起来（居然还有你小可爱！）直接对比 求both：guess里面猜的次数就是总体的次数，secret里面的次数是真实的次数，对于每个在guess里面（set）的元素都看看分别在两个里面是多少个，然后小的那个就是both的大小 这里介绍.count()小可爱，居然还可以数数！ 最后both-bull就是结果了 134 gas station居然自己搞出来了一个看起来很蠢的1234567891011121314151617181920212223242526class Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: if sum(gas) &lt; sum(cost): return -1 tank = 0 current = 0 counter = 0 while(True): tank = tank + gas[current] - cost[current] if tank &lt; 0: if current &lt; len(gas): current = current + 1 counter = 0 tank = 0 continue else: return -1 current += 1 current = current % len(gas) counter += 1 # print(current,counter) if counter == len(gas): return current % len(gas) ~时间超过了百分之48的人，感觉可能还可以吧~时间都是骗人的又跑了一次居然超过了百分之86的！！ 重点 一直按着顺序跑，不会跳着走 如果gas的总量从一开始就小于cost的总量，那绝对不可能 我的思路： 从第一个点开始试着跑，一直到试着从最后一个点开始跑，找到了就直接返回 增加一个计数的var，记一共跑了多远，因为是按着顺序跑的所以这个var等于gas的长度的时候就是跑完了 避免out of range问题，需要求余数 遇到问题： 当tank小于0，更新完条件之后记得continue继续循环呀 刚开始想用的判断条件是for或者while里面带条件，还想了一下要不要zip这两个数据，但是都是list实在是没有必要。但是感觉是想的实在是太多了 1234567891011class Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: if sum(gas) &lt; sum(cost): return -1 rest = start = 0 for i in range(len(gas)): rest += gas[i] - cost[i] if rest &lt; 0: start = i + 1 rest = 0 return start 居然有这么简要的写法！！ 所以只要不是sum(gas) &lt; sum(cost)就一定会有解诶，神奇。也就是说我上面有一个返回的-1是没有意义的 而且用for的话就不用再考虑counter的问题了 从哪里失败就从哪里的下一个爬起来 118 Pascal’s TriangleExample: Input: 5Output:[ [1], [1,1], [1,2,1], [1,3,3,1], [1,4,6,4,1]] 1234567891011121314151617181920212223class Solution: def generate(self, numRows: int) -&gt; List[List[int]]: result = [] if numRows == 0: return [] for row in range(numRows): now_row = [] if row == 0: now_row = [1] elif row == 1: now_row = [1,1] else: now_row = [1] for member in range(1,row): now_row.append(result[row-1][member-1] + result[row-1][member]) now_row.append(1) result.append(now_row) return result 总算是自己写出来一个东西了 好简单，除了前两行是特定的，其他的可以归为一类 求一个简单的数学关系就行了，数数别数错了！！注意数0 唯一没有注意的点就是：事先不知道list的大小，所以初始化成空的之后需要用append添加元素 119 杨辉三角形2Given a non-negative index k where k ≤ 33, return the kth index row of the Pascal’s triangle. Note that the row index starts from 0. Input: 3Output: [1,3,3,1] 1234567class Solution: def getRow(self, rowIndex: int) -&gt; List[int]: L = [1] while True: if len(L) == rowIndex + 1: return L L = [u+v for u,v in zip([0]+L,L+[0])] 没想到杨辉三角形的代码也有简要的解法，这个是用L记录了上一行的信息，然后再把这行扩充两个0，相当于这个三角形的本质是两行错位相加！！ 注意最后的L得到的是一个list，list要有list的样子 更加理解了一下zip和单行for的用法 index从0开始，结果开始没有注意到 while true 加上一个 if的效果等同于for的效果！！！越写越糊涂 169 Majority ElementGiven an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. You may assume that the array is non-empty and the majority element always exist in the array. Example 1: Input: [3,2,3]Output: 3Example 2: Input: [2,2,1,1,1,2,2]Output: 2 1234class Solution: def majorityElement(self, nums: List[int]) -&gt; int: nums.sort() return nums[len(nums)//2] 思路：这回想到了很多历遍的方法，但是感觉太蠢了，终于开始想怎么才能更好的实现了 在写写画画的时候突然考虑到，如果有超过一半的数量都是这个数的话，把这个list排序之后最中间的那个数肯定是这个数 极限情况就是两个元素差1，这时候是多一点的那个数的边界上 其他的情况下就是在出现最多的那个数的中间 本来想要用floor的，但是发现需要math包，所以用了 // 来求除之后的整数 229 Majority Element 2Given an integer array of size n, find all elements that appear more than ⌊ n/3 ⌋ times. Note: The algorithm should run in linear time and in O(1) space. Example 1: Input: [3,2,3]Output: [3]Example 2: Input: [1,1,1,3,3,2,2,2]Output: [1,2]12345678910111213141516171819202122class Solution: def majorityElement(self, nums: List[int]) -&gt; List[int]: if not nums: return [] major1,major2,count1,count2 = 0,1,0,0 for n in nums: if major1 == n: count1 += 1 elif major2 ==n: count2 += 1 elif count1 ==0: major1 = n count1 = 1 elif count2 == 0: major2 = n count2 =1 else: count1 -= 1 count2 -= 1 return [n for n in (major1,major2) if nums.count(n) &gt; len(nums) // 3] 注意这道题说的是出现次数大于1/3的数字，所以结果只有只能是没有，1个或者两个，不存在结果是三个的情况！ 这个想了半天不会做，查了一下用的是Boyer-Moore Majority Vote algorithm 这个算法的主要意思是如果两拨人打架，打架一对一抵消，然后看看剩下的部分哪个比较多 记录剩下的东西的方法就是增加了一个额外的部分，包括major和count两部分，major记录的是有剩余的数是什么，count记录还有多少个 如果count没有了，那么就从现在遇到的新的数开始记 如果现在的数不是需要的，那么count - 1，如果是现在需要的那么count + 1 最开始是用在一个数组里面找超过一半的数的，但是我上一道题用了其他方法所以没用到 注意因为是求1/3的数字，所以虽然有剩下的，但是剩下的不一定都是符合要求的，需要再数一下个数对不对（这才有了return这一行里面的东西） 人类的算法真是奇幻无穷 274 h-indexGiven an array of citations (each citation is a non-negative integer) of a researcher, write a function to compute the researcher’s h-index. According to the definition of h-index on Wikipedia: “A scientist has index h if h of his/her N papers have at least h citations each, and the other N − h papers have no more than h citations each.” Example: Input: citations = [3,0,6,1,5]Output: 3Explanation: [3,0,6,1,5] means the researcher has 5 papers in total and each of them had received 3, 0, 6, 1, 5 citations respectively. Since the researcher has 3 papers with at least 3 citations each and the remaining two with no more than 3 citations each, her h-index is 3.Note: If there are several possible values for h, the maximum one is taken as the h-index. 1234567891011class Solution: def hIndex(self, citations: List[int]) -&gt; int: result = 0 for h_cand in range(len(citations) + 1): h_more = 0 for citation in citations: if citation &gt;= h_cand: h_more += 1 if h_more &gt;= h_cand: result = max(result,h_cand) return result 思路，非常直观的方法，直接iterate所有的元素，如果找到了更大的result的值就取最大的（根据题目要求） 注意的点在需要 h_more &gt;= h_cand而不是等于，因为给出的定义的意思是index-h是有h个的值大于等于h，h_more的个数会比h_cand多（但是因为取了下面的max，所以等于其实也是可以得） 这个的速度真的好慢，尝试一下binary search 1234567891011121314151617class Solution: def hIndex(self, citations: List[int]) -&gt; int: bucket = [0 for n in range(len(citations)+1)] for nums in citations: if nums &gt;= len(citations): bucket[len(citations)] += 1 else: bucket[nums] += 1 result = 0 for nums in range(len(bucket)): nums = len(bucket) - nums -1 result += bucket[nums] if result &gt;= nums: return nums return 0 用了桶排序的神奇方法 还是取决于定义，如果一共有5个paper的话，可以选的h的值有6个，分别是0 1 2 3 4 5，把这留个值分成六个桶，每个里面放的就是比这桶的inde等于的paper的数量 如果总数直接大于最大的桶数，就放在最后一个里面 这是在第一个循环干的事情 第二个循环里，把这些桶里面的值取出来就是比这个桶的index大于等于的paper的数量，从后往前数，如果这个paper的数量大于了现在的index，那就说明现在的index就是h！ 这里学到了一个创建固定长度列表的方法bucket = [0 for n in range(len(citations)+1)] 12345678class Solution: def hIndex(self, citations: List[int]) -&gt; int: citations.sort(reverse = True) result = 0 for i,n in enumerate(citations): if n &gt;= i+1: result = max(result,i+1) return result 再另一种思路，用了排序 如果把这个list按降序排序的话，index的数量加一就是目前数过的paper的数量，citation[index]就是这个数量上面对应的citation的数量，这两个值应该正好相等，或者citation更大一点，需要在排好序的内容里面找到这一项！ 这样速度比桶排序稍微慢一点但是还是蛮快的，起码比第一种要快很多了 275 h-index 21234567891011class Solution: def hIndex(self, citations: List[int]) -&gt; int: n = len(citations) l, r = 0, n-1 while l &lt;= r: mid = (l+r)//2 if citations[mid] &gt;= n-mid: r = mid - 1 else: l = mid + 1 return n-l 可以依然沿用上面的方法，但是可能是因为数据量上去的原因，所以速度变慢了 这里可以加入二分法搜索取代上面的直接iterate while的条件是因为移动一位，所以会出现l&gt;r的情况，在这种情况下就可以停下来了 二分法就是这么写的！ 217 contains duplicate123456class Solution: def containsDuplicate(self, nums: List[int]) -&gt; bool: for n in nums: if nums.count(n) &gt;= 2: return True return False 1234567class Solution: def containsDuplicate(self, nums: List[int]) -&gt; bool: setNums = set(nums) if len(setNums) == len(nums): return False else: return True 消耗时间太长了！！ 说明这个count的时间还是不可以 想到了用set但是没相当怎么用set set可以把有重复内容的变成没有重复内容的！！ 所以set和list的长度是不一样的 219 contains duplicate2Given an array of integers and an integer k, find out whether there are two distinct indices i and j in the array such that nums[i] = nums[j] and the absolute difference between i and j is at most k. Example 1: Input: nums = [1,2,3,1], k = 3Output: trueExample 2: Input: nums = [1,0,1,1], k = 1Output: trueExample 3: Input: nums = [1,2,3,1,2,3], k = 2Output: false 1234567891011class Solution: def containsNearbyDuplicate(self, nums: List[int], k: int) -&gt; bool: if len(set(nums)) &gt;= len(nums): return False extra = &#123;&#125; for i,n in enumerate(nums): if n in extra and i-extra[n] &lt;= k: return True extra[n] = i return False 注意这里需要找到的差的绝对值是最大是k，所以找到一个比k小的很容易！！只要找到就能返回 判断边界条件 把元素作为key放进extra里面，val是这个元素的index，因为key是唯一的所以可以一直找到离得最近的index，这样就越来越能确保满足条件，一旦满足条件就返回，如果所有的都不满足就false 220Given an array of integers, find out whether there are two distinct indices i and j in the array such that the absolute difference between nums[i] and nums[j] is at most t and the absolute difference between i and j is at most k. Example 1: Input: nums = [1,2,3,1], k = 3, t = 0Output: trueExample 2: Input: nums = [1,0,1,1], k = 1, t = 2Output: trueExample 3: Input: nums = [1,5,9,1,5,9], k = 2, t = 3Output: false 1234567891011121314151617class Solution: def containsNearbyAlmostDuplicate(self, nums: List[int], k: int, t: int) -&gt; bool: if t &lt; 0: return False buckets = &#123;&#125; for i in range(len(nums)): bucket = nums[i] // (t+1) if bucket in buckets: return True elif bucket - 1 in buckets and nums[i] - buckets[bucket-1] &lt;= t: return True elif bucket + 1 in buckets and buckets[bucket+1] - nums[i] &lt;=t: return True buckets[bucket] = nums[i] if i &gt;= k: del bucket[nums[i-k] // (t+1)] return False 运用的是桶排序的思路，每个nums[i]会放在一个桶里，这个桶的宽度是这两个数字的差 如果想要这两个数值的差值小于等于t，那么需要这两个数字在一个桶里或者在相邻的桶里（因为后面增加了k的判断条件，所以不用考虑k） 思路 首先考虑了一下k，如果i大于k的时候，就可以直接扔掉i-k之前的数据了，只考虑中间的k+1个数据，这样的话空间复杂度很低。这里的扔掉指的是把bucket里面的值直接扔掉，这样就避免了找到在相同的桶里面却i和j的差值超过k的问题 首先iterate整个nums，把不同的数字放在不同的桶里，注意桶的个数是t+1 然后如果在放之前这个桶有东西，或者相邻的桶的值和现在的值的差是小于等于t的，那么就存在，返回true 如果都不存在的话，把现在的数字放到对应的桶里面 另外一个思路考虑的是二叉树的数据结构，用这个结构可以很快的搜索到离这个数最近的数据并且判断这个数据和这个数的差是不是小于t！ 55 Jump game123456789101112131415class Solution: def canJump(self, nums: List[int]) -&gt; bool: if nums is []: return False if len(nums) == 1: return True current = len(nums) - 1 while current &gt;= 0: flag = False for i in range(0,current): if current - i &lt;= nums[i] and current &gt;= i: flag = True current = i if current == 0: return True if flag == False: return False 虽然超时了但是写的还不错的iterate =。=算了这就是一坨屎！！！ 123456789class Solution: def canJump(self, nums: List[int]) -&gt; bool: current = len(nums) - 1 for i in range(len(nums))[::-1]: if current - i &lt;= nums[i]: current = i if current == 0: return True return False 我的方法其实思路是没有问题的，主要在于太啰嗦了而且循环太多了，其实直接从后往前找就行了！！！从后往前找不用考虑怎么让他循环起来呀，直接一个一个往前推就可以了 前面那个的问题在于多叠了一个while，于是时间瞬间爆炸，写前面的那个的时候也在想着如何找回循环里面去，结果还是用了个蠢办法 1234567class Solution: def canJump(self, nums: List[int]) -&gt; bool: j = 0 for i,n in enumerate(nums): if j &lt; i: return False j = max(i+n,j) return True i+n就是从这步开始可以移动的最大距离，j是上一步可以移动的最大距离，这两个哪个大就走哪个 如果这个距离还赶不上i，那就说明走不到最后了，告辞 45 Jump game 2123456789101112class Solution: def jump(self, nums: List[int]) -&gt; int: if len(nums) &lt;= 1: return 0 start, end = 0, 0 step,maxend = 0,0 while True: step += 1 for i in range(start, end+1): if i+nums[i] &gt;= len(nums) -1: return step maxend = max(maxend, i + nums[i]) start = end + 1 end = maxend 实际上来说用的是BFS的思想，但是不是每次都把东西从queue里面拿出来，而是确定了每次寻找的开始的阀内 start和end分别代表现在可以开始寻找的开始和结束，如果在这个范围里面找到了符合要求的结果，那么直接返回这个步数，如果没找到的话就从下一个范围开始找，下一个范围是上一个范围的end+1 到目前能到的最大的范围 注意符合的要求是大于等于n-1而不是正好走到这个点 求maxend和之前的一样 121 Best Time to Buy and Sell StockSay you have an array for which the ith element is the price of a given stock on day i. If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit. Note that you cannot sell a stock before you buy one. Example 1: Input: [7,1,5,3,6,4]Output: 5Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5. Not 7-1 = 6, as selling price needs to be larger than buying price.Example 2: Input: [7,6,4,3,1]Output: 0Explanation: In this case, no transaction is done, i.e. max profit = 0. 12345678910class Solution: def maxProfit(self, prices: List[int]) -&gt; int: minBuy = float('inf') maxProfit = 0 for i in prices: if i &lt; minBuy: minBuy = i elif i - minBuy &gt; maxProfit: maxProfit = i - minBuy return maxProfit 用brute的算法会time limit，这里用的方法是用两个变量分别记录最低的价钱和最高的利润，这样的话只需要对数组遍历一次就能得到最终的结果 因为判断这个价钱低了的话，求这个东西的最大利润也就只能用这个最低价钱之后的东西求了，所以不会冲突 122 现在可以进行多次交易了，但是每次之间不能重叠 其实只要后一次比前一次贵，这个profit就可以一直累计，分为一直上涨或者中间掉下来一下再重新买的感觉 1234567class Solution: def maxProfit(self, prices: List[int]) -&gt; int: maxProfit = 0 for i in range(1,len(prices)): if prices[i] &gt; prices[i-1]: maxProfit += prices[i] - prices[i-1] return maxProfit 123 现在最多进行两次交易,找到最大的利润 1234567891011121314class Solution: def maxProfit(self, prices: List[int]) -&gt; int: cost_1 = float('inf') profit_1 = 0 cost_2 = float('inf') profit_2 = 0 for price in prices: cost_1 = min(cost_1,price) profit_1 = max(profit_1, price - cost_1) cost_2 = min(cost_2,price - profit_1) profit_2 = max(profit_2, price - cost_2) return profit_2 其中，下标带1的是第一次交易之后的结果，下标带2的是第二次交易之后的结果 从总体上来看，第二次买入之后花掉的钱实际上是第二次买入的实际花费 - 第一次交易之后挣的钱（可以是负数）。而第二次卖出之后的总的收益为 第二次卖出的钱 - 第二次买入之后的实际花费 所以，如果需要利润最大，需要第二次买入的实际花费最小，需要第一次的利润最大，需要第一次买入的花费最小，最终形成了这个代码 188 现在需要进行最多k次交易，把profit弄到最大 这部分好像大家都用到了DP 1234567891011121314class Solution: def maxProfit(self, k: int, prices: List[int]) -&gt; int: n = len(prices) if n &lt; 2: return 0 if k &gt;= n/2: return sum(i-j for i, j in zip(prices[1:],prices[: -1]) if i &gt; j) profits = [0] * n for _ in range(k): preprofit = 0 for i in range(1,n): profit = prices[i] - prices[i-1] preprofit = max(preprofit + profit, profits[i]) profits[i] = max(preprofit, profits[i-1]) return profits[-1] 首先考虑边界条件，如果k的数量已经比n/2大了，那么可以直接认为可以进行无限次交易了，就和上面的第二题一样 主要思路就是现在定义了两个变量，一个变量表示在前i天完成的交易，已经得到的最大利润。另一个变量定义了在第i天卖出的话，这时候得到的最大利润。这两个变量的都是在在第j次交易里。 用一个长n的list profits来记录这个天数之后获得的利益。在k次交易中一直更新这个profits里面的最大值。所以实际上关于k的变量不需要考虑 首先分析在第i天得到的利润，就是这一天的价格减去前一天的价格。更新之前i天里面的总利润，就是把最开始的preprfit再加上这一天获得的利润，和本来的preprofit来比大小，更新preprofit 更新实际上第i天的利润，对比实际上前一天的利润和前i天的利润哪个大 309 中间带冷却的买股票 每次卖出去之后必须要cooldown一轮 用了dp和state machine来表示，一共会有三种状态 s0(reset) -sell-&gt; s1 -cool-&gt; s2(reset) -buy-&gt; s0 用一个数组来记录在每天在这个状态里面的最大利润，然后再从最后一天的最大利润里面挑出来一个 注意考虑边界条件 学会了一个新的初始化list的方法 感觉自己终于理解了dp呢（并没有）12345678910111213141516class Solution: def maxProfit(self, prices: List[int]) -&gt; int: n = len(prices) if n &lt; 2: return 0 s0,s1,s2 = [0]*n,[0]*n,[0]*n s0[0] = -prices[0] s1[0] = float('-inf') s2[0] = 0 for i in range(1,n): price = prices[i] s0[i] = max(s0[i-1],s2[i-1] - price) s1[i] = s0[i-1] + price s2[i] = max(s1[i-1],s2[i-1]) return max(s0[n-1],s1[n-1],s2[n-1]) 11 装水Given n non-negative integers a1, a2, …, an , where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water. Note: You may not slant the container and n is at least 2. 12345678910111213class Solution: def maxArea(self, height: List[int]) -&gt; int: n = len(height) start, end = 0, n-1 maxArea = 0 while start &lt; end: if height[start] &gt;= height[end]: maxArea = max(maxArea,height[end] * (end-start)) end -= 1 else: maxArea = max(maxArea,height[start] * (end-start)) start += 1 return maxArea 这道题的重点在这个装水的大小是由比较短的那条边决定的。而且肯定是底边越长越牛逼，所以从底边最长的两边开始找，然后在两个高度里面取比较大的继续找下一个 需要用一个变量来储存 max area的大小（这个我想到了） 然后比较快的方法是从两遍开始逼近，这样的话只遍历了这个list一次，时间复杂度是n，好像有个排序算法和这个的想法也差不多 注意while的判断条件其实就是这个 42 装水Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining. 12345678910111213141516class Solution: def trap(self, height: List[int]) -&gt; int: n = len(height) if n &lt; 2: return 0 left_max,right_max = [0]*n,[0]*n left_max[0],right_max[-1] = height[0],height[-1] maxTrap = 0 for i in range(1,n): left_max[i] = max(left_max[i-1],height[i]) for i in reversed(range(0,n-1)): right_max[i] = max(right_max[i+1],height[i]) for i in range(n): maxTrap += min(left_max[i],right_max[i]) - height[i] return maxTrap 用dp解决的这个问题 核心思想在竖着（按列）数每个格子，这个格子可不可以装水和左右两边的最高点有关，这个格子能装多少水和1.最短的高点和2.这个格子本身的高度有关 所以可以用三个循环搞定这个问题，用空间换时间，在list里面记录下来每个列对应的左边的最高点和右边的最高点，然后再数每个列的容量，大小是（左右最高中间短的那个） - （这个列对应的高度） 334 升序的三个数字Given an unsorted array return whether an increasing subsequence of length 3 exists or not in the array. Formally the function should: Return true if there exists i, j, ksuch that arr[i] &lt; arr[j] &lt; arr[k] given 0 ≤ i &lt; j &lt; k ≤ n-1 else return false.Note: Your algorithm should run in O(n) time complexity and O(1) space complexity. 123456789101112131415class Solution: def increasingTriplet(self, nums: List[int]) -&gt; bool: if len(nums) &lt; 3: return False first = float('inf') second = float('inf') third = None for i,num in enumerate(nums): if num &lt;= first: first = num elif num &gt; first and num &lt;= second: second = num else: third = num return (third != None) 我最初的思路没有错，需要有变量来保存这三个升序的东西 其实核心的思路在于，如果现在这个数比第一个升序的数字小，那么这个数字完全就可以成为新的第一个数字，比如 3 2 4 5，那么345和245没有什么本质的区别，而一旦third有了取值，那么就说明肯定已经有了一个结果 128Given an unsorted array of integers, find the length of the longest consecutive elements sequence. Your algorithm should run in O(n) complexity. Example: Input: [100, 4, 200, 1, 3, 2]Output: 4Explanation: The longest consecutive elements sequence is [1, 2, 3, 4]. Therefore its length is 4. 123456789101112131415161718192021222324252627282930313233343536class Solution: def longestConsecutive(self, nums: List[int]) -&gt; int:# if nums == []: return 0# max_num = max(nums)# min_num = min(nums)# if max_num &gt; len(nums) or -max# if min_num &lt; 0:# max_num -= min_num# ass_list = [None] * (max_num + 1)# for i,num in enumerate(nums):# # 确保都是正数# if min_num &lt; 0:# num = num-min_num# ass_list[num] = 1 # max_length = 0# prev_length = 0# for i in range(len(ass_list)):# if ass_list[i] != None:# max_length += 1# else:# prev_length = max(max_length,prev_length)# max_length = 0# return max(max_length,prev_length) if nums == []: return 0 current_length,prev_length = 1,1 num_set = set(nums) for num in num_set: if num - 1 not in num_set: current_num = num while current_num+1 in num_set: current_num += 1 current_length += 1 prev_length = max(prev_length,current_length) current_length = 1 return max(current_length, prev_length) 这个问题一开始的思路是错的，已经comment掉了，但是感觉这个想法其实就是更具体化的hash表而已，第一个思路是把所有的数字平均的放在一个list里面，然后每个数字的本身就对应的是他的index，这样的话就可以直接知道有哪些数字是连续的了。但是这种方法在数字特别大的时候空间上就爆炸了，空间复杂度也是和数字大小有关 这时候又要拿出来快乐的hash表了，记住python自己自带hash表 每遇到一个数字，需要判断这个数字的下一个数字在不在这个nums里面，如果在的话更新数字和长度，如果不再的话刷新计数器并且开始下一个数字 但是直接这样算还是会时间爆炸（比如一堆连续的只有一个是跳开的），所以又加进去了一个新的判断条件，这个条件的精髓在于，如果这个数之前的数字在nums里面，那么这个数在算他前面那个数的时候就应该被算上了，所以这部分就可以跳过这个数了，只有当前一个数字不在的时候才需要数长度 164Given an unsorted array, find the maximum difference between the successive elements in its sorted form. Return 0 if the array contains less than 2 elements. Example 1: Input: [3,6,9,1]Output: 3Explanation: The sorted form of the array is [1,3,6,9], either (3,6) or (6,9) has the maximum difference 3.Example 2: Input: [10]Output: 0Explanation: The array contains less than 2 elements, therefore return 0.Note: You may assume all elements in the array are non-negative integers and fit in the 32-bit signed integer range.Try to solve it in linear time/space. 123456789101112class Solution: def maximumGap(self, nums: List[int]) -&gt; int: nums.sort() max_gap = 0 if len(nums) &lt; 2: return 0 for i in range(1,len(nums)): gap = nums[i] - nums[i-1] max_gap = max(max_gap, gap) return max_gap 直接用python自带的排序速度不一定很慢，虽然只超过了百分了20的人但是最后还是跑出来了 这个方法非常直接了 12345678910111213141516171819202122232425262728class Solution: def maximumGap(self, nums: List[int]) -&gt; int: n = len(nums) if n &lt; 2: return 0 max_num, min_num = max(nums), min(nums) if max_num == min_num: return 0 wide = max((max_num - min_num) // (n-1),1) num_b = (max_num - min_num) // wide + 1 maxGap = 0 # prev_bucket = float('-inf') max_b = [0]* num_b min_b = [float('inf')]* num_b for i, num in enumerate(nums): idx = (num-min_num) // wide max_b[idx] = max(max_b[idx],num) min_b[idx] = min(min_b[idx],num) prev_max = max_b[0] for i in range(1,num_b): if max_b[i] == 0: continue maxGap = max(maxGap,min_b[i] - prev_max) prev_max = max_b[i] return maxGap 这个桶排序终于写出来了，基本思路是上面的截图，需要注意的有几点 第一，python不导入math的话没办法求ceiling，但是可以用 -（-a // b）来求 第二，在求bucket的个数的时候，需要多加上一个bucket，因为一个bucket里面最后的数字是放在下一个bucket里面最前面的 第三，可能会有空的bucket，所以不能直接用这个的min减去上一个的max，必须要留一个变量保存上一个的max 第四，当所有数字都相同的时候会变得很麻烦，最后加上去一个条件过滤掉这个部分 28 implement strStr（）Implement strStr(). Return the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack. Example 1: Input: haystack = “hello”, needle = “ll”Output: 2Example 2: Input: haystack = “aaaaa”, needle = “bba”Output: -1 123456789class Solution: def strStr(self, haystack: str, needle: str) -&gt; int: if needle == "": return 0 for i, ch in enumerate(haystack): if ch == needle[0]: if needle == haystack[i:i+len(needle)]: return i return -1 刚开始特别快乐的暴力破解了，真是万万没想到 感觉python处理起来字符串是真的开心 但是这个的时间不是很快乐 关于字符串匹配有另外两个算法KMP和BM（BM更快一点） 14Write a function to find the longest common prefix string amongst an array of strings. If there is no common prefix, return an empty string “”. Example 1: Input: [“flower”,”flow”,”flight”]Output: “fl”Example 2: Input: [“dog”,”racecar”,”car”]Output: “”Explanation: There is no common prefix among the input strings.123456789101112131415161718192021class Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: if len(strs) == 0: return "" if len(strs) == 1: return strs[0] LCP = self.compare(strs[0],strs[1]) for i in range(2,len(strs)): LCP = self.compare(LCP,strs[i]) return LCP def compare(self,a,b): i = j = 0 counter = 0 while i &lt; len(a) and j &lt; len(b): if a[i] == b[i]: counter += 1 else: break i += 1 j += 1 if counter == 0: return "" else: return a[:counter] 思路：平行比较，先找出来前两个里面的prefix，然后再用这个prefix和第三个东西比较 注意输入的长度是1的时候，需要输出整个字符串 注意如果比较失败了的话，要直接停止比较！ 58Given a string s consists of upper/lower-case alphabets and empty space characters ‘ ‘, return the length of last word in the string. If the last word does not exist, return 0. Note: A word is defined as a character sequence consists of non-space characters only. Example: Input: “Hello World”Output: 512345678910111213141516class Solution: def lengthOfLastWord(self, s: str) -&gt; int: if len(s) == 0: return 0 while s[-1] == " ": s = s[:-1] if len(s) == 0: return 0 new_str = s.split(" ") # return new_str print(new_str) return len(new_str[-1]) # cnt = 0 # for v in reversed(s): # if v.isspace(): # if cnt: break # else: cnt += 1 # return cnt 原来运行时间也很玄学 但是还是别人的代码看起来厉害一点！ 387Given a string, find the first non-repeating character in it and return it’s index. If it doesn’t exist, return -1. Examples: s = “leetcode”return 0. s = “loveleetcode”,return 2.1234567891011class Solution: def firstUniqChar(self, s: str) -&gt; int: count = collections.Counter(s) index = 0 for ch in s: if count[ch] == 1: return index else: index += 1 return -1 居然有这么个东西叫做counter，感到震惊！！！ 383Given an arbitrary ransom note string and another string containing letters from all the magazines, write a function that will return true if the ransom note can be constructed from the magazines ; otherwise, it will return false. Each letter in the magazine string can only be used once in your ransom note. Note:You may assume that both strings contain only lowercase letters. canConstruct(“a”, “b”) -&gt; falsecanConstruct(“aa”, “ab”) -&gt; falsecanConstruct(“aa”, “aab”) -&gt; true 这个题目也太写意了吧，意思就是我需要写一个勒索信，然后要从杂志上面找单词，看看能不能用杂志上面的东西拼凑出来这个单词 1234567891011class Solution: def canConstruct(self, ransomNote: str, magazine: str) -&gt; bool: alphabet = [0]*26 for ch in magazine: index = ord(ch) - ord('a') alphabet[index] += 1 for ch in ransomNote: index = ord(ch) - ord('a') alphabet[index] -= 1 if alphabet[index] &lt; 0: return False return True 看到了一个清奇的思路然后自己实现了一下 统计magazine里面每个字母的数量，和需要的字母数量对比，如果不够的话就不行 我在写的时候多iteration了一次26个字母，但是其实在ransomNote里面直接对比和0的大小就可以了 感觉字母和数字最大的区别就在于字母有限而数字无限 344reverse一个list，要求in-place而且占用o1的空间12345678910class Solution: def reverseString(self, s: List[str]) -&gt; None: """ Do not return anything, modify s in-place instead. """ length = len(s) for i in range(length // 2): temp = s[i] s[i] = s[length - 1 - i] s[length - 1 - i] = temp 其实可以不用temp的，直接用 s[i]，s[length - 1 - i] = s[length - 1 - i], s[i]就可以了 151reverse一个string，让这句话倒过来，主要会有多个空格123class Solution: def reverseWords(self, s: str) -&gt; str: return " ".join(s.split()[::-1]) python真的是很作弊了 split不加参数就可以直接分开所有大小的空格 70 爬楼梯 DPYou are climbing a stair case. It takes n steps to reach to the top. Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top? Note: Given n will be a positive integer. 123456789101112class Solution: def climbStairs(self, n: int) -&gt; int: if n == 1: return 1 elif n ==0: return 0 elif n == 2: return 2 f = (n+1)*[0]#走n节的时候可以有的方法数量 f[1] = 1 f[2] = 2 for i in range(3,n+1): f[i] = f[i-1] + f[i-2] return f[n] 其实就相当于斐波那契数列，第i种的可能的方法是从i-2走一个2，以及从i-1走一个1的和 345把一个string里面的原因反序 1234567891011121314class Solution: def reverseVowels(self, s: str) -&gt; str: vowels = "AEIOUaeiou" index = [] for i, j in enumerate(s): if j in vowels: index.append(i) s = list(s) i,j = 0,len(index)-1 while i&lt;j: s[index[i]],s[index[j]] = s[index[j]],s[index[i]] i += 1 j -= 1 return "".join(s) 首先判断哪个是原因 然后把元音的部分倒过来 .join把list转回string 205 Isomorphic StringsEasy 767 217 Favorite ShareGiven two strings s and t, determine if they are isomorphic. Two strings are isomorphic if the characters in s can be replaced to get t. All occurrences of a character must be replaced with another character while preserving the order of characters. No two characters may map to the same character but a character may map to itself. Example 1: Input: s = “egg”, t = “add”Output: true1234567891011class Solution: def isIsomorphic(self, s: str, t: str) -&gt; bool: n = len(s) sub_1,sub_2 = [0]*256,[0]*256 for i in range(n): a,b = s[i],t[i] if sub_1[ord(a)] != sub_2[ord(b)]: return False sub_1[ord(a)] = i + 1 sub_2[ord(b)] = i + 1 return True 注意这里面的mapping不一定是字母，也可以是数字 ascii码一共是256个，所以是不会超出这个范围的 主要思路就是这样的，两个数组分别记录的是对应位置的ascii码的mapping的位数，如果这两个位数不一样的话，就说明这两个的mapping方式有问题，所以return False，不然的话return True 290 word patternGiven a pattern and a string str, find if str follows the same pattern. Here follow means a full match, such that there is a bijection between a letter in pattern and a non-empty word in str. Example 1: Input: pattern = “abba”, str = “dog cat cat dog”Output: true12345class Solution: def wordPattern(self, pattern: str, str: str) -&gt; bool: pattern = list(pattern) string = str.split(" ") return len(set(zip(pattern,string))) == len(set(string)) == len(set(pattern)) and len(pattern) == len(string) 又到了活用zip的时候，返回的是一个个对应的东西，也就是说返回的是 a-dog,b-cat,b-cat,a-dog 这时候把他们转化成set，得到的就是不带重复的东西的长度 如果匹配上的长度和原先的长度全都相同（去掉重复的元素），那么就证明匹配上了 49 变位词12345678910111213141516171819class Solution: def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]: # for i,item in enumerate(strs): # item = list(item) # item.sort() # item = "".join(item) # temp[i],temp_sort = item,item # temp_sort.sort() # print(temp,strs) d = &#123;&#125; for word in strs: key = "".join(sorted(word)) if key in d: d.get(key).append(word) else: d[key] = [word] # d[key] = d.get(key,[]) + [word] return d.values() 核心思想 -&gt; 排序，排序之后的变位词就都一样了 leetcode 242,49 这个题的核心思路就是，每个单词按字母顺序排序之后的答案就是这个单词的key，如果两个单词的key一样的话这两个单词就是变位词，如果不一样的话就是新的词 在python里面直接用字典可以很好的储存变位词 56 merge intervalsGiven a collection of intervals, merge all overlapping intervals. Example 1: Input: [[1,3],[2,6],[8,10],[15,18]]Output: [[1,6],[8,10],[15,18]]Explanation: Since intervals [1,3] and [2,6] overlaps, merge them into [1,6]. 12345678910class Solution: def merge(self, intervals: List[List[int]]) -&gt; List[List[int]]: intervals.sort(key = lambda x : x[0]) output = [] for i in intervals: if output and i[0] &lt;= output[-1][1]: output[-1][1] = max(output[-1][1], i[1]) else: output.append(i) return output 注意点： 给的数据输入并不一定是排好序的，所以需要先排好序。这里用到了排序的key的功能。lambda是定义任意函数 g(x)= x[0] 需要输出的格式是list套list，所以需要append 思路错了的一个方向是，其实每个i不应该和隔壁的i比大小，而是应该和output里面的最终结果比大小，因为需要考虑到好几个内容都可以合并的情况 57插入12345678910111213141516171819202122232425262728class Solution: def insert(self, intervals: List[List[int]], newInterval: List[int]) -&gt; List[List[int]]: out = [] adding = False if len(intervals) == 0: return [newInterval] if newInterval[1] &lt; intervals[0][0]: out.append(newInterval) adding = True for i in intervals: if adding is False: if newInterval[0] &gt; i[1]: out.append(i) elif newInterval[1] &lt; i[0]: out.append(newInterval) adding = True else: after_insert = [min(i[0],newInterval[0]),max(i[1],newInterval[1])] out.append(after_insert) adding = True # print("adding") if adding is True: if i[0] &gt; out[-1][1]: out.append(i) else: out[-1][1] = max(out[-1][1],i[1]) if adding is False: out.append(newInterval) return out 自己苦思冥想了一个多小时的答案 有点繁琐，debug的时候主要是情况考虑的不够明确，包括没有考虑空的情况，在最后插入的情况，在最前插入的情况 但是最后总结的想，应该对插入的前后一视同仁，因为状况其实是差不多的，而我把前面分成了好多种状况，后面倒是写成了一种情况 12345678910111213left = []right = []s,e = newInterval[0],newInterval[1]for i in intervals: if i[1] &lt; s: left.append(i) elif i[0] &gt; e: right.append(i) else: s = min(i[0],s) e = max(i[1],e)return left + [[s,e]] + right 这是discussion里面的一种简要的解法，思路的不同就是他是每次都merge到new里面了（也就是s和e），而我是merge到out里面了 其实我的代码本身的也有merge到new的意思，但是被我分出了太多种太复杂的情况 101对称树1234567891011121314151617181920212223# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def isSymmetric(self, root: TreeNode) -&gt; bool: now = [] if root: now.append(root) while now: vals = [] for i in now: if i: vals.append(i.val) else: vals.append(None) if list(reversed(vals)) != vals: return False else: now = [j for i in now if i for j in (i.left, i.right)] return True 遍历的方法，最后一个now的表达式值得学习 注意root是node，而这个node实际的值在vals里面，因为好久没处理node了所以忘记了这一点 12345678910def isSymmetric(self, root: TreeNode) -&gt; bool: def Symm(L,R): if L and R: return L.val == R.val and Symm(L.left,R.right) and Symm(L.right,R.left) else: return L == R # return L is None and R is None #同等意义 return Symm(root, root) # 没有关于空的树的判断条件，所以需要从root开始 这个方法过于优雅，我要哭出来了 87 Scramble StringGiven a string s1, we may represent it as a binary tree by partitioning it to two non-empty substrings recursively. Below is one possible representation of s1 = “great”: great / \ gr eat / \ / \g r e at / \ a tTo scramble the string, we may choose any non-leaf node and swap its two children. For example, if we choose the node “gr” and swap its two children, it produces a scrambled string “rgeat”. rgeat / \ rg eat / \ / \r g e at / \ a t 思路： 首先，如果我这个单词的substring满足这个要求的话，上面一层的单词就满足这个要求，也就是说可以recursive的完成这个工作，对于不同的substring call这个函数来检验是否满足要求 边界条件： 如果string的长度小于等于2，那么怎么换其实都是满足的 如果两个string直接相等，那么也是满足的 先决条件： 如果这两个string的长度都不一样，那么肯定也不一样 如果这两个string里面字母的sort之后都不一样，那么肯定不一样 判断条件： 对于一个string，如果从k位置来分的话，有两种不同的结果。or关系 结果1：s1的前k个和s2的前k个一样 and s1的后n-k个和s2的后n-k个一样 结果2：s1的前k个和s2的后k个一样 and s1的前n-k个和s2的前n-k个一样 1234567891011class Solution: def isScramble(self, s1: str, s2: str) -&gt; bool: n1,n2 = len(s1),len(s2) if n1 != n2 or sorted(s1) != sorted(s2): return False if n1 &lt;= 2 or s1 == s2: return True f = self.isScramble for i in range(1,n1): if (f(s1[0:i], s2[0:i]) and f(s1[i:],s2[i:])) or \ f(s1[0:i], s2[n2-i:]) and f(s1[i:],s2[0:n2-i]): return True return False 38 count and sayThe count-and-say sequence is the sequence of integers with the first five terms as following: 1 11 21 1211 1112211 is read off as “one 1” or 11.11 is read off as “two 1s” or 21.21 is read off as “one 2, then one 1” or 1211. Given an integer n where 1 ≤ n ≤ 30, generate the nth term of the count-and-say sequence. Note: Each term of the sequence of integers will be represented as a string. 自己的智障解法12345678910111213141516171819202122232425262728class Solution: def countAndSay(self, n: int) -&gt; str: if n == 1: return "1" result = [1] for i in range(2,n+1): result = self.Say(result) return result def Say(self,num): n = len(num) counter = 1 counters = "" nums = str(num[0]) for i in range(1,n): if num[i] == num[i-1]: counter += 1 else: counters += str(counter) counter = 1 nums += str(num[i]) counters += str(counter) result = "" for i in range(len(counters)): result += counters[i] result += nums[i] return result 注意，如果要把list接成string，需要先把里面的所有项都转成string 感觉自己还是很不擅长recursive #316 remove deplicate lettersGiven a string which contains only lowercase letters, remove duplicate letters so that every letter appears once and only once. You must make sure your result is the smallest in lexicographical order among all possible results. Example 1: Input: “bcabc”Output: “abc”Example 2: Input: “cbacdcbc”Output: “acdb” 12345678910111213141516171819class Solution: def removeDuplicateLetters(self, s: str) -&gt; str: # s = sorted(s) # i = 0 # for n in s: # # print(n,i,s[i-1]) # if i &lt; 1 or n != s[i-1]: # s[i] = n # i += 1 # return "".join(s[:i]) s = list(s) result = [] last_occurrence = &#123;c: i for i, c in enumerate(s)&#125; for i,n in enumerate(s): if n not in result: while result and n &lt; result[-1] and result[-1] in s[i:]: result.pop() result.append(n) return "".join(result) 这道题里面的重点在lexicographical order 也就是说，在操作的时候，如果这个字母在后面的位置上出现了，但是放在前面的位置上会导致前面变大，那么就取后面的那个结果 本来我想的是可以先把没出现过的放进去，然后再刷新。但是直接放最好的应该更好一些 几种情况： 如果已经出现了：那么直接跳过 如果没出现： 如果比之前的小，并且前面的那个在后面还有，就得往前顶。还要考虑顶没了的情况，也就是result不为空 这里注意这三个条件是并列的，需要同时and。我刚开始把在后面出现放到循环里面去了，所以死循环了 在上面顶完之后，再把最新的加到最后 168 excel column titleGiven a positive integer, return its corresponding column title as appear in an Excel sheet. For example: 1 -&gt; A 2 -&gt; B 3 -&gt; C ... 26 -&gt; Z 27 -&gt; AA 28 -&gt; AB ... 123456789101112131415161718class Solution: def convertToTitle(self, n: int) -&gt; str: result = [] while n &gt; 0: letter = n % 26 n = n // 26 if letter == 0: letter = 26 n = n-1 result.append(letter) result = result[::-1] for i,item in enumerate(result): item += 64 item = str(chr(item)) result[i] = item # print(result) return "".join(result) 自己的傻逼方法： 最先得到的余数应该是最后的字母的值，所以这里出来的result需要翻转一下 翻转list最快的方法是 [::-1] str(chr(n))把数字转成char，ord把char转成数字，大写A是65，小写a是97 1return "" if num == 0 else self.convertToTitle((num - 1) / 26) + chr((num - 1) % 26 + ord('A')) 大佬的一行 忘记了这种str的连接方法 直接减-1计算更方便 171 Excel Sheet Column Number1234567class Solution: def titleToNumber(self, s: str) -&gt; int: # s = s[::-1] # result = 0 # for i,item in enumerate(s): # result += 26^(i) + (ord(item) - ord("A")) return 0 if s == "" else self.titleToNumber(s[:-1]) * 26 + ord(s[-1]) - ord("A") + 1 上面那道题的友情题，模拟大佬写出了解法 注意list的上限，到-1的话是到-2不包括-1 13 roman to integerRoman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Symbol ValueI 1V 5X 10L 50C 100D 500M 1000For example, two is written as II in Roman numeral, just two one’s added together. Twelve is written as, XII, which is simply X + II. The number twenty seven is written as XXVII, which is XX + V + II. Roman numerals are usually written largest to smallest from left to right. However, the numeral for four is not IIII. Instead, the number four is written as IV. Because the one is before the five we subtract it making four. The same principle applies to the number nine, which is written as IX. There are six instances where subtraction is used: I can be placed before V (5) and X (10) to make 4 and 9.X can be placed before L (50) and C (100) to make 40 and 90.C can be placed before D (500) and M (1000) to make 400 and 900.Given a roman numeral, convert it to an integer. Input is guaranteed to be within the range from 1 to 3999.1234567891011121314151617181920class Solution: def romanToInt(self, s: str) -&gt; int: trans = &#123; "I":1, "V":5, "X":10, "L":50, "C":100, "D":500, "M":1000 &#125; s = s.replace("IV","IIII").replace("IX","VIIII") s = s.replace("XL","XXXX").replace("XC","LXXXX") s = s.replace("CD","CCCC").replace("CM","DCCCC") result = 0 for c in s: result += trans[c] return result 比较典型的用dict解决的例子，善用string里面的replace方法 12 int to roman上面的友情题 虽然可以穷举实现，但是我骄傲的自己写出来了recursive的方法 需要注意字母的替换顺序，不然会换错1234567891011121314151617181920212223242526272829303132class Solution: def intToRoman(self, num: int) -&gt; str: space = ["M", "D", "C", "L", "X", "V", "I"] trans = &#123; "I": 1, "V": 5, "X": 10, "L": 50, "C": 100, "D": 500, "M": 1000 &#125; s = self.find_raw(num, 0, space, trans) s = s.replace("DCCCC", "CM").replace("CCCC", "CD") s = s.replace("LXXXX", "XC").replace("XXXX", "XL") s = s.replace("VIIII", "IX").replace("IIII", "IV") return s def find_raw(self, num, name, space, trans): if num &lt; 5: return num * "I" else: temp = (num // trans[space[name]]) * space[name] after = self.find_raw( num % trans[space[name]], name + 1, space, trans) return temp + afters = Solution()print(s.intToRoman(9)) 273 int to english1234567891011121314151617181920212223242526272829class Solution: def numberToWords(self, num: int) -&gt; str: t0to19 = ["Zero", "One", "Two", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Eleven", "Twelve", "Thirteen", "Fourteen", "Fifteen", "Sixteen", "Seventeen", "Eighteen", "Nineteen"] tens = ["Twenty", "Thirty", "Forty", "Fifty", "Sixty", "Seventy", "Eighty", "Ninety"] def word(num, i = 0): if num == 0: return [""] if num &lt; 20: return [t0to19[num]] if num &lt; 100: return [tens[num // 10 - 2]] + word(num % 10) if num &lt; 1000: return [t0to19[num // 100]] + ["Hundred"] + word(num % 100) else: trans = &#123;"Billion": int(1e9), "Million": int( 1e6), "Thousand": int(1e3)&#125; part = ["Billion", "Million", "Thousand"][i] if num // trans[part] == 0: return word(num % trans[part], i + 1) else: return word(num // trans[part]) + [part] + word(num % trans[part], i + 1) s = word(num, 0) while "" in s: s.remove("") return " ".join(s) or "Zero" 注意移除空项的时候，需要用while而不是if 因为最后需要空格连接，所以最好先扔到list里面再出来 这题也太傻比了=。=无论怎么样都要自己手打这么多东西 # 68 text justification需要把这一行字左右对齐1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution: def fullJustify(self, words: List[str], maxWidth: int) -&gt; List[str]: # 每一行填满，如果填不满的时候，词的中间的空格尽量平均 # 最后一行右边加空格 # 每读一个单词后面需要加一行 space = 0 line = [0] # 每一行开头的单词的坐标 for i, word in enumerate(words): if space + len(word) &lt; maxWidth: space += len(word) + 1 elif space + len(word) == maxWidth and i != len(words) - 1: space = 0 line.append(i + 1) elif space + len(word) &gt; maxWidth: space = len(word) + 1 #注意这里的长度变化了 line.append(i) output = [] for i in range(len(line)): if i &lt; len(line) - 1: s = "" this_line = words[line[i]:line[i+1]] length = -1 # 最后一个单词不带空格 for w in this_line: length += len(w) + 1 if len(this_line) == 1: s = this_line[0] + (maxWidth - len(this_line[0])) * " " else: space_len = (maxWidth - length) // (len(this_line) - 1) extra_space = (maxWidth - length) % (len(this_line) - 1) for i,w in enumerate(this_line): if i &lt; len(this_line) - 1: s = s + w + " " + space_len*" " if i &lt;= extra_space - 1: s = s + " " else: s = s + w output.append(s) else: this_line = words[line[i]:] s = " ".join(this_line) s = s + " "*(maxWidth-len(s)) output.append(s) return output 思路 先分开单词 再往里插空格 6把整数转过来12345678910111213141516171819202122class Solution: def reverse(self, x: int) -&gt; int: Positive = True x2 = [] if str(x)[0] == "-": Positive = False x = int(x - x * 2) while x &gt;= 10: num = x % 10 x = x // 10 x2.append(str(num)) x2.append(str(x)) output = "".join(x2) output = int(output) if Positive: output = int(output) else: output = int(output) - 2 * int(output) if output &gt; 2**31 - 1 or output &lt; -2**31: return 0 else: return output 注意啊2的31次方不是2e31啊啊我在干什么]]></content>
      <categories>
        <category>算法</category>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment2之Pytorch]]></title>
    <url>%2F2019%2F05%2F07%2FCS231nassignment2Pytorch%2F</url>
    <content type="text"><![CDATA[这部分需要在torch和TensorFlow两个framework里面选一个。 PyTorchWhat 加入了Tensor的object（类似于narray），不需要手动的backprop了 Why 在GPU上面跑，不需要CUDA就可以在自己的GPU上面跑NN functions很多 站在巨人的肩膀上！ 在实际使用中应该写的深度学习代码 学习资料 Justin Johnson has made an excellenttutorial for PyTorch. DetailedAPI doc If you have other questions that are not addressed by the API docs, the PyTorch forum is a much better place to ask than StackOverflow. 整体结构 第一部分，准备，使用dataset 第二部分，abstraction level1，直接在最底层的Tensors上面操作 第三部分，abstraction level2，nn.Module定义一个任意的NN结构 第四部分，abstraction level3，nn.Sequential，定义一个简单的线性feed - back网络 第五部分，自己调参，尽量让CIFAR - 10的精度尽可能高 Part 1.Preparationpytorch里面有下载dataset，预处理并且迭代成minibatch的功能 import torchvision.transforms as T 这个包包括了预处理以及增强data的功能，在这里选择了减去平均的RGB并且除以标准差 然后对不同的部分分别构建了一个dataset object（训练，测试，val），这个dataset会载入一次training example，并且在DataLoader部分构建minibatch 1234567891011121314151617181920212223242526272829NUM_TRAIN = 49000# The torchvision.transforms package provides tools for preprocessing data# and for performing data augmentation; here we set up a transform to# preprocess the data by subtracting the mean RGB value and dividing by the# standard deviation of each RGB value; we've hardcoded the mean and std.transform = T.Compose([ T.ToTensor(), T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])# We set up a Dataset object for each split (train / val / test); Datasets load# training examples one at a time, so we wrap each Dataset in a DataLoader which# iterates through the Dataset and forms minibatches. We divide the CIFAR-10# training set into train and val sets by passing a Sampler object to the# DataLoader telling how it should sample from the underlying Dataset.cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True, transform=transform)loader_train = DataLoader(cifar10_train, batch_size=64, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True, transform=transform)loader_val = DataLoader(cifar10_val, batch_size=64, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, transform=transform)loader_test = DataLoader(cifar10_test, batch_size=64) 需要一个是否使用GPU的flag，并且set到true。在这个作业里面不是必须用GPU跑，但是如果电脑不能enableCUDA的话，就会自动返回CPU模式。 除此之外，建立了两个global var，dtype代表float32，device代表用哪个 因为mac本身不支持CUDA，而且好像新版本的系统还不能安装N卡的部分，所以现在用的CPU 12345678910111213USE_GPU = Truedtype = torch.float32 # we will be using float throughout this tutorialif USE_GPU and torch.cuda.is_available(): device = torch.device('cuda')else: device = torch.device('cpu')# Constant to control how frequently we print train lossprint_every = 100print('using device:', device) Part2 Barebones PyTorch 虽然有很多高层的API已经有了很多功能，但是这部分从比较底层的部分来进行 建立一个简单的fc - relu net，两个中间层，没有bias 用Tensor的method来计算forward，并且用自带的autograd来计算back 如果设定了requires_grad = True，那么在计算的时候不仅会计算值，还会生成计算back的graph if x is a Tensor with x.requires_grad == True then after backpropagation x.grad will be another Tensor holding the gradient of x with respect to the scalar loss at the end PyTorch Tensors: Flatten Function Tensors是一个和narray很像的东西，定义了很多比较好用的功能，比如flatten来reshape image data 在Tensor里面一个图片的形状是NxCxHxW datapoint的数量 channels feature map的H和W 但是在affine里面我们希望一个datapoint可以表现成一个单独的vector，而不是channel和宽和高 所以在这里用flatten来首先读取NCHW的数据，然后返回这个data的view（相当于array里面的reshape，把它改成了Nx？？，其中？？可以是任何值） 123456def flatten(x): N = x.shape[0] # read in N, C, H, W # "flatten" the C * H * W values into a single vector per image return x.view(N, -1) Barebones PyTorch: Two-Layer Network当定义一个 two_layer_fc的时候，会有两层的中间带relu的forward，在写好了forward之后需要确保输出的形状是对的并且没有什么问题(最近好像对这个大小已经没有什么疑问了) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import torch.nn.functional as F # useful stateless functionsdef two_layer_fc(x, params): """ A fully-connected neural networks; the architecture is: NN is fully connected -&gt; ReLU -&gt; fully connected layer. Note that this function only defines the forward pass; PyTorch will take care of the backward pass for us. The input to the network will be a minibatch of data, of shape (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units, and the output layer will produce scores for C classes. Inputs: - x: A PyTorch Tensor of shape (N, d1, ..., dM) giving a minibatch of input data. - params: A list [w1, w2] of PyTorch Tensors giving weights for the network; w1 has shape (D, H) and w2 has shape (H, C). Returns: - scores: A PyTorch Tensor of shape (N, C) giving classification scores for the input data x. """ # first we flatten the image x = flatten(x) # shape: [batch_size, C x H x W] w1, w2 = params # Forward pass: compute predicted y using operations on Tensors. Since w1 and # w2 have requires_grad=True, operations involving these Tensors will cause # PyTorch to build a computational graph, allowing automatic computation of # gradients. Since we are no longer implementing the backward pass by hand we # don't need to keep references to intermediate values. # you can also use `.clamp(min=0)`, equivalent to F.relu() x = F.relu(x.mm(w1)) x = x.mm(w2) return xdef two_layer_fc_test(): hidden_layer_size = 42 # minibatch size 64, feature dimension 50 x = torch.zeros((64, 50), dtype=dtype) w1 = torch.zeros((50, hidden_layer_size), dtype=dtype) w2 = torch.zeros((hidden_layer_size, 10), dtype=dtype) scores = two_layer_fc(x, [w1, w2]) print(scores.size()) # you should see [64, 10]two_layer_fc_test() Barebones PyTorch: Three-Layer ConvNet 上下这两个都是，在测试的时候可以直接pass 0来测试tensor的大小是不是对的 网络的结构 conv with bias，channel_1 filters，KW1xKH1，2 zero - padding RELU conv with bias，channel_2 filters，KW2xKH2，1 zero - padding RELU fc with bias，输出C class 注意！在这里fc之后没有softmax的激活层，因为在后面计算loss的时候会提供softmax，计算起来更加有效率 注意2！在conv2d之前不需要flatten，在fc之前才需要flatten 123456789101112131415161718192021222324252627282930313233343536373839404142434445def three_layer_convnet(x, params): """ Performs the forward pass of a three-layer convolutional network with the architecture defined above. Inputs: - x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images - params: A list of PyTorch Tensors giving the weights and biases for the network; should contain the following: - conv_w1: PyTorch Tensor of shape (channel_1, 3, KH1, KW1) giving weights for the first convolutional layer - conv_b1: PyTorch Tensor of shape (channel_1,) giving biases for the first convolutional layer - conv_w2: PyTorch Tensor of shape (channel_2, channel_1, KH2, KW2) giving weights for the second convolutional layer - conv_b2: PyTorch Tensor of shape (channel_2,) giving biases for the second convolutional layer - fc_w: PyTorch Tensor giving weights for the fully-connected layer. Can you figure out what the shape should be? (N,channel_2*H*W) - fc_b: PyTorch Tensor giving biases for the fully-connected layer. Can you figure out what the shape should be? (C,) Returns: - scores: PyTorch Tensor of shape (N, C) giving classification scores for x """ conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params scores = None ################################################################################ # TODO: Implement the forward pass for the three-layer ConvNet. # ################################################################################ # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** x = nn.functional.conv2d(x, conv_w1, bias=conv_b1, padding=2) x = nn.functional.conv2d(F.relu(x), conv_w2, bias=conv_b2, padding=1) x = flatten(x) x = x.mm(fc_w) + fc_b scores = x # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ################################################################################ # END OF YOUR CODE # ################################################################################ return scores Barebones PyTorch: Initialization random_weight(shape) initializes a weight tensor with the Kaiming normalization method. -&gt; 使用了KAIMING normal zero_weight(shape) initializes a weight tensor with all zeros. Useful for instantiating bias parameters. 123456789101112131415161718192021222324252627def random_weight(shape): """ Create random Tensors for weights; setting requires_grad=True means that we want to compute gradients for these Tensors during the backward pass. We use Kaiming normalization: sqrt(2 / fan_in) """ if len(shape) == 2: # FC weight fan_in = shape[0] else: # conv weight [out_channel, in_channel, kH, kW] fan_in = np.prod(shape[1:]) # randn is standard normal distribution generator. w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in) w.requires_grad = True return wdef zero_weight(shape): return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)# create a weight of shape [3 x 5]# you should see the type `torch.cuda.FloatTensor` if you use GPU.# Otherwise it should be `torch.FloatTensor`random_weight((3, 5)) Barebones PyTorch: Check Accuracy 在这部分不需要计算grad，所以要关上torch.no_grad()避免浪费 输入 一个DataLoader来给我们想要check的data分块 一个表示模型到底是什么样子的model_fn，来计算预测的scores 这个model需要的参数 没有返回值但是会print出来acc 12345678910111213141516171819202122232425262728def check_accuracy_part2(loader, model_fn, params): """ Check the accuracy of a classification model. Inputs: - loader: A DataLoader for the data split we want to check - model_fn: A function that performs the forward pass of the model, with the signature scores = model_fn(x, params) - params: List of PyTorch Tensors giving parameters of the model Returns: Nothing, but prints the accuracy of the model """ split = 'val' if loader.dataset.train else 'test' print('Checking accuracy on the %s set' % split) num_correct, num_samples = 0, 0 with torch.no_grad(): for x, y in loader: x = x.to(device=device, dtype=dtype) # move to device, e.g. GPU y = y.to(device=device, dtype=torch.int64) scores = model_fn(x, params) _, preds = scores.max(1) num_correct += (preds == y).sum() num_samples += preds.size(0) acc = float(num_correct) / num_samples print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc)) BareBones PyTorch: Training Loop 用stochastic gradient descent without momentum来train，并且用torch.functional.cross_entropy来计算loss 输入 model_fc params learning_rate 没有输出 进行的操作 把data移动到GPU或者CPU 计算score和loss loss.backward() update params，这部分不需要计算grad BareBones PyTorch: Training a ConvNet 需要网络 Convolutional layer(with bias) with 32 5x5 filters, with zero - padding of 2 ReLU Convolutional layer(with bias) with 16 3x3 filters, with zero - padding of 1 ReLU Fully - connected layer(with bias) to compute scores for 10 classes 需要自己初始化参数，不需要tune hypers 注意1：fc的w的大小是D,C，跟数据无关需要从上一层的输出求 conv之后的图片大小从32-&gt; 30 12345678910111213141516171819202122232425262728293031learning_rate = 3e-3channel_1 = 32channel_2 = 16conv_w1 = Noneconv_b1 = Noneconv_w2 = Noneconv_b2 = Nonefc_w = Nonefc_b = None################################################################################# TODO: Initialize the parameters of a three-layer ConvNet. ################################################################################## *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****conv_w1 = random_weight((channel_1, 3, 5, 5))conv_b1 = zero_weight(channel_1)conv_w2 = random_weight((channel_2, channel_1, 5, 5))conv_b2 = zero_weight(channel_2)fc_w = random_weight((channel_2 * 30 * 30, 10))fc_b = zero_weight(10)# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****################################################################################# END OF YOUR CODE #################################################################################params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]train_part2(three_layer_convnet, params, learning_rate) Part3 PyTorch Module API 上面的所有过程是手算来track整个过程的，但是在更大的net里面就没有什么用了 nn.Module来定义网络，并且可以选optmi的方法 Subclass nn.Module. Give your network class an intuitive name like TwoLayerFC. __init__()里面定义自己需要的所有层. nn.Linear and nn.Conv2d 都在模块里自带了. nn.Module will track these internal parameters for you. Refer to the doc to learn more about the dozens of builtin layers. Warning: don’t forget to call the super().__init__() first!（调用父类） In the forward() method, define the connectivity of your network. 直接用init里面初始化好的方法来forward，不要再forward里面增加新的方法 用上面的方法来写一个三层的layer 注意需要初始化w和b的参数，用kaiming的方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class ThreeLayerConvNet(nn.Module): def __init__(self, in_channel, channel_1, channel_2, num_classes): super().__init__() ######################################################################## # TODO: Set up the layers you need for a three-layer ConvNet with the # # architecture defined above. # ######################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** self.conv_1 = nn.Conv2d(in_channel,channel_1,5,stride=1, padding=2,bias=True) nn.init.kaiming_normal_(self.conv_1.weight) nn.init.constant_(self.conv_1.bias, 0) self.conv_2 = nn.Conv2d(channel_1,channel_2,3,stride=1, padding=1,bias=True) nn.init.kaiming_normal_(self.conv_2.weight) nn.init.constant_(self.conv_2.bias, 0) self.fc_3 = nn.Linear(channel_2 * 32 * 32 , num_classes) nn.init.kaiming_normal_(self.fc_3.weight) nn.init.constant_(self.fc_3.bias, 0) # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ######################################################################## # END OF YOUR CODE # ######################################################################## def forward(self, x): scores = None ######################################################################## # TODO: Implement the forward function for a 3-layer ConvNet. you # # should use the layers you defined in __init__ and specify the # # connectivity of those layers in forward() # ######################################################################## # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** x = self.conv_1(x) x = self.conv_2(F.relu(x)) x = flatten(F.relu(x)) x = self.fc_3(x) scores = x # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** ######################################################################## # END OF YOUR CODE # ######################################################################## return scoresdef test_ThreeLayerConvNet(): x = torch.zeros((64, 3, 32, 32), dtype=dtype) # minibatch size 64, image size [3, 32, 32] model = ThreeLayerConvNet(in_channel=3, channel_1=12, channel_2=8, num_classes=10) scores = model(x) print(scores.size()) # you should see [64, 10]test_ThreeLayerConvNet() Module API: Check Accuracy 不用手动pass参数了，直接就可以得到整个net的acc Module API: Training Loop 用optimizer这个object来update weights 输入 model optimizer epoch，可选 没有return，但是会打印出来training时候的acc 其实就是设置好model和optimizer就可以了 Part4 PyTorch Sequential API nn.Sequential没有上面的灵活，但是可以集成上面的一串功能 需要提前定义一个在forward里面能用的flatten 123456789101112131415161718192021# We need to wrap `flatten` function in a module in order to stack it# in nn.Sequentialclass Flatten(nn.Module): def forward(self, x): return flatten(x)hidden_layer_size = 4000learning_rate = 1e-2model = nn.Sequential( Flatten(), nn.Linear(3 * 32 * 32, hidden_layer_size), nn.ReLU(), nn.Linear(hidden_layer_size, 10),)# you can use Nesterov momentum in optim.SGDoptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)train_part34(model, optimizer) 实现三层，注意需要初始化参数 这里遇到了一个问题是当用random_weight实现的时候，acc会特别低 从这里发现可以重新定义另一个计算方法不同的weights 从这里得知如何给module增加新的function 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def xavier_normal(shape): """ Create random Tensors for weights; setting requires_grad=True means that we want to compute gradients for these Tensors during the backward pass. We use Xavier normalization: sqrt(2 / (fan_in + fan_out)) """ if len(shape) == 2: # FC weight fan_in = shape[1] fan_out = shape[0] else: fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW] fan_out = shape[0] * shape[2] * shape[3] # randn is standard normal distribution generator. w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / (fan_in + fan_out)) w.requires_grad = True return wchannel_1 = 32channel_2 = 16learning_rate = 1e-2model = Noneoptimizer = None################################################################################# TODO: Rewrite the 2-layer ConvNet with bias from Part III with the ## Sequential API. ################################################################################## *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****model = nn.Sequential( nn.Conv2d(3, channel_1,5,stride = 1,padding = 2), nn.ReLU(), nn.Conv2d(channel_1, channel_2,3,stride = 1,padding = 1), nn.ReLU(), Flatten(), nn.Linear(32*32*channel_2, 10),)def init_weights(m): print(m) if type(m) == nn.Linear: m.weight.data = xavier_normal(m.weight.size()) m.bias.data = zero_weight(m.bias.size())model.apply(init_weights)optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****################################################################################# END OF YOUR CODE ################################################################################train_part34(model, optimizer) Part5 来训练CIFAR-10吧！自己找net的结构，hyper，loss，optimizers来把CIFAR-10的val_acc在10个epoch之内升到70%以上！ Layers in torch.nn package: http://pytorch.org/docs/stable/nn.html Activations: http://pytorch.org/docs/stable/nn.html#non-linear-activations Loss functions: http://pytorch.org/docs/stable/nn.html#loss-functions Optimizers: http://pytorch.org/docs/stable/optim.html 一些可能的方法： Filter size: Above we used 5x5; would smaller filters be more efficient? Number of filters: Above we used 32 filters. Do more or fewer do better? Pooling vs Strided Convolution: Do you use max pooling or just stride convolutions? Batch normalization: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster? Network architecture: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include: [conv-relu-pool]xN -&gt; [affine]xM -&gt; [softmax or SVM] [conv-relu-conv-relu-pool]xN -&gt; [affine]xM -&gt; [softmax or SVM] [batchnorm-relu-conv]xN -&gt; [affine]xM -&gt; [softmax or SVM] Global Average Pooling: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in Google’s Inception Network (See Table 1 for their architecture). Regularization: Add l2 weight regularization, or perhaps use Dropout. 一些tips： 应该会在几百个iter里面就看到进步，如果params work well tune hyper的时候从一大片range和小的train开始，找到好一些的之后再围绕这个范围找（多训一点） 在找hyper的时候应该用val set 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647model = Noneoptimizer = None# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****channel_1 = 16channel_2 = 32channel_3 = 64channel_4 = 64fc_1 = 1024num_classes = 10model = nn.Sequential( nn.Conv2d(3, channel_1,3,stride = 1,padding = 1), nn.BatchNorm2d(channel_1), nn.ReLU(), nn.MaxPool2d(kernel_size = 2), nn.Conv2d(channel_1, channel_2,3,stride = 1,padding = 1), nn.BatchNorm2d(channel_2), nn.ReLU(), nn.MaxPool2d(kernel_size = 2), nn.Conv2d(channel_2, channel_3,3,stride = 1,padding = 1), nn.BatchNorm2d(channel_3), nn.ReLU(), nn.Conv2d(channel_3, channel_4,3,stride = 1,padding = 1), nn.BatchNorm2d(channel_4), nn.ReLU(), nn.MaxPool2d(kernel_size = 2), Flatten(), nn.Linear(4*4*channel_4, num_classes)# nn.Linear(fc_1,num_classes) )learning_rate = 1e-3optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****################################################################################# END OF YOUR CODE ################################################################################# You should get at least 70% accuracytrain_part34(model, optimizer, epochs=10) 第四层conv试过ksize=1，效果不是很好 BN好像效果很好 maxpool多一些，计算负担少而且效果好像比较好 最终val_acc在77-79左右，test_acc = 76.22]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于python生成动态变量名]]></title>
    <url>%2F2019%2F05%2F07%2F%E5%85%B3%E4%BA%8Epython%E7%94%9F%E6%88%90%E5%8A%A8%E6%80%81%E5%8F%98%E9%87%8F%E5%90%8D%2F</url>
    <content type="text"><![CDATA[动态生成变量名如果想要生成一系列的a0，a1，….a20这种变量名，直接手写太麻烦了 localslocal()，以字典的类型返回当前位置的全部局部变量 1234arrange_list = locals()for i in range(10): arrange_list['list_' + str(i)] = [] 调用动态变量，可以用字典的get方法得到变量的值 1234arrange_list = locals()for i in range(10): print(arrange_list.get('var'+str(i)), end = " ") 利用exec进行赋值12for i in range(5): exec('var&#123;&#125; = &#123;&#125;'.format(i, i)) 调用动态变量12for i in range(5): exec('print(var&#123;&#125;, end = " ")'.format(i))]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>动态生成变量名</tag>
        <tag>变量名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于多维数组的转置和增加新的维度]]></title>
    <url>%2F2019%2F04%2F25%2F%E5%85%B3%E4%BA%8E%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%E7%9A%84%E8%BD%AC%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在二维转置的时候，a[i][j] = a[j][i]在多维数组转置的时候，需要交换他们的下标比如原来的数组是(X,Y,Z)，转置之后是(Z,X,Y)这时候应该用的是np.transpose(A,(2,0,1)) np.newaxis -&gt; 增加新的维度原来是（6，）的数组，在行上增加维度变成（1,6）的二维数组，在列上增加维度变为(6,1)的二维数组]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>narray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习OpenCV十八章_Camera models & calibration]]></title>
    <url>%2F2019%2F04%2F22%2FOpenCVCameracalibration%2F</url>
    <content type="text"><![CDATA[camera models &amp; calibration物体会吸收一部分的光，然后反射一部分的光，反射的光就是他自己的颜色，这个光被我们的眼睛（或者相机）接收，然后投影到我们的视网膜（或者相机的图片）上，这之间的几何关系在CV上面非常重要 其中一个非常简单的模型就是pinhole camera model。光穿过一面墙上的一个小的aperture，这个是这章的模型的开始，但是真实pinhole模型不是很好因为他不能快速曝光（聚集的光不够）-&gt; 眼睛会更厉害一点，但是len还会distort图片。 这章的目的： 如何camera calibration 纠正普通的pinhole模型的len的偏差 calibration也同样是获取三维世界的主要方式，因为一个场景不仅仅是三维，他们还有物理的空间和体积，所以获取pixel和三维诗句坐标的关系也很重要 18章纠正的是len的distortion，19章构建整个3D的结构 homography transform -&gt; 一个非常重要的要素 camera model 投影到image plane上面，结果在这个plane上面总是对焦的focus，图片的大小和这个焦距的长度有关 对于理想的pinhole来说，image plane到pinhole的距离就是准确的焦距 从这个基础的模型上 -&gt; 得到一个计算起来更加简单的模型 交换pinhole和projection plane的位置 现在pinhole的位置变成了projective plane的中心 每一个离开物体表面（Q）的光线都朝着projection center走 在横轴和投影面上的交点被定义为principal point 这个新的平面和以前的projective平面一样，上面投影上的物体也都是和原来一样的尺寸 换了模型之后没有了负号：x/f = X/Z 在理想的模型里可能觉得这个principal point就是image的中心，但是实际上中心不会在横轴和投影面的交点上 引入了两个新的参数 cx 和 cy 这两个参数实际上就是中心点的偏差（平面上的偏差），所以得到投影在image plane上面的实际坐标如下 在上面的公式里面用了两个不同的f，fx和fy，这是因为 在实际的图片里来说，其实每个像素格不是正方形而是长方形的 fx = 实际的focal length * sx（每个mm里面的像素数量） -&gt; 最终得到的fx是像素格 注意： sx和sy在calibration的时候并不能直接测量 physical focal length也不能被实际测量 我们只能得到这两个东西的乘积，f basic of projective geometry projective transform -&gt; 把physical world里面的一组点Qi（Xi,Yi,Zi）map到一张图片上面(xi,yi)的过程 用这个东西的时候，一个比较方便的方法是用homogeneous coordinates associ‐ ated with a point in a projective space of dimension n are typically expressed as an (n + 1)-dimensional vector (e.g., x, y, z becomes x, y, z, w), with the additional restric‐ tion that any two points whose values are proportional are, in fact, equivalent points 投影平面上面的维度是两维，我们可以把它表示成三维的东西 -&gt; 把现在存在维度的数字除以增加的维度的值就可以得到以前的值 用这种办法，可以把之前的fx，fy,cx,cy重新组织成一个矩阵：camera intrinsics matrix 下面这个形式重新乘回来就是之前的关系 在opencv里面也有得到homogeneous coordinates和由结果反推回来的函数 注意，在pinhole里面的成像速度是非常慢的，如果需要更快速地形成图片，我们需要通过lens来聚焦非常广范围里面的光 -&gt; 但是结果就是lens会产生distortion Rodrigues Transform 在三维的范围里，经常会使用一个3x3的矩阵来表示一个物体的旋转 只要把需要旋转的vector乘上这个mat就可以得到相应的结果 但是不是很好直观的得到这个旋转矩阵 介绍一种在opencv里面的表示方法 -&gt; 更容易直观的理解意思 本质上来说就是用一个vector表示每个角度上需要旋转多少 Rodrigues Transform指的就是矩阵表示法和向量表示法之间的关系 数学原理：余弦定理？（知道两个向量可以求出来他们之间的角度） 这两个关系之间可以很轻易的互相转化，opencv里面也有相应的库 lens distortion 在实际使用中因为制造球形的镜头更容易一些，并且很难测量是不是平的，所以lens都会产生distortion 在这部分介绍了两种主要得distortion，how to model radial distortion -&gt; 镜片的形状产生的 tangential distortion -&gt; 组装整个相机的时候产生的 radial 相机的distortion一般都会产生在接近imager边缘的部分（fisheye effect） 远离lens中心的部分比起中心部分会折叠更多，所以如果投影一个正方形，边的部分都会鼓起来 如果相机比较便宜的话（web camera），周围的折叠会更多，而好的相机会更注重减少radial distortion的效果 对于辐射的畸变来说，distortion会随着接近边边而增加 在实际中这个畸变很小，所以可以用泰勒级数的r=0附近展开来解决 对于比较便宜的web camera，可以选用k1或者k2 对于鱼眼这种畸变很大的，可以用k3 在distort之后的位置可以用以下的公式表示 (x,y)是原来的位置，corrected是教政治和的位置 r是离开中心的半径 tangential 在制造相机的时候，lens和image plane没有完全平行导致的，所以投影上去会是一个几何变换 这个distortion基本是由两个参数组成：p1和p2 总结下来，在相机的distortion里一共有五个参数，k1k2k3p1p2，这五个函数构成了一个distortion vector(5x1) 虽然在图像里面还有一些其他的畸变，但是因为影响没有这两个大所以opencv没有考虑这部分 calibration 上一部分得到了如何表示相机的参数以及distortion的参数 这部分考虑如何计算这些参数 其中一个函数clibrationCamera() 用相机去照一个已经知道结构的东西，里面有很多已经定义好了的点 通过这个可以得到相机的相对位置和角度，同时也可以得到intrinsic parameters 平移矩阵和旋转矩阵 对于每张照的图片的物体，这个物体的pose可以用一个旋转矩阵+一个平移矩阵描述，也就是用这个矩阵把现实世界中的点转化到投影平面上 旋转矩阵 旋转运动无论在多少维都可以被描述为：一个坐标的vector乘对应大小的方阵 -&gt; 用一个新的坐标系来描述这个点的位置 -&gt; 其实也就是改成了极坐标系？ 三维范围里面的旋转可以用两个角度表示 绕着x，y，z三个方向旋转的角度以及对应的矩阵是这个样子的 这三个方向的R乘在一起就是最后的旋转矩阵R，但是这个的方向是反着的，所以还需要一个transpose转回来 平移矩阵 平移矩阵用来描述怎么从一个坐标系统shift到另一个坐标系统 -&gt; 也就是一个从第一个坐标系原点到第二个坐标系原点的offset 在calibration的时候，就是从物体坐标系的原点到了相机坐标系的原点 平移矩阵： T→ = origin_object − origin_camera. 综合 结合上面两个矩阵来说，从object上面的一个点投影到camera plane上面的一个点的关系为 Pc→ =R⋅(Po→ −T→) 注意分清楚这里面的矩阵和向量 把上面的这个公式，再加上camera自己的intrinsic-correction。整体就是opencv里面需要求的所有部分 所求参数 三维的旋转用三个角度表示，三维的平移用三个parameter表示(x,y,z) -&gt; 现在得到了6个参数 相机的intrinsic mat(fx,fy,cx,cy) -&gt; 一共四个参数 现在一共需要求10个参数（但是相机的intrinsic是不变的） 求参数 -&gt; 在求解的时候，如果使用一个平面物体，那么每张图片都可以得到8个参数（位置的6个会随着图片变化 + 只能用两个参数来求intrinsic） 至少需要2张图片来得到所有的参数 calibration boards 从原则上来说，任何有特征的东西都可以被用来calibration，包括棋盘，圆格，randpattern，arUco等等,有些方法是基于三维的物体的基础上的，但是二维平面的物体更好操作 在这里主要选择的是用棋盘进行calibration 关于chessboard的函数cv::findChessboardCorners() 可以用这个函数找到棋盘的corners， param 需要输入8bit图片 需要输入这个棋盘每行每列应该有的格子数（计算的是内部点） 输出的是这么corner的坐标 可选flag决定需不需要多余的filter cv::cornerSubPix() 上面一步找到的只是corner的大概位置 在find corner里面自动call了这个函数，为了能得到更精确的结果 如果需要得到更精确的结果，可以重复的call这个函数，但是会有tighter termination criteria cv::drawChessboardCorners() 为debug用，更明确的画出来找到的corner 如果没有找到所有的，会把其他可能的用红色circle画出来，如果找到了，每一行的颜色会不一样 下一步转到perspective transform，这个transform会形成一个3x3的homography mat 关于circle grid的函数cv::findCirclesGrid() 和上面的棋盘没有什么本质的区别，主要就是画出来一个是黑白格，另一个是白色的背景上面有黑色的圆点，输出小圆点的位置 这个方法需要圆点是对称的，上下一组算做一行，竖着一列算一列，怎么数非常重要 Homography planar homography是一个平面到另一个平面的projection mapping，所以从一个2D平面到相机平面的过程就是一个planar homography 用矩阵的乘法就可以表示这个过程 其中Q是现实中的点，q是成像器上面的点，整体关系为：q→ = s ⋅ H ⋅ Q→ s，一个随意的scale参数，homography就是由着一个参数决定的 conventionally factored H, H由两个部分组成 physical上面的transformation，实际就是我们看到的这个物体的位置W = [R,t→] projection，取决于相机的intrinsic q→ = s ⋅ M ⋅ W ⋅ Q→，其中M是相机的intrinsic mat 我们希望Q不是给所有空间定义的点，而是一个定义在我们看的平面上面的坐标，这样计算起来会方便（三维转二维） 所以把Q里面的Z的坐标改成了0，这样旋转矩阵就会被简化为一个3x1的列 并且第三个列乘了Z的0之后就被消掉了 最后就可以把H表示出来了 -&gt; 3x3 = intrinsic(3x3) x (rota + trans)(1x3) 在计算homography mat的时候，用了多张同样内容的东西来计算translation和intrinsic 三个旋转，三个平移 -&gt; 每张图片有6个未知的参数 每张图片可以得到8个等式 把一个正方形mapping成一个四边形可以得到4个不同的(x,y) points 所以每多一张图片就可以多出来计算两个新的参数的机会 这样看，pdst→ = H * psrc→，反着也可以推回来，这样我们就算不知道M也可以计算H，或者说我们是用H来计算M 在opencv里面，cv::findHomography()可以用take一堆有关系的点然后返回他们之间的homo mat，点越多计算的越准确 虽然有其他的方法可以计算结果，但是对测量误差不是很友好 three robust fitting methods method to cv::RANSAC 随机的选择提供的点的subset，然后只用这些subset来计算homo mat 然后把剩下的数据拿来计算一下靠谱和不靠谱的 最后保存最有潜力的inliers 在现实中比较好用，可以过滤掉一部分噪音 LMeDS algorithm 减少median error 不需要更多的info和data来运行 但是it will perform well only if the inliers constitute at least a majority of the data points RHO algorithm 加权的第一种方法，运行速度更快 camera calibration棋盘corner个数 到底有多少参数 camera intrinsic 四个 distortion五个（或者更多）三个辐射（可以增加到6个） + 两个平移 这五个参数是从2D -&gt; 2D的 三个corner points可以得到6个信息，足够处理这五个参数 所以一张图就够了（只是原则上这么说） extrinsic parameters，这个东西的实际位置 但是因为intrinsic和extrinsic之间有对应的关系，一张图片并不够 -&gt; 因为在一张图片里还需要计算extrinsic的部分 假设有N个corner，一共有K个images（不同的position） 一共会有 2 N K个，2是x,y的坐标会有两个，然后N个corner，K个图片 暂时忽略distortion的参数，这样需要4个in和6K个ex（因为每张图片的ex都是不一样的） 2NK &gt;= 6K + 4 如果N = 5，只需要一张图片就可以解决。但是为了得到homo mat，至少需要两个K(之前说到过的) 无论检测到多少corner，得到的有用信息就是四个角 -&gt; 由此推测至少两个K 在实际的应用里面，一般需要7x8，至少十张图，这样受到noise的影响更小 具体的数学计算 为了简单，首先假设在calibration的时候根本没有distortion 对于每个view，会得到一个Homo mat，把这个mat拆成一个列向量(3x1) 在前面也知道H可以拆成M和一个[r1,r2,t]的向量相乘，再乘上一个scale s H=[h1,h2,h3] =s⋅M⋅[r1,r2,t],其中landa是1/s: 旋转向量的基底(orthogonal)是互相垂直的，因为已经把scale这个参数提出去了，所以可以直接认为r1和r2是基了，这样的话他们的点乘是0 把r1和r2用M和h来表示，这样的话r1r2等于0就可以转化成一个hM的公式 r1和r2的模也相等，所以可以继续得到一个等式 设置一个矩阵B等于M.-T * M-1，这样可以计算出来B的值(B算出来是对称的) 把B带回原来的等式，化简，然后把K个等式叠加在一起 这样就可以推出来几个参数的表达式 calibration的函数cv::calibrateCamera().来解决calibration的问题 得到的结果包括in mat, dis_co, 旋转向量和平移向量 输出的in mat的大小是3x3 输出的dis_co的大小取决于用多少级的distortion，一般来说是4，5个的已经对fisheye足够了，8个的话calibration的精度就特别高了 如果需要高精度的calibration的话，需要的图片数量也会疯狂增加 输入的部分包括 物体的坐标，指的是在chessboard上面的坐标点，是二维的点，其实也就是第几个格子？ 注意这里，统计的单位是格子，所以如果想要得到physical上的距离，需要在calibration board上量出来一个格子的长度，然后乘这个格子的数量 image上面的坐标，corners 一口气计算所有的参数不是很好实现，一般使用的方法是先固定一部分计算另一部分，然后再固定另一部分计算这一部分。当所有的东西都估计的差不多了，再一起计算 最后还有一个参数是termination criteria，终止的基准 -&gt; epsilon 会根据一个error来计算是否终止 只计算extrinsiccv::slovePnP() 有的时候我们已经得到了相机的intrinsic，只希望得到object的位置 大部分内容和上面都是一样的，除了 物体的位置只需要一个view distCo和intrinsic都是自己设置好的，不需要计算 cv::solvePnPRansac() 上面的函数对于outliers的robust效果不是很好，对于chessboard来说，这个robust不是很重要，因为棋盘自己本身已经很可靠了。但是对于现实世界中的物体来说不是这么可靠 加入了RANSAC部分？ Undistortion 在calibration里面有两个需要解决的事情，一个是distortion，一个是三维表达的正确性 opencv自己有一个可以用的方法 cv::undistor() -&gt; 可以一瞬间完成 cv::initUndistortRectifyMap() + cv::remap() -&gt; 在video上面使用的时候效率更高一些 undistortion map 在把一张图片undistort的时候，我们需要把每个像素都对应到output里面对应的地方去，有几种不同的表达方法 2-channel float 有一个对于NxM的remapping，表示成NxM的array，有两个channel（分别对应X和Y方向的remap），里面是浮点数 对于每一个输入的像素位置(i,j)，有一个对于这两个位置的向量，来表达这两个量应该哪里去 如果计算出来的结果不是一个整数，那么用interpolation来计算最后应该占的格子的数量 第二种表达式是 2-array float，每一个array是一个channel的移动 第三种是fixed point，计算的速度更快一点，但是需要提供的信息的精确度更高 cv::convertMaps() 因为有上面的三种不同的表达形式，所以这个函数用来在各个形式之间转变 cv::initUndistortRectifyMap() 从刚才的部分知道了到底什么是undistortion map，现在开始讨论如何计算这个map 现在先从单目相机开始monocular，如果双目的话可以直接计算depth（下一章） 步骤，分开是因为计算map只需要一次 先计算undistortion map cv::initUndistortRectifyMap() 输入的参数是intrinsic mat和distortion coefficient（从camera calibration得到的） 可以得到一个新的camera mat，这样的话即使不undistortion也可以得到正确的图片（在多个相机的calibration的时候比较重要） 最后会输出两张map 然后在图片上undistort cv::remap() 当计算了上面的map之后，就可以用remap这个函数进行校正了 输入的map的种类也是上面提到的三种都可以 cv::undistort() 如果只有一张图片，或者对于每张图片都需要重新计算map的时候，就需要用这个函数了（所以在项目里面用这个的速度会变慢） sparse undistortion cv::distortionPoints() 如果我没有整张图片，只有一些图片上的点，然后我只关心这些图片上的点，可以用这个函数计算这张图片上面关注点的位置]]></content>
      <categories>
        <category>图像处理</category>
        <category>OpenCV</category>
        <category>Calibration</category>
      </categories>
      <tags>
        <tag>camera</tag>
        <tag>calibration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231nassignment2CNN]]></title>
    <url>%2F2019%2F04%2F18%2FCS231nassignment2CNN%2F</url>
    <content type="text"><![CDATA[target 之前已经实践了fc的相关东西，但是在实际的使用里大家使用的都是CNN 所以这部分就开始实践CNN了 convolution: Native forward pass CNN的核心部分就是卷积 in cs231n/layers.py，conv_forward_naive 首先这时候不用考虑效率问题，最轻松的写就可以了 输入的数据是N个data，每个有C个channel，H的高度和W的宽度 每个输入和F个不同的filter做卷积，每个卷积核对所有的channel作用，卷积核的大小是HHxWW input x, (N,C,H,W) w, fliter weights of shape (F,C,HH,WW) b, bias, (F,) conv_param: dict “stride” 步长 “pad” zero-padding的大小 注意在padding的时候不要调整x，而是得到一个padding之后的新的东西 output out, (N,F,H’,W’) H’ = 1 + (H + 2 * pad - HH) / stride W’ = 1 + (W + 2 * pad - WW) / stride cache: (x,w,b,conv_param) implement 首先需要对输入的图片进行padding np.pad 输入的array pad的宽度，如果默认的话就是前后都加，然后是这个数字的宽度 -&gt; 注意这里的时候因为一共有四个维度，前两个维度是不用pad的 mode = ‘constant’ constant_values，表示的是pad进去的值，可以前后pad的不一样，因为这里是0-padding所以这里是0 要对所有图片进行处理，需要在N个图片里选择一个 在filter的所有里面选择一个 考虑在H方向和W方向的移动步数，然后通过这个步数和步长的乘积在原图里面取需要做卷积的部分 注意这里可以不用考虑channel，因为图片和filter的channel是同样的层数，所以直接可以boardcast 然后这个部分和卷积核相乘（直接乘），求和，加上bias，就是这个像素点上应该的数值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def conv_forward_naive(x, w, b, conv_param): """ A naive implementation of the forward pass for a convolutional layer. The input consists of N data points, each with C channels, height H and width W. We convolve each input with F different filters, where each filter spans all C channels and has height HH and width WW. Input: - x: Input data of shape (N, C, H, W) - w: Filter weights of shape (F, C, HH, WW) - b: Biases, of shape (F,) - conv_param: A dictionary with the following keys: - 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions. - 'pad': The number of pixels that will be used to zero-pad the input. During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides) along the height and width axes of the input. Be careful not to modfiy the original input x directly. Returns a tuple of: - out: Output data, of shape (N, F, H', W') where H' and W' are given by H' = 1 + (H + 2 * pad - HH) / stride W' = 1 + (W + 2 * pad - WW) / stride - cache: (x, w, b, conv_param) """ out = None ########################################################################### # TODO: Implement the convolutional forward pass. # # Hint: you can use the function np.pad for padding. # ########################################################################### stride = conv_param['stride'] pad = conv_param['pad'] N, C, H, W = x.shape F, _, HH, WW = w.shape H_out = 1 + (H + 2 * pad - HH) // stride W_out = 1 + (W + 2 * pad - WW) // stride out = np.zeros((N, F, H_out, W_out)) x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), 'constant', constant_values=0) # 对原图片的每层进行卷积 for pics in range(N): image = x_pad[pics] for filters in range(F): for H_move in range(H_out): for W_move in range(W_out): image_conv = image[:, stride * H_move: stride * H_move + HH, stride * W_move: stride * W_move + WW] filter_conv = w[filters, :] out_pixel = np.sum(image_conv * filter_conv) + b[filters] out[pics, filters, H_move, W_move] = out_pixel ########################################################################### # END OF YOUR CODE # ########################################################################### cache = (x, w, b, conv_param) return out, cache 可视化中间的图像过程 这里输入了两个不同的输入图片 分别可视化了这个图片的不同weights Convolution: Naive backward pass愉快的简单计算back的过程，先不用考虑cost input dout cache（x,w,b,conv_param) -&gt; 参数是padding 和 stride output dx dw db 实现：conv是怎么求导的？其实排除位置的改变之外，forward只进行了三个操作 把x padding为x_pad wx_pad_conv + b -&gt; 求出一个大小和filter相同的矩阵 把求出来的一个(HH,WW)的矩阵的所有值求sum backward的思路 首先，每一张图片的每一个channel的，dout的大小和输出图片的大小一样 应该是H_out = 1 + (H + 2 * pad - HH) // stride这样求出来的结果 整个dout的size是(N,F,Hout,Wout)，其中N是之前图片的数量，F是新形成的图片的channel 所以在for循环中，dout中选中[n,f,hout,wout]，就可以得到这个特点定的值，称为df df的得到方法是wx+b得到一个矩阵，然后再对这个矩阵求和 因为求和实际就是累加，求导数的时候只要把每一个格子的dx，dw，db导数求出来，然后加在一起就行了 因为公式就是wx + b，所以dx是w，dw是x，db是常数 -&gt; 然后再把每个格子求出来的加在一起，注意各个矩阵的大小，dx应该是在x矩阵里取做卷积的部分，这部分的导数等于w乘df的和 最后，因为x被padding了，dx应该去dx_pad中没有被padding的部分，也就是从[pad:pad + H] 代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def conv_backward_naive(dout, cache): """ A naive implementation of the backward pass for a convolutional layer. Inputs: - dout: Upstream derivatives. - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive Returns a tuple of: - dx: Gradient with respect to x - dw: Gradient with respect to w - db: Gradient with respect to b """ dx, dw, db = None, None, None ########################################################################### # TODO: Implement the convolutional backward pass. # ########################################################################### x, w, b, conv_param = cache N, C, H, W = x.shape F, _, HH, WW = w.shape _, _, H_dout, W_dout = dout.shape stride = conv_param['stride'] pad = conv_param['pad'] x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), 'constant', constant_values=0) db = np.zeros_like(b) dw = np.zeros_like(w) dx_pad = np.zeros_like(x_pad) # print(dx_pad.shape) for pics in range(N): for filters in range(F): for H_move in range(H_dout): for W_move in range(W_dout): # f=sum(wx_pad + b) (df is a number now) df = dout[pics, filters, H_move, W_move] # d for sum, size (HH,WW) # dsum = df * np.ones((HH, WW)) db[filters] += df dx_pad[pics, :, H_move * stride: H_move * stride + HH, W_move * stride: W_move * stride + WW] += df * w[filters] dw[filters] += x_pad[pics, :, stride * H_move: stride * H_move + HH, stride * W_move: stride * W_move + WW] * df dx = dx_pad[:, :, pad:pad + H, pad:pad + W] ########################################################################### # END OF YOUR CODE # ########################################################################### return dx, dw, db Max-Pooling：Native forwardinput x, (N,C,H,W) pool_param -&gt; dict ‘pool_height’ ‘pool_width’ ‘stride’ 不需要进行padding output out, (N,C,H’,W’) H’ = 1 + (H - pool_height) / stride W’ = 1 + (W - pool_width) / stride cache(x,pool_param) 实现 直接找到相应的块然后求max 注意求max的时候要注意axis,我们需要求得是在一张图片每个channel上面的最大值，在这个式子里面因为已经确定了pics的值，实际上的out其实是一个三维的数组，所以应该求axis = (1,2)上面的最大值，而不是求(2,3上面的) 代码12345678910111213141516171819202122232425262728293031323334353637383940def max_pool_forward_naive(x, pool_param): """ A naive implementation of the forward pass for a max-pooling layer. Inputs: - x: Input data, of shape (N, C, H, W) - pool_param: dictionary with the following keys: - 'pool_height': The height of each pooling region - 'pool_width': The width of each pooling region - 'stride': The distance between adjacent pooling regions No padding is necessary here. Output size is given by Returns a tuple of: - out: Output data, of shape (N, C, H', W') where H' and W' are given by H' = 1 + (H - pool_height) / stride W' = 1 + (W - pool_width) / stride - cache: (x, pool_param) """ out = None ########################################################################### # TODO: Implement the max-pooling forward pass # ########################################################################### N, C, H, W = x.shape pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride'] H_out = 1 + (H - pool_height) // stride W_out = 1 + (W - pool_width) // stride out = np.zeros((N,C,H_out,W_out)) for pics in range(N): for h_out in range(H_out): for w_out in range(W_out): out[pics,:,h_out,w_out] = np.max(x[pics,:,stride * h_out: stride* h_out + pool_height,stride* w_out: stride*w_out + pool_width],axis = (1,2)) ########################################################################### # END OF YOUR CODE # ########################################################################### cache = (x, pool_param) return out, cache Max-pooling: Native backwardinput dout, size = (N,C,W_out,W_out) cache output dx, size = (N,C,W,H) 实现 max的值在实际上是一个router，local gradient对于最大的值的地方是1，其他值的地方影响是0 需要找到x里面值等于最大值的坐标，然后把这个坐标的dx改成对应dout的值（因为链式法则应该dout * 1），其他地方的dx都是0 关于找到这个点的坐标 我用了显得很傻的方法，在x的范围里面找到这个范围里最大的坐标，用了很多圈循环 实际上可以从max的值找到原来的坐标 numpy.unravel_index(indices, dims) 结合np.argmax，返回最大值的坐标 -&gt; ind = np.unravel_index(np.argmax(a, axis=None), a.shape) 这样的话找到的是在每个max的框框里最大值的坐标，在这个框框的范围里找到这个坐标就是需要改变的地方 1234567891011121314151617181920212223242526272829303132333435363738def max_pool_backward_naive(dout, cache): """ A naive implementation of the backward pass for a max-pooling layer. Inputs: - dout: Upstream derivatives - cache: A tuple of (x, pool_param) as in the forward pass. Returns: - dx: Gradient with respect to x """ dx = None ########################################################################### # TODO: Implement the max-pooling backward pass # ########################################################################### x, pool_param = cache N, C, H, W = x.shape pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride'] _,_,H_out,W_out = dout.shape dx = np.zeros_like(x) for pics in range(N): for channels in range(C): for h_out in range(H_out): for w_out in range(W_out): # for H in range(stride * h_out, stride* h_out + pool_height): # for W in range(stride * w_out, stride * w_out + pool_width): # if x[pics,channels,H,W] == np.max(x[pics,channels,stride * h_out: stride* h_out + pool_height,stride* w_out: stride*w_out + pool_width]): # dx[pics,channels,H,W] = dout[pics,channels,h_out,w_out] ind = np.unravel_index(np.argmax(x[pics,channels,stride * h_out: stride* h_out + pool_height,stride* w_out: stride*w_out + pool_width]), (pool_height,pool_width)) dx[pics,channels,stride * h_out: stride* h_out + pool_height,stride* w_out: stride*w_out + pool_width][ind] = dout[pics,channels,h_out,w_out] # print(ind) ########################################################################### # END OF YOUR CODE # ########################################################################### return dx Fast layers 在cs231n/fast_layers.py里面直接提供了比较快版本的计算方法 The fast convolution implementation depends on a Cython extension; to compile it you need to run the following from the cs231n directory: 1python setup.py build_ext --inplace 记得重启一下jupter 12345678910111213141516171819202122232425Testing conv_forward_fast:Naive: 5.283360sFast: 0.014807sSpeedup: 356.809600xDifference: 4.926407851494105e-11Testing conv_backward_fast:Naive: 9.893734sFast: 0.015421sSpeedup: 641.578958xdx difference: 1.949764775345631e-11dw difference: 5.155328198575201e-13db difference: 3.481354613192702e-14Testing pool_forward_fast:Naive: 0.212025sfast: 0.002980sspeedup: 71.143680xdifference: 0.0Testing pool_backward_fast:Naive: 0.391351sfast: 0.012568sspeedup: 31.138711xdx difference: 0.0 可以发现fast版本conv的速度会快300倍，而pooling也会快几十倍 conv sandwich layer -&gt; 已经写好了，conv + relu + poolThree-layer ConvNetcs231n/classifiers/cnn.py,implement一个三层的CNN结构 conv - relu - 2x2 maxpool - affine1 - relu - affine2 - softmax 输入图片的minibatch为(N,C,H,W) init input_dim: (C,H,W)是每张图片长什么样子 num_filters: conv层里面filter的个数 filters_size，直接把高和宽统一成一个数字了，反正都是方形的 hidden_dim：用fc层的数量 num_classes: 最后输出的class的数量 weight_scale：初始化的时候的scale reg：L2 dtype：计算所用的datatype（如 np.float32) loss + gradient 需要初始化三层的参数，W123和b123 初始化weights（正态分布）和bias（全是0） -&gt; 注意fc和conv层的不一样 因为在loss中有帮助input的大小保持的操作，所以第二层的图片可以不考虑padding和stride的变化 W1的大小是filter的大小(F,C,HH,WW)，需要filter的数量，channel的数量，以及每个filter的大小，b1是(filter,) conv_relu之后进行了一次max pool，所以图片的大小缩小了一半 后面两个affine的大小就跟输入，hidden_num和最后的num_classes有关系了，b的大小跟输出走 注意第二个affine之后不需要relu loss用之前写好的softmax 注意需要regularization 直接用之前写好的把gradient back回去就可以了 Sanity check loss¶在建立一个新的net的时候，第一件事就应该是这个 用softmax的时候，我们希望random weight，没有reg的结果是log(C) 如果加上了reg，这个数量会轻微增加一点 overfit small data 直接用非常少的数据来训练一个新的网络，应该能在这个上面overfit 应该会产生一个非常高的训练精度和非常低的val精度 注意在loss里面的时候需要记录下来scores，我就是因为变量名写错了所以一直bug 最后训练出来的train_acc接近100%，而val_acc只有百分之20 1234567891011121314151617181920np.random.seed(231)num_train = 100small_data = &#123; 'X_train': data['X_train'][:num_train], 'y_train': data['y_train'][:num_train], 'X_val': data['X_val'], 'y_val': data['y_val'],&#125;model = ThreeLayerConvNet(weight_scale=1e-2)solver = Solver(model, small_data, num_epochs=15, batch_size=50, update_rule='adam', optim_config=&#123; 'learning_rate': 1e-3, &#125;, verbose=True, print_every=1)solver.train() 训练这个三层的网络 直接用所有数据训练这个网络，应该得到的train_acc应该在40% 最后训练了1个epoch，980次iter1(Epoch 1 / 1) train acc: 0.496000; val_acc: 0.489000 可视化第一层的filter Spatial Batch Normalization 在之前我们已经看到BN对于训练NN很有用了，根据15年的一个论文，CNN里面也可以用BN -&gt; SBN 普通的BN会接收(N,D)大小的input，并且output是同样的大小，normal的时候用data的总数N CNN里面，input为(N,C,H,W)，output大小相同（也就是和X同样尺寸） 如果用卷积得到的特征map，we expect the statistics of each feature channel to be relatively consistent both between different imagesand different locations within the same image -&gt; 所以在SBN里面，计算对每个C里面的特征计算mean和var Spatial batch normalization: forwardcs231n/layers.pyinput: x,(N,C,H,W) gamma,scale parameter (C,) beta,shift param (C,) bn_param: dict mode: train/test eps momentum running_mean running_varoutput out,(N,C,H,W) cache, back的时候需要的东西 注意，可以调用之前写的关于 batchnorm_forward 的内容，代码应该少于五行 这里需要用到多维数组的转置，需要把矩阵变成（N H W） * C的格式，然后在求完bn之后再转回去 之前fc里面使用的时候的大小是(N,D)，这样的话是在所有的N上面取平均 这里的C代替了以前的D，NHW代替了以前的N(把每张特征图看做一个特征处理（一个神经元），这里的特征图指的是一层的东西) 这里用到的是一张特征图里面的所有神经元的参数共享 Spatial batch normalization: backward 输入dout(N,C,H,W)和cache，输出dx，dgamma和dbeta 同样也是直接调用之前的，变形方法和之前一样 Group Normalization 同样原理，把维度变化之后使用 np.newaxis()]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231nassignment2之Dropout]]></title>
    <url>%2F2019%2F04%2F18%2FCS231nassignment2Dropout%2F</url>
    <content type="text"><![CDATA[Target regularization NN randomly setting some features to 0 during forward pass Geoffrey E. Hinton et al, “Improving neural networks by preventing co-adaptation of feature detectors”, arXiv 2012 Dropout forward + backwardin cs231n/layers.py IO input x,input data, of any shape dropout_params p，每个neuron是不是保留的可能性是p mode：’train’的时候会进行dropout，‘test’的时候会直接return input seed：用来generate random number for dropout output out, 和x同样大小 cache, tuple(dropout_params, mask). In training, mask is used to multiply the input 在实现中不推荐用vanilla的方法 123456NOTE: Please implement **inverted** dropout, not the vanilla version of dropout.See http://cs231n.github.io/neural-networks-2/#reg for more details.NOTE 2: Keep in mind that p is the probability of **keep** a neuronoutput; this might be contrary to some sources, where it is referred toas the probability of dropping a neuron output. 实现 在训练的时候在hidd层都drop了一部分，如果愿意的话也可以在input层就drop 在predict的时候不再drop了！但是需要根据drop的比例对output的数量进行scale -&gt; 所以这样就会变得很麻烦（vanilla的方法） 比如比例是p，drop之后剩下了px 那在test的时候x的大小也应该变成px(x -&gt; px) inverted dropout，在训练的时候就对大小进行放缩，在test的时候不接触forward pass 12H1 = np.maximum(0, np.dot(W1, X) + b1)U1 = (np.random.rand(*H1.shape) &lt; p) / p # /p!!! back的实现更容易了，如果这个点被drop了的话对再往前的dx就没有影响，如果这个点没有被drop的话对之前的影响就是常数 code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081def dropout_forward(x, dropout_param): """ Performs the forward pass for (inverted) dropout. Inputs: - x: Input data, of any shape - dropout_param: A dictionary with the following keys: - p: Dropout parameter. We keep each neuron output with probability p. - mode: 'test' or 'train'. If the mode is train, then perform dropout; if the mode is test, then just return the input. - seed: Seed for the random number generator. Passing seed makes this function deterministic, which is needed for gradient checking but not in real networks. Outputs: - out: Array of the same shape as x. - cache: tuple (dropout_param, mask). In training mode, mask is the dropout mask that was used to multiply the input; in test mode, mask is None. NOTE: Please implement **inverted** dropout, not the vanilla version of dropout. See http://cs231n.github.io/neural-networks-2/#reg for more details. NOTE 2: Keep in mind that p is the probability of **keep** a neuron output; this might be contrary to some sources, where it is referred to as the probability of dropping a neuron output. """ p, mode = dropout_param['p'], dropout_param['mode'] if 'seed' in dropout_param: np.random.seed(dropout_param['seed']) mask = None out = None if mode == 'train': ####################################################################### # TODO: Implement training phase forward pass for inverted dropout. # # Store the dropout mask in the mask variable. # ####################################################################### mask = (np.random.randn(*x.shape) &lt; p) / p out = x * mask ####################################################################### # END OF YOUR CODE # ####################################################################### elif mode == 'test': ####################################################################### # TODO: Implement the test phase forward pass for inverted dropout. # ####################################################################### out = x ####################################################################### # END OF YOUR CODE # ####################################################################### cache = (dropout_param, mask) out = out.astype(x.dtype, copy=False) return out, cachedef dropout_backward(dout, cache): """ Perform the backward pass for (inverted) dropout. Inputs: - dout: Upstream derivatives, of any shape - cache: (dropout_param, mask) from dropout_forward. """ dropout_param, mask = cache mode = dropout_param['mode'] dx = None if mode == 'train': ####################################################################### # TODO: Implement training phase backward pass for inverted dropout # ####################################################################### dx = dout * mask ####################################################################### # END OF YOUR CODE # ####################################################################### elif mode == 'test': dx = dout return dx FC with DP 应该在每层的relu之后，增加dropout的部分 在之前定义的function里面加上新的dropout部分，因为倔强的想加在定义好的函数里面，所以产生了一些奇怪的延伸问题 如果想要可选参数，在def function里面直接定义好就行了 如果返回值不需要，直接在返回的时候_就好了 注意在fc_net里面如果dropout = 1 的话，实际上的flag是没有意义的 1234567891011121314151617181920212223242526272829303132333435363738def affine_Normal_relu_dropout_forward(self, x, w, b, mode, gamma=None, beta=None, bn_params=None): Normal_cache = None dp_cache = None a, fc_cache = affine_forward(x, w, b) if mode == "batchnorm": mid, Normal_cache = batchnorm_forward(a, gamma, beta, bn_params) elif mode == "layernorm": mid, Normal_cache = layernorm_forward(a, gamma, beta, bn_params) else: mid = a dp, relu_cache = relu_forward(mid) if self.use_dropout: out, dp_cache = dropout_forward(dp, self.dropout_param) else: out = dp cache = (fc_cache, Normal_cache, relu_cache, dp_cache) return out, cachedef affine_Normal_relu_dropout_backward(self, dout, cache, mode): fc_cache, Normal_cache, relu_cache, dp_cache = cache dgamma = 0.0 dbeta = 0.0 if self.use_dropout: ddp = dropout_backward(dout, dp_cache) else: ddp = dout da = relu_backward(ddp, relu_cache) if mode == "batchnorm": dmid, dgamma, dbeta = batchnorm_backward_alt(da, Normal_cache) elif mode == "layernorm": dmid, dgamma, dbeta = layernorm_backward(da, Normal_cache) else: dmid = da dx, dw, db = affine_backward(dmid, fc_cache) return dx, dw, db, dgamma, dbeta regularization experiment 训练一个2层的网络，500个training，一个没有dropout，另一个0.25的dp 并且可视化了最终的结果 从结果上来看感觉，如果epoch比较少的话，dropout的效果会更好 加上dropout，normalization，的fc网络全部代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260class FullyConnectedNet(object): """ A fully-connected neural network with an arbitrary number of hidden layers, ReLU nonlinearities, and a softmax loss function. This will also implement dropout and batch/layer normalization as options. For a network with L layers, the architecture will be &#123;affine - [batch/layer norm] - relu - [dropout]&#125; x (L - 1) - affine - softmax where batch/layer normalization and dropout are optional, and the &#123;...&#125; block is repeated L - 1 times. Similar to the TwoLayerNet above, learnable parameters are stored in the self.params dictionary and will be learned using the Solver class. """ def __init__(self, hidden_dims, input_dim=3 * 32 * 32, num_classes=10, dropout=1, normalization=None, reg=0.0, weight_scale=1e-2, dtype=np.float32, seed=None): """ Initialize a new FullyConnectedNet. Inputs: - hidden_dims: A list of integers giving the size of each hidden layer. - input_dim: An integer giving the size of the input. - num_classes: An integer giving the number of classes to classify. - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=1 then the network should not use dropout at all. - normalization: What type of normalization the network should use. Valid values are "batchnorm", "layernorm", or None for no normalization (the default). - reg: Scalar giving L2 regularization strength. - weight_scale: Scalar giving the standard deviation for random initialization of the weights. - dtype: A numpy datatype object; all computations will be performed using this datatype. float32 is faster but less accurate, so you should use float64 for numeric gradient checking. - seed: If not None, then pass this random seed to the dropout layers. This will make the dropout layers deteriminstic so we can gradient check the model. """ self.normalization = normalization self.use_dropout = dropout != 1 self.reg = reg self.num_layers = 1 + len(hidden_dims) self.dtype = dtype self.params = &#123;&#125; ############################################################################ # TODO: Initialize the parameters of the network, storing all values in # # the self.params dictionary. Store weights and biases for the first layer # # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be # # initialized from a normal distribution centered at 0 with standard # # deviation equal to weight_scale. Biases should be initialized to zero. # # # # When using batch normalization, store scale and shift parameters for the # # first layer in gamma1 and beta1; for the second layer use gamma2 and # # beta2, etc. Scale parameters should be initialized to ones and shift # # parameters should be initialized to zeros. # ############################################################################ pr_num = input_dim # can't use enumerate beacuse I need the number more than the size of hidden_dims for layer in range(self.num_layers): layer += 1 weights = 'W' + str(layer) bias = 'b' + str(layer) # 这时候是最后一层(the last layer) if layer == self.num_layers: self.params[weights] = np.random.randn( hidden_dims[len(hidden_dims) - 1], num_classes) * weight_scale self.params[bias] = np.zeros(num_classes) # other layers else: hidd_num = hidden_dims[layer - 1] self.params[weights] = np.random.randn( pr_num, hidd_num) * weight_scale self.params[bias] = np.zeros(hidd_num) pr_num = hidd_num if self.normalization in ["batchnorm", "layernorm"]: self.params['gamma' + str(layer)] = np.ones(hidd_num) self.params['beta' + str(layer)] = np.zeros(hidd_num) # print(len(self.params)) # print(self.params) ############################################################################ # END OF YOUR CODE # ############################################################################ # When using dropout we need to pass a dropout_param dictionary to each # dropout layer so that the layer knows the dropout probability and the mode # (train / test). You can pass the same dropout_param to each dropout layer. self.dropout_param = &#123;&#125; if self.use_dropout: self.dropout_param = &#123;'mode': 'train', 'p': dropout&#125; if seed is not None: self.dropout_param['seed'] = seed # With batch normalization we need to keep track of running means and # variances, so we need to pass a special bn_param object to each batch # normalization layer. You should pass self.bn_params[0] to the forward pass # of the first batch normalization layer, self.bn_params[1] to the forward # pass of the second batch normalization layer, etc. self.bn_params = [] if self.normalization == 'batchnorm': self.bn_params = [&#123;'mode': 'train'&#125; for i in range(self.num_layers - 1)] if self.normalization in ["batchnorm", "layernorm"]: self.bn_params = [&#123;&#125; for i in range(self.num_layers - 1)] # Cast all parameters to the correct datatype for k, v in self.params.items(): self.params[k] = v.astype(dtype) def loss(self, X, y=None): """ Compute loss and gradient for the fully-connected net. Input / output: Same as TwoLayerNet above. """ X = X.astype(self.dtype) mode = 'test' if y is None else 'train' # Set train/test mode for batchnorm params and dropout param since they # behave differently during training and testing. if self.use_dropout: self.dropout_param['mode'] = mode if self.normalization == 'batchnorm': for bn_param in self.bn_params: bn_param['mode'] = mode scores = None ############################################################################ # TODO: Implement the forward pass for the fully-connected net, computing # # the class scores for X and storing them in the scores variable. # # # # When using dropout, you'll need to pass self.dropout_param to each # # dropout forward pass. # # # # When using batch normalization, you'll need to pass self.bn_params[0] to # # the forward pass for the first batch normalization layer, pass # # self.bn_params[1] to the forward pass for the second batch normalization # # layer, etc. # ############################################################################ cache = &#123;&#125; temp_out = X for i in range(self.num_layers): w = self.params['W' + str(i + 1)] b = self.params['b' + str(i + 1)] if i == self.num_layers - 1: scores, cache['cache' + str(i + 1)] = affine_forward(temp_out, w, b) else: if self.normalization in ["batchnorm", "layernorm"]: gamma = self.params['gamma' + str(i + 1)] beta = self.params['beta' + str(i + 1)] temp_out, cache['cache' + str(i + 1)] = self.affine_Normal_relu_dropout_forward( temp_out, w, b, self.normalization, gamma, beta, self.bn_params[i]) else: # temp_out, cache['cache' + # str(i + 1)] = affine_relu_forward(temp_out, w, b) temp_out, cache['cache' + str(i + 1)] = self.affine_Normal_relu_dropout_forward( temp_out, w, b, mode=self.normalization) ############################################################################ # END OF YOUR CODE # ############################################################################ # If test mode return early if mode == 'test': return scores loss, grads = 0.0, &#123;&#125; ############################################################################ # TODO: Implement the backward pass for the fully-connected net. Store the # # loss in the loss variable and gradients in the grads dictionary. Compute # # data loss using softmax, and make sure that grads[k] holds the gradients # # for self.params[k]. Don't forget to add L2 regularization! # # # # When using batch/layer normalization, you don't need to regularize the scale# # and shift parameters. # # # # NOTE: To ensure that your implementation matches ours and you pass the # # automated tests, make sure that your L2 regularization includes a factor # # of 0.5 to simplify the expression for the gradient. # ############################################################################ loss, dscores = softmax_loss(scores, y) reg_loss = 0.0 pre_dx = dscores # dgamma = self.params['gamma'] for i in reversed(range(self.num_layers)): i = i + 1 reg_loss = np.sum(np.square(self.params['W' + str(i)])) loss += reg_loss * 0.5 * self.reg # 最后一层 if i == self.num_layers: pre_dx, dw, db = affine_backward( pre_dx, cache['cache' + str(i)]) else: if self.normalization in ["batchnorm", "layernorm"]: pre_dx, dw, db, dgamma, dbeta = self.affine_Normal_relu_dropout_backward( pre_dx, cache['cache' + str(i)], self.normalization) grads['gamma' + str(i)] = dgamma grads['beta' + str(i)] = dbeta else: pre_dx, dw, db, _, _ = self.affine_Normal_relu_dropout_backward( pre_dx, cache['cache' + str(i)], self.normalization) dw += self.reg * self.params['W' + str(i)] db += self.reg * self.params['b' + str(i)] grads['W' + str(i)] = dw grads['b' + str(i)] = db ############################################################################ # END OF YOUR CODE # ############################################################################ return loss, grads def affine_Normal_relu_dropout_forward(self, x, w, b, mode, gamma=None, beta=None, bn_params=None): Normal_cache = None dp_cache = None a, fc_cache = affine_forward(x, w, b) if mode == "batchnorm": mid, Normal_cache = batchnorm_forward(a, gamma, beta, bn_params) elif mode == "layernorm": mid, Normal_cache = layernorm_forward(a, gamma, beta, bn_params) else: mid = a dp, relu_cache = relu_forward(mid) if self.use_dropout: out, dp_cache = dropout_forward(dp, self.dropout_param) else: out = dp cache = (fc_cache, Normal_cache, relu_cache, dp_cache) return out, cache def affine_Normal_relu_dropout_backward(self, dout, cache, mode): fc_cache, Normal_cache, relu_cache, dp_cache = cache dgamma = 0.0 dbeta = 0.0 if self.use_dropout: ddp = dropout_backward(dout, dp_cache) else: ddp = dout da = relu_backward(ddp, relu_cache) if mode == "batchnorm": dmid, dgamma, dbeta = batchnorm_backward_alt(da, Normal_cache) elif mode == "layernorm": dmid, dgamma, dbeta = layernorm_backward(da, Normal_cache) else: dmid = da dx, dw, db = affine_backward(dmid, fc_cache) return dx, dw, db, dgamma, dbeta]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>Drop out</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenGL笔记]]></title>
    <url>%2F2019%2F04%2F16%2FOpenGL%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Learn OpenGl on Modern OpenGL. -&gt; 从graphics的programming开始讲的 Getting StartOPENGL 是一个进行图像处理的工具 可以被认为是API，但是实际上是specification 明确说明了每个function应该的输入和输出，以及如何perform 用户在用这个说明来解决问题，因为没有给出明确的implement的过程，所以只要结果符合规则，怎么implement都可以 Core-profile vs Immediate mode 以前的版本用的是immediate mode 比较好用来画图 具体的是实现都在lib里面，developer不是很好的能看到如何计算 效率越来越低 Core-profile 在3.2版本之后改成了这个 强制使用modern practices，如果想要用被分出去的function就会直接报错 效率高，更灵活，更难学 extensions 支持extensions，只要检查支不支持graphic card就可以知道能不能用 可以直接用比较新的东西，不用等着OPENGL更新新的功能 需要在用之前判断他是不是available的，如果不是需要用原来的方法搞 State Machine OpenGL自己就是一个State Machine：一个var的集合，来判断他现在应该如何操作 state -&gt; context 改变state：设定一些options，操作一些buffer，在现在的context来render 例子： 如果我想画三角形，而不是画线了，就改变draw的state 只要这个改变传达到了，下一条线就画的是三角形了 state-changing用来改变context，state-using在现在的state上面开始进行操作 Objects 一个集合来表现OpenGL的subset的state 比如可以用一个object来表示对window的设定，可以设置大小，设置支持的颜色等等123456// The State of OpenGLstruct OpenGL_Context &#123; ... object_name* object_Window_Target; ... &#125;; 12345678910// create objectunsigned int objectId = 0;glGenObject(1, &amp;objectId);// bind object to contextglBindObject(GL_WINDOW_TARGET, objectId);// set options of object currently bound to GL_WINDOW_TARGETglSetObjectOption(GL_WINDOW_TARGET, GL_OPTION_WINDOW_WIDTH, 800);glSetObjectOption(GL_WINDOW_TARGET, GL_OPTION_WINDOW_HEIGHT, 600);// set context target back to defaultglBindObject(GL_WINDOW_TARGET, 0); 流程 首先创建了一个object，里面存了一个ref是这个object的id 然后把这个object和context的目标位置bind在了一起 设置了这些window的参数 最后un-bind这两个东西，把window target改回原来的值 这样的话我们可以创建很多object，提前设置好里面的量，等到需要用的时候就直接bind就可以用了 比如我们有一堆object包含了小人，小马，小鹿 想画哪个就把哪个绑定到draw里面，就可以直接画出来了 Crateing a window因为操作系统的问题，所有操作系统上面不是很一样。但是已经有一些提供这些功能的函数了，这里用的是GLFW GLFW 一个lib，用C写的，主要目的是提供把东西渲染到屏幕的功能 可以创建一个context，定义窗口的params，处理用户的输入 已经一口气配置好了这些！https://www.jianshu.com/p/25d5fbf792a2记得在link lib里面把openGL的framework加进去！！！！！ GLAD 因为openGL还需要不同版本的driver的支持，需要有东西来处理这部分的内容 和其他的东西不同，GLAD用的是web service 在这个网页上选择好语言，版本号，确保profile是core，然后生成 直接下载下来对应的zip，然后把include放进include里面，.c文件放在project里面 莫名其妙并不需要这一步，神奇，可能是我在include里面已经搞进来了！！ Hello Window初始化12345678910int main()&#123; glfwInit(); glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3); glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3); glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE); //glfwWindowHint(GLFW_OPENGL_FORWARD_COMPAT, GL_TRUE); return 0;&#125; 首先进行了初始化 然后configure了GLFW，设置了a large enum of possible options prefixed with GLFW_. （第三行就是最小） -&gt; 大概是设置要用GLFW的版本号 然后也告诉了他想用core 然后需要使用glfwCreateWindow这个函数，来创建这个GLFWwindow* window的变量 创建的函数需要窗口的长宽 窗口名 创建完之后就可以把这个窗口设置成glfwMakeContextCurrent(window);也就是说设置成了现在的thread里面12345678GLFWwindow* window = glfwCreateWindow(800, 600, "LearnOpenGL", NULL, NULL);if (window == NULL)&#123; std::cout &lt;&lt; "Failed to create GLFW window" &lt;&lt; std::endl; glfwTerminate(); return -1;&#125;glfwMakeContextCurrent(window); GLAD GLAD是为OpenGL来管理这些函数的，在使用这些函数之前需要初始化GLAD12345if (!gladLoadGLLoader((GLADloadproc)glfwGetProcAddress))&#123; std::cout &lt;&lt; "Failed to initialize GLAD" &lt;&lt; std::endl; return -1;&#125; viewpoint 在开始render之前我们还需要告诉GL渲染窗口的大小，用到了glViewport这个函数 前面两个参数定义了这个窗口左下角的坐标 后面两个参数定义了需要render的窗口的大小 每次调整window的大小的时候viewport也需要被调整 engines 我们希望这个engine可以一直持续画图，直到最后我们告诉这个窗口要关闭，所以要建立一个循环 12345while(!glfwWindowShouldClose(window))&#123; glfwSwapBuffers(window); glfwPollEvents(); &#125; 在这个循环里面，pollevent是来检查是不是有trigger进来的事情（比如键盘输入），更新窗口的状态，并且call相应的函数 swapbuffer，会交换color buffer（包括每个像素点颜色的buffer），然后show在窗口里面 last thing glfwTerminate退出这个循环之后，需要清除这些相关的资源，用这个函数放在最底下 input 需要一些键盘上的操作来调整的时候，写了一个processInput的函数 比如下面这个函数就是检测了有没有按下去esc，如果按了的话就关闭窗口 写完之后把这个函数在while循环里面调用12345void processInput(GLFWwindow *window)&#123; if(glfwGetKey(window, GLFW_KEY_ESCAPE) == GLFW_PRESS) glfwSetWindowShouldClose(window, true);&#125; rendering 希望在一个loop里面放上去所有的rendering的命令，整个循环看起来应该是这个样子的12345678910111213// render loopwhile(!glfwWindowShouldClose(window))&#123; // input processInput(window); // rendering commands here ... // check and call events and swap the buffers glfwPollEvents(); glfwSwapBuffers(window);&#125; hello triangleopengl里面所有东西都是在3D的空间里的，但是屏幕上显示的东西是2D的。整个这个转换的过程叫做graphics pipeline，可以分成两个步骤：第一个是把物体的3D坐标转化成2D的坐标，第二个是把2D的坐标转化成pixel上面的具体值 pipeline 所有的转化步骤都可以parallel的进行，现在的显卡有很多小的core来进行 -&gt; shaders 在最开始的时候pass进去了一个list的3D坐标 （Vertex Data） 第一步：vertex shader 把3D的坐标转化成不同的3D坐标（相当于把数据转化成点？） primitive assembly 从上一步得到的左右的点得到输入 然后形成一个基本的图形 geometry shader 根据新给的点，形成新的不同的形状，比如在例子里面形成了新的一条线 rasterization stage 把上面得到的primitives map到最后的屏幕上面的相应的pixel上面 Clipping 这一步丢掉了所有在视线外面的fragments，提升性能 fragment shader 计算这个pixel最后的颜色，会在这一步计算光影，以及光线的颜色等等东西 当每个像素的颜色决定了以后，这个object会被送到alpha test和blending 这一步会测试深度原因，判断fragment是在物体的前面还是后面 还会考虑透明度的问题虽然上面的东西很复杂，但是在实际应用的时候只需要要考虑vertex和fragment shader vertex input openGL是3D的东西，所有的点设置input的时候都需要设置三维的坐标 xyz 只有在坐标在 -1 到 1 中间的时候，才会处理这些坐标，这个范围里面的数字是根据屏幕的比例得出来的normalized device coordinates 比如在这个例子里面，需要的渲染一个三角形，那么需要这个三角形的三个点的坐标。注意这个例子里面根本没有考虑深度，而是直接画在了平面上面 12345float vertices[] = &#123; -0.5f, -0.5f, 0.0f, 0.5f, -0.5f, 0.0f, 0.0f, 0.5f, 0.0f&#125;; 在定义好坐标之后需要把这个东西放进vertex shader里面，需要在GPU里面创建一部分内存来存储这个数据，并且需要在GPU里面存储大量的数据（这样不用每次都送了） 每个部分的object都会有一个自己的buffer id，可以通过下面的方法生成一个id。也可以把一串array绑到这个id上面12unsigned int VBO;glGenBuffers(1, &amp;VBO);]]></content>
      <categories>
        <category>OpenGl</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment2之Batch Normalization]]></title>
    <url>%2F2019%2F04%2F15%2FCS231Nassignment2BN%2F</url>
    <content type="text"><![CDATA[target 之前的内容讲了lr的优化方法，比如Adam，另一种方法是根据改变网络的结构，make it easy to train -&gt; batch normalization 想去掉一些uncorrelated features(不相关的特征)，可以在训练数据之前preprocess，变成0-centered分布，这样第一层是没有问题的，但是后面的层里还是会出问题 所以把normalization的部分加入了DN里面，加入了一个BN层，会估计mean和standard deviation of each feature，这样重新centre和normalized learnable shift and scale parameters for each feature dimension 核心思想：粗暴的用BN来解决weights初始化的问题 ref：https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html Batch normalization: forward这个东西的要义就是NN里面的一层，不对维度改变，但是会改变这些值的分布 首先setup，并且载入好了preprocess的数据cs231n/layers.py -&gt; batchnorm_forward keep exp decay 来运行mean &amp; variance of each feature -&gt; 在test的时候去normalize data test-time: 计算sample mean和varience的时候用大量的训练数据而不是用所有图片的平均值，但是在作业里面用的是平均值，因为可以省去一步estimate（torch7 也用的是平均值） 12running_mean = momentum * running_mean + (1 - momentum) * sample_meanrunning_var = momentum * running_var + (1 - momentum) * sample_var I/O input x，data(N,D) gamma：scale parameter(D,) beta：shift parameter(D,) bn_param: 一个dict mode：‘train’ or ‘test’ eps：为了数字上的稳定性的一个常数 momentum：在计算mean和variance上面的一个常数 running mean：(D,)，是running mean running var：(D,) output out：(N,D) cache:在back的时候用 todo 用minibatch的统计来计算mean和variance，用这两个值把data normalize，并且用gamma和beta拉伸这个值，以及shift这些值的位置 在分布的上面，虽然求得是running variance，但是需要normalize的时候考虑的是standard（也就是平方根） implement 其实是和如何计算息息相关的，知道输入，求这个玩意的normal的步骤如下（其中的x就是这个minibatch的全部数据） 求mu，也就是x的mean（注意这里要对列求mean，也就是把所有图片的像素均匀分布，最后得到的结果是D个不是N个） 求var，知道这个东西，可以直接用 np.var(x, axis = 0)来求方差 求normalize： x - x.mean / np.sqrt(x.var + eps) 其中刚开始求出来的var就是方差，也就是标准差的平方 eps是偏差值，这个值加上方差开方是标准差 scale和shift，乘scale的系数，加shift的系数 最后需要计算什么cache和back的推导息息相关 Batch normalization: backward 可以直接画出来计算normal的路径，然后根据这个路径back 要义就是一步一步的求导！一步一步的链式法则 注意的就是求mean回来的导数，理解上来说就是这个矩阵在求导的过程中升维了，从(D,)变成了(N,D)，而在最开始求得时候所有的数字的贡献都是1，所以往回走的时候乘一个（N，D）的全是1的矩阵，并且1/N的常数还在 Batch normalization: alternative backward 在sigmoid的back的过程中有两种不同的方法 一种是写出来整体计算的图（拆分成各种小的计算），然后根据这张图的再back回去 另一种是在纸上先简化了整体的计算过程，然后再直接实现，这样代码会比较简单 ref:https://kevinzakka.github.io/2016/09/14/batch_normalization/ 最终目标 f: BN之后的整体输出结果 y：对normal之后的线性变换（gamma + beta） x’：normal的input mu：batch mean varbatch vatiance 需要求 df/dx,df/dgamma,df/dbeta -&gt; 最终结果整体速度比以前快了x2.5左右，这一步的主要目的就是用来提速的 可以把整体的计算分为以下的三个步骤 这三部分的代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208def batchnorm_forward(x, gamma, beta, bn_param): """ Forward pass for batch normalization. During training the sample mean and (uncorrected) sample variance are computed from minibatch statistics and used to normalize the incoming data. During training we also keep an exponentially decaying running mean of the mean and variance of each feature, and these averages are used to normalize data at test-time. At each timestep we update the running averages for mean and variance using an exponential decay based on the momentum parameter: running_mean = momentum * running_mean + (1 - momentum) * sample_mean running_var = momentum * running_var + (1 - momentum) * sample_var Note that the batch normalization paper suggests a different test-time behavior: they compute sample mean and variance for each feature using a large number of training images rather than using a running average. For this implementation we have chosen to use running averages instead since they do not require an additional estimation step; the torch7 implementation of batch normalization also uses running averages. Input: - x: Data of shape (N, D) - gamma: Scale parameter of shape (D,) - beta: Shift paremeter of shape (D,) - bn_param: Dictionary with the following keys: - mode: 'train' or 'test'; required - eps: Constant for numeric stability - momentum: Constant for running mean / variance. - running_mean: Array of shape (D,) giving running mean of features - running_var Array of shape (D,) giving running variance of features Returns a tuple of: - out: of shape (N, D) - cache: A tuple of values needed in the backward pass """ mode = bn_param['mode'] eps = bn_param.get('eps', 1e-5) momentum = bn_param.get('momentum', 0.9) N, D = x.shape running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype)) running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype)) out, cache = None, None if mode == 'train': ####################################################################### # TODO: Implement the training-time forward pass for batch norm. # # Use minibatch statistics to compute the mean and variance, use # # these statistics to normalize the incoming data, and scale and # # shift the normalized data using gamma and beta. # # # # You should store the output in the variable out. Any intermediates # # that you need for the backward pass should be stored in the cache # # variable. # # # # You should also use your computed sample mean and variance together # # with the momentum variable to update the running mean and running # # variance, storing your result in the running_mean and running_var # # variables. # # # # Note that though you should be keeping track of the running # # variance, you should normalize the data based on the standard # # deviation (square root of variance) instead! # # Referencing the original paper (https://arxiv.org/abs/1502.03167) # # might prove to be helpful. # ####################################################################### mean = np.mean(x, axis=0) xmu = x - mean sq = np.square(xmu) var = np.var(x, axis=0) sqrtvar = np.sqrt(var + eps) ivar = 1. / sqrtvar normalize_raw = xmu * ivar normalize_result = gamma * normalize_raw + beta out = normalize_result running_mean = momentum * running_mean + \ (1 - momentum) * mean running_var = momentum * running_var + (1 - momentum) * var cache = (normalize_raw, gamma, xmu, ivar, sqrtvar, var, eps) ####################################################################### # END OF YOUR CODE # ####################################################################### elif mode == 'test': ####################################################################### # TODO: Implement the test-time forward pass for batch normalization. # # Use the running mean and variance to normalize the incoming data, # # then scale and shift the normalized data using gamma and beta. # # Store the result in the out variable. # ####################################################################### x_normalize = (x - running_mean) / (np.sqrt(running_var + eps)) out = x_normalize * gamma + beta ####################################################################### # END OF YOUR CODE # ####################################################################### else: raise ValueError('Invalid forward batchnorm mode "%s"' % mode) # Store the updated running means back into bn_param bn_param['running_mean'] = running_mean bn_param['running_var'] = running_var return out, cachedef batchnorm_backward(dout, cache): """ Backward pass for batch normalization. For this implementation, you should write out a computation graph for batch normalization on paper and propagate gradients backward through intermediate nodes. Inputs: - dout: Upstream derivatives, of shape (N, D) - cache: Variable of intermediates from batchnorm_forward. Returns a tuple of: - dx: Gradient with respect to inputs x, of shape (N, D) - dgamma: Gradient with respect to scale parameter gamma, of shape (D,) - dbeta: Gradient with respect to shift parameter beta, of shape (D,) """ dx, dgamma, dbeta = None, None, None ########################################################################### # TODO: Implement the backward pass for batch normalization. Store the # # results in the dx, dgamma, and dbeta variables. # # Referencing the original paper (https://arxiv.org/abs/1502.03167) # # might prove to be helpful. # ########################################################################### normalize_raw, gamma, xmu, ivar, sqrtvar, var, eps = cache N, D = dout.shape dbeta = np.sum(dout, axis=0) dgammax = dout dgamma = np.sum(dgammax * normalize_raw, axis=0) dnormalize_raw = dgammax * gamma divar = np.sum(dnormalize_raw * xmu, axis=0) dxmu = dnormalize_raw * ivar dsqrtvar = -1. / (sqrtvar ** 2) * divar dvar = 0.5 * 1. / np.sqrt(var + eps) * dsqrtvar dsq = 1. / N * np.ones((N, D)) * dvar dxmu2 = 2 * xmu * dsq dx1 = (dxmu + dxmu2) dmu = -1 * np.sum(dxmu + dxmu2, axis=0) dx2 = 1. / N * np.ones((N, D)) * dmu dx = dx1 + dx2 ########################################################################### # END OF YOUR CODE # ########################################################################### return dx, dgamma, dbetadef batchnorm_backward_alt(dout, cache): """ Alternative backward pass for batch normalization. For this implementation you should work out the derivatives for the batch normalizaton backward pass on paper and simplify as much as possible. You should be able to derive a simple expression for the backward pass. See the jupyter notebook for more hints. Note: This implementation should expect to receive the same cache variable as batchnorm_backward, but might not use all of the values in the cache. Inputs / outputs: Same as batchnorm_backward """ dx, dgamma, dbeta = None, None, None ########################################################################### # TODO: Implement the backward pass for batch normalization. Store the # # results in the dx, dgamma, and dbeta variables. # # # # After computing the gradient with respect to the centered inputs, you # # should be able to compute gradients with respect to the inputs in a # # single statement; our implementation fits on a single 80-character line.# ########################################################################### normalize_raw, gamma, xmu, ivar, sqrtvar, var, eps = cache N, D = dout.shape dbeta = np.sum(dout, axis=0) dgamma = np.sum(dout * normalize_raw, axis=0) # intermediate partial derivatives dxhat = dout * gamma # final partial derivatives dx = (1. / N) * ivar * (N * dxhat - np.sum(dxhat, axis=0) - normalize_raw * np.sum(dxhat * normalize_raw, axis=0)) ########################################################################### # END OF YOUR CODE # ########################################################################### return dx, dgamma, dbeta Fully Connected Nets with Batch Normalizationin cs231n/classifiers/fc_net.py, add the BN layers into the net. 应该在每个relu之前加上BN，所以在这里不能直接用之前的affine，relu的过程，因为中间又插了一个新的BN层，所以要写一个新的function 最后一层之后的输出不应该BN(应该是涉及到循环的问题) 实现中遇到的问题 self.bn_params的参数类型不是dict而是list，代表的是所有层里面的参数的所有和，当进入到每层的时候具体对应的才是这里的dict 当把affine_BN_relu结合在一起的时候，注意最后一层输出的地方没有BN，所以没有他的cache，需要分开讨论，不然cache的数量不对 注意这个fc_net的class因为需要实现多种不同的功能，所以对于是不是BN要加上条件判断 确实非常像搭乐高了！！ 这里主要，写到这才发现最后一层的时候好像是不需要relu也不需要batchnorm 定义好的函数块123456789101112131415def affine_BN_relu_forward(self, x, w, b, gamma, beta, bn_params): a, fc_cache = affine_forward(x, w, b) mid, BN_cache = batchnorm_forward(a, gamma, beta, bn_params) out, relu_cache = relu_forward(mid) cache = (fc_cache, BN_cache, relu_cache) return out, cache def affine_BN_relu_backward(self, dout, cache): fc_cache, BN_cache, relu_cache = cache da = relu_backward(dout, relu_cache) dmin, dgamma, dbeta = batchnorm_backward_alt(da, BN_cache) dx, dw, db = affine_backward(dmin, fc_cache) return dx, dw, db, dgamma, dbeta 结论 可视化之后可以发现加了norm的话好像会下降的快一点 Batch normalization and initialization 进行试验，了解BN和weight initialization的关系 训练一个八层的网络，包括和不包括BN，用不同的weight initialization plot出来train acc, val_acc,train_loss和weight initialization的关系 BN的作用从图中可以看出来，有了BN以后，weight init对最终结果的影响明显会降低： weight的初始化对最终结果影响很严重，比如如果全是0的话，得到的所有neuron的功能都是一样的 BN其实就是在实际中解决weight init的办法，这样可以减少初始化参数的影响 核心思想就是如果你需要更好的分布，你就加一层让他变成更好的分布 在计算的过程中越乘越小（或者越大），所以计算出来的结果越来越接近0 所以这时候如果把一些input重新分布了，就会减少这个接近0的可能性 Batch normalization and batch size 试验验证BN和batch size的关系 训练6-layer的网络，分别with和without BN，使用不同的batch size By increasing batch size your steps can be more accurate because your sampling will be closer to the real population. If you increase the size of batch, your batch normalisation can have better results. The reason is exactly like the input layer. The samples will be closer to the population for inner activations. Layer Normalization（LN） 前面的所有的BN已经可以让Net更好的被训练了，但是BN的大小和batch的大小有关，所以在实际应用的时候会受到一些限制 在复杂的网络里面的时候，batch_size是被硬件机能限制的 每个minibatch的数据分布可能会比较接近，所以训练之前要shuffle，否则结果会差很多 其中一种解决的方法就是layer normalization 不是在batch上面normal 在layer上面normal each feature vector corresponding to a single datapoint is normalized based on the sum of all terms within that feature vector. Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. “Layer Normalization.” stat 1050 (2016): 21. LN 综合一层的所有维度的输入，计算该层的平均输入和平均方差 然后用同一个规范化操作转换各个维度的输入 相当于以前我们希望可以正则到这个minibatch里面的大家都差不多，现在我们不管batch了，而是调整到一张图片里面的所有数据都是normal的 implementcs231n/layers.py -&gt; layernorm_backward forward + back input x, (N,D) gamma, scale beta,shift ln_params: eps output output,(N,D) cache 实现方法 -&gt; 实际上就是从对一列的操作变成了对一行的操作 比如之前对x取mean就是求每列的mean，现在变成了取每行的mean 在所有normal之后并且scale之前，把这个矩阵在tranpose回来 back 把需要参与计算的东西都tranpose 然后把计算完的dx tranpose回来 fc_nets在fc_nets里面稍加改动，在normalization里面增加BN_NORM和Layer_NORM的选项就可以了，整体改动不大123456789101112131415161718192021def affine_Normal_relu_forward(self, x, w, b, gamma, beta, bn_params, mode): a, fc_cache = affine_forward(x, w, b) if mode == "batchnorm": mid, Normal_cache = batchnorm_forward(a, gamma, beta, bn_params) elif mode == "layernorm": mid, Normal_cache = layernorm_forward(a, gamma, beta, bn_params) out, relu_cache = relu_forward(mid) cache = (fc_cache, Normal_cache, relu_cache) return out, cache def affine_Normal_relu_backward(self, dout, cache, mode): fc_cache, Normal_cache, relu_cache = cache da = relu_backward(dout, relu_cache) if mode == "batchnorm": dmid, dgamma, dbeta = batchnorm_backward_alt(da, Normal_cache) elif mode == "layernorm": dmid, dgamma, dbeta = layernorm_backward(da, Normal_cache) dx, dw, db = affine_backward(dmid, fc_cache) return dx, dw, db, dgamma, dbeta 可以从图像看出来，layernorm中，batchsize的影响变小了]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>Batch Normalization</tag>
        <tag>Layer Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CppPrimer笔记]]></title>
    <url>%2F2019%2F04%2F15%2FCppPrimer%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第一部分 Basics C++在编译的时候就会检查类型 allow programmers to define types that include operations as well as data 第二章 varibles and basic type2.1 bulid-in基础类型包括算数类型（arithmtic type）和void类型，void用于没有返回值的函数 2.1.1 算数类型include：intergal（bool&amp; char）/ float 不同的长度的叫法不同： char -&gt; wchar_t, char16_t, char32_t(后面两个for unicode，自然语言里面) int -&gt; short, long, long long float -&gt; double, long double 扩展：在储存信息方面，储存在每个byte里面，是最小的计量单位（8bits）。在内存中，每个byte拥有一个address，需要知道地址和类型才知道到底存储的是什么东西。 signed &amp; unsigned（除了bool）： signed：包括0的正数和负数 unsigned：大于等于零 没有写unsigned的话就是signed的 对于char来说，signed，unsigined和char三种 char到底算不算unsigned显示取决于编译器 unsigned从0-255 signed一般是-128 ～ 127 关于到底怎么用： 当确定不可能是负值的时候用unisgned short太短了，long太长了，平常用int，int不够用long long 如果用char，为了考虑到不同编译的结果不同，确定好用signed还是unsigned 用double，float的精度不够，long double小号内存 2.1.2 类型转换（conversions）在把一个东西加到另一个的过程中，支持自动的类型转换 当bool被赋值为非0时，为true，0时为false 给int赋值double，自动变成整数；给doubl赋值int，变成整数后面.0 当unsigned超出界限时，the result is the remainder of the value modulo the number of values the target type can hold. 当signed超出界限的时候，结果是undefined。不知道会发生什么事。 在写代码的时候尽量避免没有定义的，或者在implement中才定义的行为，因为这样容易导致在这个电脑上行得通但是换个地方没准就行不通了 一些引起的问题： int和unsigned相加，会引起wrap around 两个unsigned相减如果小于零会出问题 在for循环的条件里，unsiged作为变量的话永远不会小于零 求求你了反正不要混着用 2.1.3 literals每个literal都有一个type，这是由form和value决定的。 整数类型的 0开头的整数是八进制，0x是16进制 负号不是literal的一部分，比如 -42，42是literal，负号是operator float类型的 十进制带小数点的数字 用exponent，E或者e，e后面的东西就是十的多少次方 character string的类型是一个char的array，complier会在每个string 后面加上‘\0’（null character） escape -&gt; 一些带有奇怪意义的\n，\t,\’等等 如果在\后面跟着多于三个数字，只会读取前三个 \x会读取跟在她后面的所有hex digits 单独改变一个literal，在数字后面加后缀suffix U：unsigned type L：long ULL：unsigned long long 但是如果你要给1024后面加个f就不行，因为1024是整形（这时候又开始怀念python） bool 2.2 变量 Variables变量提供的是：命名好的存储空间，程序可以对他执行，每个变量都有type（其实也就是class或者object吗） 2.2.1 变量的定义 assignment(赋值)和initialize(初始化)在c++里面是不一样的，初始化是在创建的时候赋予的值，赋值是之后改变这个变量的值 list的初始化：尖括号。 在使用bulit-in的时候，可能无法list初始化这个变量，因为会丢失一些信息。比如把一个double扔到int里面 是否可以不初始化就使用取决于class的定义。比如要是int没有初始化的时候是0，string没有初始化的时候是空字符串 2.2.2 变量declaration（声明？）和定义 在分开编写代码的时候，需要知道调用的函数从哪里来。注意：不初始化变量容易出问题，建议初始化每个bulit-in declaration：让程序知道函数的名字 在前面加上extern，就可以declare但是不define 但是如果已经初始化了函数，就不能加extern了，会引起错误 在其他函数的地方调用的时候（use a varible in multiple files），不需要再define了，但是需要声明 defination：创建相应的实体 除了干declaration的事情，他还分配内存，或者提供初始值 变量被define一次，但是可以被declaration无数次。 2.2.3 identifiers（定义的名字） 要求 数字，字母，下划线underscore 对大小写有区分 不能使用C++的关键词 不能含有两个相连的下划线 2.2.4 名字的scope 使用的意义：同一个名字可能会在程序的其他地方被使用，所以要用scope确定这个名字在哪个范围里面有意义（不是namespace啊啊啊啊啊竟然是大括号我震惊） nested scope 在外层被定义的名字可以在内层被重新定义 温情建议：局部变量和全局变量不要使用一个名字 2.3 compound types（有范围的类型？）就是定义在其他类型之上的类型。在c++里面有两个，pointer和reference 2.3.1 reference（lvalue reference）在创建的时候，copy的不是对象的值，而是把refer和值绑在了一起，在创建之后不能再和别的东西绑在一起。reference必须初始化。 不是对象，是一个已经存在的对象的另一个名字 给refer赋值的时候，实际上是赋值给refer所绑定的对象 当给一个refer赋值另一个refer的时候，其实是绑到了同一个对象（但是不应该这么定义） 定义 在refer的名字之前加上&amp;，但是在后续使用的时候可以不带了 refer只能初始化成一个对象，不能是一个具体的值 类型要正确 2.3.2 pointer和refer不同，指针是一个object，他们可以被assign或者copy，在定义的时候不必须初始化，一个指针可以指向不同的东西。在定义的时候用 * 来表示 取址 pointer可以得到另一个对象的address。&amp;也可以作为取址符号得到一个对象的地址（和refer不一样！！） 类型必须匹配 pointer的值（可以是以下四个之一） 指向一个对象 指向一个在对象中末尾的位置（没有使用的实际意义） null指针，表示还没有和其他的绑定 无效的？？如果是无效的话是不能访问的 访问对象 当一个pointer指向对象的时候，使用dereference（ ）来得到她的值。（pointer p是一个地址， p是他这个地址上的值 空指针NULL 使用nullptr定义 assign一个int变量给pointer是非法的，即使这个数是0（赋值的时候给的是变量的地址，带&amp;的） 真诚建议：初始化所有的pointer，没有初始化的很难分辨出来到底这个地址是有效还是无效 assignment 写成 pi = &amp;val的时候，改变的是pi的值，他指向了val 写成 * pi = 0 的时候，改变的是val的值，val变成了0 void* Pointers void* 是一个很牛逼的指针，可以hold所有对象的地址 作用：可以传到函数或者作为函数的返回值，可以和其他指针比较，可以赋值给另一个void* 指针，但是不能操控对象的地址 2.3.3 理解 定义多个变量 虽然在定义指针的时候可以加空格，但是 int* p1，p2之中，p1是指针，p2是int pointer到pointer 写成一串星号可以表示从pointer到pointer refer到pointer &amp;r可以定义成一个pointer（指针写在=右边） 2.4 修饰词 const 作用：希望定义一个variable，value不可以被改变，这时候就用上了const。在创建的时候必须初始化 在实际操作的时候，到底是不是const对数值没有影响，可以用非const来初始化const或者用const来初始化其他的 在创建之后，编译的时候所有的变量名都会换成变量的值 const对每个file来说是local的 如果希望定义一个在所有file里面都可以用的，然后在其他使用的时候声明，这时候用extern const（在定义和后续声明的时候都需要使用） 2.4.1 refer to const 可以refer到一个const，但是之后就不能通过refer来改变变量的值，也不能把一个const的变量赋值给一个非const的refer const refer的意思是这个refer不能被用来改变变量，但是被绑的变量本身可以改变。比如const int &amp;r2 = i，这时候改变i是合法的 2.4.2 pointer and const pointer to const不能被用于改变指向的东西 但是pointer是const的和变量自己改不改没关系。变量是const的话指针必须是const的 const pointer 指针本身就是一个对象，所以指针自己也可以是const的 必须被初始化，一旦初始化了，他的内容（所指向的地址）就不能改变了。 定义的时候用 int * const cpr = &amp;num （const的位置改变了） 但是可以用const pointer来改变所指向东西的值！！！只是这两个东西绑定了不能改了而已 2.4.3 top-level 可以分开考虑pointer和对象 top-level：pointer自己是一个const -&gt; 本身就是const的，可以出现在任何的对象里面 low-level：指向一个const -&gt; 只出现在refer和pointer里面 当copy一个对象的时候，top-level是被忽略的常量指针就是一个常量，不能把常量给普通但是可以把普通给常量 2.4.4 constexpr 常量表达式是编译的时候不能改变的，const object或者literal都是常量表达式。只有在初始化的时候知道了的值才是常量表达式。如果是个const int但是不确定到底是什么，那么就还不算 在前面加上 constexpr，这时候只有当后面的变量是常量的时候才能用 可以在compile的时候判定 当这个类型不是literal的时候，不能定义成常量表达式（literal包括算数，pointer和refer） 在函数内定义的变量一般不会储存在固定的地址，所以这时候指针不能是constexpr（6.1.1） 当使用constexpr的时候，作用在的是指针上而不是指向的东西上1const int *p = nullptr; // p is a pointer to a const int constexpr int *q = nullptr; // q is a const pointer to int 2.5 types 类型2.5.1 type aliases 即为对另一个type的化名 -&gt; 简化比较复杂的type，更好使用 定义方法1： 12typedef double wages; // wages is a synonym for doubletypedef wages base, *p; // base is a synonym for double, p for double* 方法2: using SI = Sales_item； 和pointer以及const -&gt; 用的时候直接替代会出问题123typedef char *pstring;const pstring cstr = 0; // cstr is a constant pointer to charconst pstring *ps; // ps is a pointer to a constant pointer to char 2.5.2 auto 作用：有的时候没法定义变量的type，这时候可以用auto，编译的时候会自动指出变量的类型（从初始化的结果推断出来的） 写成一行定义的时候，auto不能包括不同的类型（如int和double） 指针refer，const和auto 当用auto然后用一个refer初始化的时候，得到的结果是refer绑定的object 如果需要auto之后的结果还是const的，需要在auto前面加上const1234const int ci = i, &amp;cr = ci;auto b = ci; // b is an int (top-level const in ci is dropped)auto c = cr; // c is an int (cr is an alias for ci whose const is top-level) autod=&amp;i; // d isan int*(&amp; ofan int objectis int*)auto e = &amp;ci; // e is const int*(&amp; of a const object is low-level const) 2.5.3 decltype 作用：从expr里面推断出来type，但是不用这个expr来初始化的时候。这时候用decltype(fun())，这时候fun用来判断变量的类型，但是不call 如果是必须初始化的东西（比如pointer或者refer）必须初始化 decltype(* p) is int&amp;, not plain int decltype((variable))肯定是一个refer，但是decltype(variable)只有当variable是refer的时候才是 第三章 string，vector，array第二章说的是c++里面的built-in类型，除此之外还有很多library的类型（标准库），定义了很多高于计算机内部直接访问的数字和字母的类型。 3.1 namespace声明 每次都声明函数的namespace比较麻烦，可以在所有的开始之前用using namespace :: name来声明使用的特定的函数的namespace。或者直接用他代表所有的。 头文件里面不应该用using，因为include的时候就加到所有的东西里面了，那就没有意义了 3.2 stringstring定义在std的namespace里面 3.2.1 定义和初始化 一个不知道的初始化方法： string s(n,’b’)，输出结果是n个b 使用s(“hiya”)和s=”hiya”一个是direct的初始化方法，另一个是copy的初始化方法。比较容易读的方法是创建： string s = string(10,’b’) 3.2.2 string的操作 string的读和写，cout和cin（iostream库） 当键盘有输入的时候，while(cin &gt;&gt; word)这种感觉的东西当条件，cin是会识别空格然后分开的！！！ getline()可以读取一整行，存在第二个参数里面，并且帮忙跳到新的一行 .empty()和.size()可以判定是否为空，以及string里面的char的数量 string:: size_type size的返回值的类型是size_type 注意：因为返回值类型不同，所以当比较size的时候，如果和一个int的负数比较，int会被转换成unisgned的一个巨大的数字，从而导致比较的失败 -&gt; 所以在使用size的语句里面，不用int比较好（亲身证明确实如此，换成double就没事了） 比较字符串 == 或者 ！= 来比较两个是否相等，需要是相同的长度且包括相同的字母 比较两个的大小时 如果长度不同，如果短的每个的字母都和长的相同，那短的比较小 如果任何一位上面的char不同，那就是第一个不同的char比较的结果 add 字符串可以直接相加（指s1+s2） 可以把string和literal混着加，但是两个带引号的不能连在一起直接加（这是什么脑残规则） 练习 可以直接通过索引vector的方法索引string里面的char（但是一个词一个词读取直接用cin&gt;&gt; s也是可以的（我是傻逼吗）） 二择的判断条件可以写成 ((str1.size() &gt; str2.size()) ? str1 : str2) 3.2.3 string里面的chars 有时候需要处理每一个字母，有的时候需要处理特殊的一些字母，定义在函数cctype里面]]></content>
      <categories>
        <category>编程语言</category>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于numpy里面random.rand和randn的区别]]></title>
    <url>%2F2019%2F04%2F11%2F%E5%85%B3%E4%BA%8Erandomrand%E5%92%8Crandn%2F</url>
    <content type="text"><![CDATA[python里面常用的两个产生随机数的函数，两个不太一样 其中 np.random.rand()是用来产生0-1之间的随机数的，这个最近应用最多的地方是在产生一个从a-b范围里面的数字，这时候可以先产生一个巨大的随机0-1的矩阵，然后再乘以a和b之间的差 np.random.randn()产生的是随机正态分布的标准值，外面可以乘上std就是需要的正态分布，这样可以用来初始化深度网络的weights，括号里填的都是生成的东西的维度 另外一个问题，randn的参数需要的是inter，所以在输入的时候要不是选择 np.random.randn(x0.shape[0], x0.shape[1]) np.random.randn(*x0.shape)]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>random</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment2之FCnet]]></title>
    <url>%2F2019%2F04%2F11%2FCS231Nassignment2FCnet%2F</url>
    <content type="text"><![CDATA[This part is from the assignment 2018:stanford cs231n assignment2 目标 之前已经实现了两层的fc net，但是在这个网络里面的loss和gradient的计算用的是数学方法 这样的计算可以在两层的网络里实现，但是多层的情况下实现起来太困难了 所以在这里把电脑分成了forward pass和backward pass forward的过程中，接受所有的input，weights，和其他的参数，返回output和cache（存储back的时候需要的东西） 12345678910def layer_forward(x, w): """ Receive inputs x and weights w """ # Do some computations ... z = # ... some intermediate value # Do some more computations ... out = # the output cache = (x, w, z, out) # Values we need to compute gradients return out, cache back的时候会接受derivative和之前存储的cache，然后计算最后的gradient 12345678910111213def layer_backward(dout, cache): """ Receive dout (derivative of loss with respect to outputs) and cache, and compute derivative with respect to inputs. """ # Unpack cache values x, w, z, out = cache # Use values in cache to compute derivatives dx = # Derivative of loss with respect to x dw = # Derivative of loss with respect to w return dx, dw 这样就可以组合各个部分达到最终需要的效果了，无论多深都可以实现了 还需要一部分的优化部分，包括Dropout，Batch/Layer的Normalization Affine layer：forwardinput x：大小（N，d_1…d_k)，minibatch of N，每张图片的维度是d_1到d_k，所以拉成一长条的维度是 d_1 d_2… d_k w：weights，(D,M)，把这个长度是d的图片，输出的时候就变成M了 b:bias,(M,) -&gt; 这个bias会被broadcast到all lines （bias的值是最终分类的class的值，在不是最后一层的时候就是output的值），相当于一个class分一个bias（一列） output output,(N,M) cache:(x,w,b) implement 这里的实现直接reshape就可以了，-1的意思是这个维度上不知道有多少反正你自己给我算算的意思，但是需要N行是确定了的 注意这里验证的时候虽然input的是size，但是实际上是把数字填到这个里面的，所以取N的时候实际上是x.shape[0] Affine layer:backwardinput dout: upstream derivative, shape(N,M) cache: Tuple x w b return dx: (N,d1,d2…,dk) dw:(D,M) db:(M,) implement 注意这里用到的是链式法则：df/dx = df/dq * dq/dx 这里的df/dq就是已经求出来的dout q的式子是 Wx + b，对这三个变量分别求导，求出来大家的，别忘了求导之后的东西需要再乘dout 结果到底怎么算应该按每个矩阵的shape来推出来 ReLU activationforward input：x，随便什么尺寸都可以，这部分只是计算relu这个函数 output out，计算出来的结果 cache，储存x，用来back的运算 implement -&gt; 直接把小于0的部分设置成0就可以了 backward input 返回回来的dout cache output： 计算出来的x的梯度 implement: 求导，当原来的x大于0的时候，导数是1，链式法则是dout。小于等于0的时候是dout 所以直接对dout进行操作就可以了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107def affine_forward(x, w, b): """ Computes the forward pass for an affine (fully-connected) layer. The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N examples, where each example x[i] has shape (d_1, ..., d_k). We will reshape each input into a vector of dimension D = d_1 * ... * d_k, and then transform it to an output vector of dimension M. Inputs: - x: A numpy array containing input data, of shape (N, d_1, ..., d_k) - w: A numpy array of weights, of shape (D, M) - b: A numpy array of biases, of shape (M,) Returns a tuple of: - out: output, of shape (N, M) - cache: (x, w, b) """ out = None ########################################################################### # TODO: Implement the affine forward pass. Store the result in out. You # # will need to reshape the input into rows. # ########################################################################### out = x.reshape(x.shape[0], -1).dot(w) + b ########################################################################### # END OF YOUR CODE # ########################################################################### cache = (x, w, b) return out, cachedef affine_backward(dout, cache): """ Computes the backward pass for an affine layer. Inputs: - dout: Upstream derivative, of shape (N, M) - cache: Tuple of: - x: Input data, of shape (N, d_1, ... d_k) - w: Weights, of shape (D, M) - b: Biases, of shape (M,) Returns a tuple of: - dx: Gradient with respect to x, of shape (N, d1, ..., d_k) - dw: Gradient with respect to w, of shape (D, M) - db: Gradient with respect to b, of shape (M,) """ x, w, b = cache dx, dw, db = None, None, None ########################################################################### # TODO: Implement the affine backward pass. # ########################################################################### dx = dout.dot(w.T).reshape(x.shape) dw = (x.reshape(x.shape[0], -1).T).dot(dout) db = np.sum(dout, axis=0) ########################################################################### # END OF YOUR CODE # ########################################################################### return dx, dw, dbdef relu_forward(x): """ Computes the forward pass for a layer of rectified linear units (ReLUs). Input: - x: Inputs, of any shape Returns a tuple of: - out: Output, of the same shape as x - cache: x """ out = None ########################################################################### # TODO: Implement the ReLU forward pass. # ########################################################################### out = x.copy() out[out &lt;= 0] = 0.0 ########################################################################### # END OF YOUR CODE # ########################################################################### cache = x return out, cachedef relu_backward(dout, cache): """ Computes the backward pass for a layer of rectified linear units (ReLUs). Input: - dout: Upstream derivatives, of any shape - cache: Input x, of same shape as dout Returns: - dx: Gradient with respect to x """ dx, x = None, cache ########################################################################### # TODO: Implement the ReLU backward pass. # ########################################################################### dout[x &lt;= 0] = 0 dx = dout ########################################################################### # END OF YOUR CODE # ########################################################################### return dx sandwich layer在文件cs231n/layer_utils.py里面，有一些比较常见的组合，可以集成成新的函数，这样用的时候就可以直接调用不用自己写了 loss layer -&gt; 和assignment1里面写的内容是一样的two-layer networkcs231n/classifiers/fc_net.py TwoLayerNet init__ 需要初始化weights和bias，weights应该是0.0中心的高斯（=weight_scale），bias应该是0，都存在self.para的字典里面，第几层的名字就叫第几 input 图片的size hidden的个数 class的数量 weight scale，看初始的weights怎么分布 reg，regularization时候的权重 forward 用前面已经写好的东西计算前向 最后得到scores 再用scores计算loss，注意 计算loss也是一层 计算loss的时候注意他这里loss的参数是scores和lable backward back的时候不要忘记了loss也是一层，所以输入第二个sandwich的时候输入的应该是dscores而不是scores？！！！！ 计算gradient，注意他的function里面已经除了总数！ 别忘了加上L2的regularization Solver把之前那些训练啊，验证啊，计算accuracy之类的部分全都扔到一个class里面叫做solver，打开cs231n/solver.py 作用 solver部分包括所有训练分类所需要的逻辑部分，在optim.py里面还用了不同的update方法来实现SGD 这个class接受training和validation的数据和labels，所以可以检查分类的准确率，是否overfitting 需要先构成一个solver的instance，把需要的model，dataset，和不同的东西（learning rate，batch，etc）放进去 先用train()来训练，然后model的para都存着所有训练完的参数 训练的过程也会记录下来（accuracy的改变啥的） 最后训练的结果大约在50%12345678910111213141516171819model = TwoLayerNet()solver = None############################################################################### TODO: Use a Solver instance to train a TwoLayerNet that achieves at least ## 50% accuracy on the validation set. ###############################################################################solver = Solver(model, data, update_rule = 'sgd', optim_config=&#123;'learning_rate': 1e-3,&#125;, lr_decay=0.95, num_epochs=10, batch_size=100, print_every=100)solver.train()############################################################################### END OF YOUR CODE ######################################################################## 可视化这个最终的结果，loss随着epoch的变化和training acc以及val acc的变化12345678910111213141516# Run this cell to visualize training loss and train / val accuracyplt.subplot(2, 1, 1)plt.title('Training loss')plt.plot(solver.loss_history, 'o')plt.xlabel('Iteration')plt.subplot(2, 1, 2)plt.title('Accuracy')plt.plot(solver.train_acc_history, '-o', label='train')plt.plot(solver.val_acc_history, '-o', label='val')plt.plot([0.5] * len(solver.val_acc_history), 'k--')plt.xlabel('Epoch')plt.legend(loc='lower right')plt.gcf().set_size_inches(15, 12)plt.show() 记下来了这个loss和acc的history，所以就可以直接用来可视化了！ Multilayer network现在开始实现有多层的net 需要注意的问题主要是数数数对了，注意数字和layer的数量的关系 为了保证验证的准确，需要把loss的regularization算对才可以 反向往回推的时候，可以用 reversed(range(a))这个东西来进行 总体来说和两层的差不多，就是加进来了for循环 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202class FullyConnectedNet(object): """ A fully-connected neural network with an arbitrary number of hidden layers, ReLU nonlinearities, and a softmax loss function. This will also implement dropout and batch/layer normalization as options. For a network with L layers, the architecture will be &#123;affine - [batch/layer norm] - relu - [dropout]&#125; x (L - 1) - affine - softmax where batch/layer normalization and dropout are optional, and the &#123;...&#125; block is repeated L - 1 times. Similar to the TwoLayerNet above, learnable parameters are stored in the self.params dictionary and will be learned using the Solver class. """ def __init__(self, hidden_dims, input_dim=3 * 32 * 32, num_classes=10, dropout=1, normalization=None, reg=0.0, weight_scale=1e-2, dtype=np.float32, seed=None): """ Initialize a new FullyConnectedNet. Inputs: - hidden_dims: A list of integers giving the size of each hidden layer. - input_dim: An integer giving the size of the input. - num_classes: An integer giving the number of classes to classify. - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=1 then the network should not use dropout at all. - normalization: What type of normalization the network should use. Valid values are "batchnorm", "layernorm", or None for no normalization (the default). - reg: Scalar giving L2 regularization strength. - weight_scale: Scalar giving the standard deviation for random initialization of the weights. - dtype: A numpy datatype object; all computations will be performed using this datatype. float32 is faster but less accurate, so you should use float64 for numeric gradient checking. - seed: If not None, then pass this random seed to the dropout layers. This will make the dropout layers deteriminstic so we can gradient check the model. """ self.normalization = normalization self.use_dropout = dropout != 1 self.reg = reg self.num_layers = 1 + len(hidden_dims) self.dtype = dtype self.params = &#123;&#125; ############################################################################ # TODO: Initialize the parameters of the network, storing all values in # # the self.params dictionary. Store weights and biases for the first layer # # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be # # initialized from a normal distribution centered at 0 with standard # # deviation equal to weight_scale. Biases should be initialized to zero. # # # # When using batch normalization, store scale and shift parameters for the # # first layer in gamma1 and beta1; for the second layer use gamma2 and # # beta2, etc. Scale parameters should be initialized to ones and shift # # parameters should be initialized to zeros. # ############################################################################ pr_num = input_dim # can't use enumerate beacuse I need the number more than the size of hidden_dims for layer in range(self.num_layers): layer += 1 weights = 'W' + str(layer) bias = 'b' + str(layer) # 这时候是最后一层(the last layer) if layer == self.num_layers: self.params[weights] = np.random.randn( hidden_dims[len(hidden_dims) - 1], num_classes) * weight_scale self.params[bias] = np.zeros(num_classes) # other layers else: hidd_num = hidden_dims[layer - 1] self.params[weights] = np.random.randn( pr_num, hidd_num) * weight_scale self.params[bias] = np.zeros(hidd_num) pr_num = hidd_num if self.normalization in ["batchnorm", "layernorm"]: self.params['gamma' + str(layer)] = np.ones(hidd_num) self.params['bata' + str(layer)] = np.zeros(hidd_num) # print(len(self.params)) # print(self.params) ############################################################################ # END OF YOUR CODE # ############################################################################ # When using dropout we need to pass a dropout_param dictionary to each # dropout layer so that the layer knows the dropout probability and the mode # (train / test). You can pass the same dropout_param to each dropout layer. self.dropout_param = &#123;&#125; if self.use_dropout: self.dropout_param = &#123;'mode': 'train', 'p': dropout&#125; if seed is not None: self.dropout_param['seed'] = seed # With batch normalization we need to keep track of running means and # variances, so we need to pass a special bn_param object to each batch # normalization layer. You should pass self.bn_params[0] to the forward pass # of the first batch normalization layer, self.bn_params[1] to the forward # pass of the second batch normalization layer, etc. self.bn_params = [] if self.normalization == 'batchnorm': self.bn_params = [&#123;'mode': 'train'&#125; for i in range(self.num_layers - 1)] if self.normalization == 'layernorm': self.bn_params = [&#123;&#125; for i in range(self.num_layers - 1)] # Cast all parameters to the correct datatype for k, v in self.params.items(): self.params[k] = v.astype(dtype) def loss(self, X, y=None): """ Compute loss and gradient for the fully-connected net. Input / output: Same as TwoLayerNet above. """ X = X.astype(self.dtype) mode = 'test' if y is None else 'train' # Set train/test mode for batchnorm params and dropout param since they # behave differently during training and testing. if self.use_dropout: self.dropout_param['mode'] = mode if self.normalization == 'batchnorm': for bn_param in self.bn_params: bn_param['mode'] = mode scores = None ############################################################################ # TODO: Implement the forward pass for the fully-connected net, computing # # the class scores for X and storing them in the scores variable. # # # # When using dropout, you'll need to pass self.dropout_param to each # # dropout forward pass. # # # # When using batch normalization, you'll need to pass self.bn_params[0] to # # the forward pass for the first batch normalization layer, pass # # self.bn_params[1] to the forward pass for the second batch normalization # # layer, etc. # ############################################################################ cache = &#123;&#125; temp_out = X for i in range(self.num_layers): w = self.params['W' + str(i + 1)] b = self.params['b' + str(i + 1)] if i == self.num_layers - 1: scores, cache['cache' + str(i + 1)] = affine_relu_forward(temp_out, w, b) else: temp_out, cache['cache' + str(i + 1)] = affine_relu_forward(temp_out, w, b) ############################################################################ # END OF YOUR CODE # ############################################################################ # If test mode return early if mode == 'test': return scores loss, grads = 0.0, &#123;&#125; ############################################################################ # TODO: Implement the backward pass for the fully-connected net. Store the # # loss in the loss variable and gradients in the grads dictionary. Compute # # data loss using softmax, and make sure that grads[k] holds the gradients # # for self.params[k]. Don't forget to add L2 regularization! # # # # When using batch/layer normalization, you don't need to regularize the scale # # and shift parameters. # # # # NOTE: To ensure that your implementation matches ours and you pass the # # automated tests, make sure that your L2 regularization includes a factor # # of 0.5 to simplify the expression for the gradient. # ############################################################################ loss, dscores = softmax_loss(scores, y) reg_loss = 0.0 pre_dx = dscores for i in reversed(range(self.num_layers)): i = i + 1 reg_loss = np.sum(np.square(self.params['W' + str(i)])) loss += reg_loss * 0.5 * self.reg pre_dx, dw, db = affine_relu_backward( pre_dx, cache['cache' + str(i)]) dw += self.reg * self.params['W' + str(i)] db += self.reg * self.params['b' + str(i)] grads['W' + str(i)] = dw grads['b' + str(i)] = db ############################################################################ # END OF YOUR CODE # ############################################################################ return loss, grads 检测网络是否overfitting 选择了一个三层的网络，小幅度改变learning rate和init scale 尝试去overfitting出现了一些问题不是太能overfitting我不知道为什么 update rules在得到了back出来的dw之后，就需要用这个dw对w进行update，这里有一些比较常见的update方法 普通的update 仅仅沿着gradient改变的反方向进行(反方向是因为计算出来的gradient是上升的方向)x += - learning_rate * dx SGD + momentumhttp://cs231n.github.io/neural-networks-3/#sgd 是对这个update一点物理上比较直观的理解（其实名字叫做动量） 可以理解为这个东西是在一个平原上跑的一个球，我们需要求的w是这个球的速度，得到的dw是这个球的加速度，而这个球的初速度是0 可以理解为这个球找最低点的时候，除了每步按dw update，还在上面加上了前面速度的影响，也就是加上了惯性！123# Momentum updatev = mu * v - learning_rate * dx # integrate velocityx += v # integrate position Nesterov Momentum(NAG) 在原来的基础上：真实移动方向 = 速度的影响（momentum）+ 梯度的影响 （gradient） 现在：既然我们已经知道了要往前走到动量的影响的位置，那么我根据那个位置的梯度再进行update，岂不是跑的更快！ 总的来说就是考虑到了前面的坡度（二阶导数），如果前面的坡度缓的话我就再跑快点，如果陡的话就跑慢点123v_prev = v # back this upv = mu * v - learning_rate * dx # velocity update stays the samex += -mu * v_prev + (1 + mu) * v # position update changes form cs231n/optim.py 加入了新的计算update的方法 具体的原理还没有看，但是计算就是这样计算的 12345678910111213141516171819202122232425262728293031def sgd_momentum(w, dw, config=None): """ Performs stochastic gradient descent with momentum. config format: - learning_rate: Scalar learning rate. - momentum: Scalar between 0 and 1 giving the momentum value. Setting momentum = 0 reduces to sgd. - velocity: A numpy array of the same shape as w and dw used to store a moving average of the gradients. """ if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-2) config.setdefault('momentum', 0.9) v = config.get('velocity', np.zeros_like(w)) next_w = None ########################################################################### # TODO: Implement the momentum update formula. Store the updated value in # # the next_w variable. You should also use and update the velocity v. # ########################################################################### v = config['momentum'] * v - config['learning_rate'] * dw w += v next_w = w ########################################################################### # END OF YOUR CODE # ########################################################################### config['velocity'] = v return next_w, config 可以看出来最终的结果会比普通的SGD上升的更快 分别又尝试了RMSProp and Adam]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>Fully Connected Net</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于空间投影增强（SAR）的论文]]></title>
    <url>%2F2019%2F04%2F11%2F%E5%85%B3%E4%BA%8E%E7%A9%BA%E9%97%B4%E6%8A%95%E5%BD%B1%E5%A2%9E%E5%BC%BA%E7%9A%84%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[相关介绍链接koike lab roomAlive 关于opencv的fisheye calibration：http://ninghang.blogspot.com/2012/08/fish-eye-camera-calibration.html 关于calibration的视角的问题，matlab可以找到全部视角：https://www.mathworks.com/help/vision/ug/fisheye-calibration-basics.html DeepCalib: A Deep Learning Approach for Automatic Intrinsic Calibration of Wide Field-of-View Cameras（CVMP ‘18）使用深度学习对fish eye相机的视野进行补全。 文章中公开的code和其他资料 Abstract 广角相机的calibration在各种各样的地方都有应用 3D重建 image undistortion AR camera motion estimation 现在存在的calibration都需要多张图片进行校准（chessboard） 提出了一种完全自动的标定方法，基于CNN 在网上找了非常多的omnidirectional的图片进行训练，生成了具有100多万张图片的dataset Intro 广角相机的calibration最重要的是测量 intrinsic parameters 两个wide FOV的重要参数：focal length &amp; distortion parameter 现存的calibration方法有很多限制： 需要一个object的多个角度的观察 需要观察一个特定的structures 在多张图片中观察相机的移动 现在最有名的方法是chessboard 他们提出的方法可以解决上述的问题，并且针对网上下下来的照片也可以用 主要方法 一个CNN with Inception-V3 architecture 目标 收集具有不同intrinsic parameters的图片，然后自动生成具有不同的focal length和distortion的图片（没懂） 对比不同的CNN结构 related work 以前存在的calibration方法主要可以分成四个不同的部分 最常使用的就是棋盘一类的，需要观察一张图片的不同部分，从而得到结果。问题主要是出在比价麻烦，而且无法对野生的照片进行calibration 基于图片上面的geometric structure，line，消失的点等。不能处理general environments self-calibration（自身还有一些容易收到影响的问题） 需要多张图片 需要camera motion estimation 基于DL的，但是都是解决了部分问题 没有把参数全都估计出来 dataset是从prespective的图片生成回来的，会有不完整的部分（但是他们的很完整而且会有很多应用） Approach选择model -&gt; 自动生成large-scale dataset -&gt; network的structure Projection &amp; distrotion model（考虑生成dataset的机器） 广角相机需要具体的projection model把3D的世界map到图片里面去 考虑了几个model Brown-Conrady’s model(1971) 在实际应用里面不适合广角相机的大的distortion hardly reversible division model [Fitzgibbon 2001] 只是为了fisheye设计的，对相机没有普适性 impossible to revert 这篇文章里面的model unified spherical model [Barreto 2006; Mei and Rives 2007] 原因 fully reversible 可以解决很大的distortion projection和back-projection都admit closed-form solution -&gt; 计算效率非常高（没怎么看懂） generation dataset 因为根本做不到并且还没有那么大的dataset，所以他们打算人工合成一些(synthetically) 没有选择用prspective的图片生成 在普通的图片里面加上distortion会把图片里应该看不到的地方看到（边缘都会变成黑色的） -&gt; 生成的图片不真实 使用panoramas得到图片 因为全景图都是360度的，那么多少度的广角都能驾驭 可以假设把相机放在任何地方 对于给的一张全景图，可以自动生成不同焦距，不同distortion的图片，这样就得到了很大的dataset network architecture Inception-V3 structure 基于上面的，实践了三种不同的网络 一层网络，输出两个不同的结果，一个是f一个是distortion DualNet，由两个独立网络组成，一个输出f，一个输出distortion，这两个值是相互独立的。 SeqNet，两个连在一起的网络，先从A网络里得到f，再把图片和f放进B得到最终的distortion 解决问题： classification regression resultnet的参数 net在imageNet上面pre-train了，然后再进行了进一步的训练 evaluation对比上面不同三个网络的performance user study估计出来的结果很难说明到底是不是成功的undistort了，所以设计了user study Combining Multiple Depth Cameras and Projectors for Interactions On, Above, and Between Surfaces（‘2010）感觉算是比较最早的SAR的部分，重点就是用多个视角的depth camera来捕捉用户的动作，完成相应的交互，不知道在桌子上的投影和在墙上的投影是怎么实现的 abstract 可以交互的displays和surface 可以投影到非常规的投影表面上面去 可以把这些东西扔来扔去，之类的 Intro the user may touch to manipulate a virtual object projected on an un-instrumented table（这个现在已经不新鲜了） office size room depth camera的妙用 这个空间的任何地方都是surface，都可以投影 整个空间是一个大的电脑 可以投影到user自己的身上去-&gt; 可以投影到用户的手上 3D mesh data 硬件构成：multiply的depth camera&amp; projector 支持的interaction 可以交互的非显示器部分（比如墙壁或者桌子） 所有的部分可以连接成一个可交互的部分，可以通过肢体来进行两个屏幕之间的交互（同时摸这两个东西他就会换位置） 可以从display上面pick up出东西来 检测出用户的动作来，支持动作的交互 implement 在天花板上装了三个depth camera和三个projector，可以看到交互的地方，不需要特别精准的calibration PrimeSense camera，有IR和RGBcamera depth image可以用来分离静止的物体 calibration both the cameras and the projectors are registered with the real world. camera a fixed grid of retro-reflective dots 3D camera pose estimation de- scribed by Horn[13] interactive space calibration之后camera就可以捕捉real time的3D mesh model 因为camera和projector一起校准过了，所以投影就可以正确的投影在相应的地方了 根据mesh model可以得到手的三维图形，根据这个图形就可以知道手在touch哪个地方了 在tracking上面用了更简单的算法：[28] [29] 直接对3D的mesh进行操作比较复杂，所以对2D的画面进行了操作 virtual camera first transforming each point in every depth camera image from local camera to world coordinates, and then to virtual camera coordinates by virtual camera view and projection matrices. z方向的坐标由xy写出来 -&gt; 把一张深度图片压成了一个2D的图片 结合多个角度判断用户的最终动作 用上方的摄像机的图片判断用户是不是同时接触两个东西了 空间里的mene -&gt; 在特殊的一个地方有投影 RoomAlive: Magical Experiences Enabled by Scalable, Adaptive Projector-Camera Units(UIST ‘14)感觉是一个比较完全的屋内投影的例子了 Abstract 可以动态的适应任何的屋子 touch, shoot, stomp, dodge, steer投影上去的东西，以及和物理的环境交互 projector-depth camera unit -&gt; 所以就不需要特别多的calibration（可以重点看看这个unit是怎么制作的） Intro 做了一个游戏系统 projector和depth camera一体的东西 cover the room’s walls and furniture with input/output pixels track用户的动作，并且根据动作在屋子里面生成对应的东西 capture &amp; analyze屋子里的结构，得到房间里面的墙以及地板之类的特征 a distributed framework for tracking body movement and touch detection using optical-flow based particle tracking [4,15], and pointing using an infrared gun [19]. -&gt; 其实还不是没有依据视觉来捕捉这个东西 居然装了6个相机-投影仪的unit （procam） related workSpatial Augmented Reality (SAR) use light to change appearance physical objects illumiroom -&gt; 非常喜欢这个idea projection mapping很多都需要在特定的东西上面mapping -&gt; 但是这个可以在整间屋子的任意部分mapping System unit -&gt; color camera + IR camera emitter + wide FOV projector + computer in a large living room (说明这种研究里面屋子的大小也非常的重要) + 6 units plug-in to the Unity3D commercial game engine （怪不得能做游戏 硬件 wide field of view projectors 每个部分connected to他自己特定的电脑 所有的部分都装在房间的屋顶上 auto calibration 并不需要calibration所有的相机 在units之间有一部分的overlap，所以东西在校准的时候观察同一个东西就行了？ 用opencv的校准function chain together所有的部分然后得到了各个相机的关系 auto scene analysis 所有的unit得到的深度信息，生成之后寻找连续的平面（墙，地板等等） Hough transform（并不会这个东西） 游戏 unity3D的plug-in 游戏设计者只需要在设计界面里添加东西就行了 mapping 事实渲染整个东西的任务没有完全解决 4个技术 content in a uniformly random way 哈。。。居然是随机投影出来的 针对不同类型的被投影的东西，会根据不同的原理出现在不同的地方（比如石头只会出现在地面上） 投影的东西针对用户现在的位置，只投在用户自己看得到的地方 在移动屋子里的物理物品的时候，改变屋子的部分 tracking user interface body movement, touching, stomping, pointing/shooting and traditional controller input [4,15]捕捉了depth map -&gt; ‘proxy particles’,就是动作游戏里面的体感捕捉的算法 -&gt; tracked by using a depth-aware optical flow algorithm gun的input选择了红外枪 也支持寻常的游戏手柄 rendering RoomAlive tracks the player’s head position and renders all virtual content with a two-pass view dependent rendering 这部分主要讲游戏怎么设计的limitation calibration errors！这样在交叠的地方会出现重影 system latency 延迟QAQ 在overlap的sensors上面解决tracking issues Peripheral Expansion of Depth Information via Layout Estimation with Fisheye Camera( ‘16)从RGBD鱼眼相机提取深度信息（但是这个用了多个相机的system） abstract 一个普通的RGB相机和一个fish eye，把视角扩展到了180° developed a new method to generate scaled layout hypotheses from relevant corners, combining the extraction of lines in the fisheye image and the depth information overcome severe occlusions. intro 主要就是把现有的RGB fisheye camera和Depth camera结合起来，得到鱼眼的深度信息 Pedestrian Detection in Fish-eye Images using Deep Learning: Combine Faster R-CNN with an effective Cutting Method(SPML ‘18)用鱼眼相机和RCNN来检测行人（感觉这个检测的目标比较小） -&gt; 怎么感觉挺水的 abstract 鱼眼相机的边缘扭曲问题 -&gt; rotary cutting to solve the problem 把相机分成了边缘部分和中间部分 Method 裁剪图片 绕着鱼眼相机的中心旋转，30度，12次 每次旋转完截取三组图片，分别是靠边缘的和靠中心的 -&gt; 更好检测人群（垂直的） 使用这些裁剪的图片training]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>projection mapping</tag>
        <tag>space augumented</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 1Q AR笔记]]></title>
    <url>%2F2019%2F04%2F09%2F2019Q1AR%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[整体步骤 检测marker 为他计算计算6DOF pose render 3D 图片 把图片和marker组合在一起 slides学习opengl(学习老师的cpp代码风格) Ex1:检测marker的四个基础点打开相机 注意初始化 videocapture 应该是在while的循环之前的 thresholding 函数 cv::threshold(…) src dst thres的值，也就是阈值的边界值 double maxval，在最后一个参数是binary的时候，确定binary的最大值 type，二值图，反二值图，保留原色等乱七八糟的 注意binary之前要先 cvtcolor到灰度图！！！ cv::adaptiveThreshold &lt;- 试试这个函数的作用，不用自己设置threshold了 并不想set这些hypers manully 除了寻常需要设置的东西之外，还需要 adaptiveMethod block size（需要被用来计算threshold的value） C：需要被从整体中减去的一个constant 有些变量的地方用到了const &lt;- 感觉应该学学老师的编程风格detect connected componentscv::findContours 函数 图片 contours vector&lt;std::vector&lt;cv::Point&gt; &gt; hierarchy vector&lt;cv::Vec4i&gt;，contour的拓扑学信息 mode （注意在这里选择要外轮廓还是内外都要） method：估计contour的方法 offset（当从ROI提取轮廓然后在整张图片里面分析的情况） 去除过小的contour12345678910vector&lt;vector&lt;Point&gt;&gt; :: iterator itc = contours.begin(); while(itc != contours.end())&#123; if(itc -&gt; size() &lt; 60)&#123; itc = contours.erase(itc); &#125; else&#123; itc++; &#125; &#125; 老师的代码里面是先进行了估计，计算了bound的面积，然后根据 面积大小，占整张图片的百分比 几个角 cv::isContourConvex ：检查这个marker的凸性，毕竟形状不能是凹的，直接输入这个多边形的array，输出的就是bool 估计contour的多边形 approxPolyDP 被估计的contour 估计出来的的多边形 估计的参数，影响估计的精度 -&gt; const auto epsilon = 0.05 * cv::arcLength(contour, true); 计算一个curve的长度 closed，估计出来的多边形是不是封闭的 画出来只有四个角的多边形 drawContours() 图片 需要画的轮廓s （注意这是这张图片里面的所有轮廓） 需要画的index thickness 线的 线得种类 Optional information about hierarchy. maxLevel Maximal level for drawn contours. If it is 0, only the specified contour is drawn. If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This parameter is only taken into account when there is hierarchy available. offset：可选 老师用的方法是计算了各个点和计算了各个边，所以可以画出来边上还有好多点的结果 一种神奇的定义颜色的方式，直接随机出来 const cv::Scalar kEdgeColor(rand() &amp; 255, rand() &amp; 255, rand() &amp; 255); -&gt; 这样的话出来的每一帧的框的颜色都会改变![] 用cv::polylines来画出来polys的curve，并且要画closed的 画出来图片的delimiters(为下一步做准备) 目的：从一个corner到另一个corner的方向vector 步骤 首先在corner上面用circle画出来 检查每个edge 每个edge上有6个点，然后计算两个corner之间的dx和dy的方向，除以部分的数量（7）就是每个小块的方向，然后把这个小块重复6次12const double dx = (double)(contour_approx[(i+1)%kNumOfCorners].x-contour_approx[i].x)/(double)(kNumOfEdgePoints+1);const double dy = (double)(contour_approx[(i+1)%kNumOfCorners].y-contour_approx[i].y)/(double)(kNumOfEdgePoints+1); 第一部分结束后的完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#include "ARdetection.hpp"ARDetection::ARDetection(void)&#123; &#125;Mat ARDetection:: SearchMarkers(Mat frame)&#123; Mat img_gray; Mat dst; // threshold之前要先改成灰度图 cvtColor(frame, img_gray, CV_BGR2GRAY); adaptiveThreshold(img_gray, dst, 255, ADAPTIVE_THRESH_MEAN_C, THRESH_BINARY, 33, 5);// threshold(img_gray, dst, 104, 255, THRESH_BINARY); imshow("lalala", dst); waitKey(1); // 每个点，一圈点事是一个contour，一堆kcontours vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(dst, contours, hierarchy, RETR_LIST , CHAIN_APPROX_NONE); // 检测图片过小部分// vector&lt;vector&lt;Point&gt;&gt; :: iterator itc = contours.begin();//// while(itc != contours.end())&#123;// if(itc -&gt; size() &lt; 60)&#123;// itc = contours.erase(itc);// &#125;// else&#123;// itc++;// &#125;// &#125; vector&lt;vector&lt;Point&gt;&gt; polys(contours.size()); for(int i=0; i &lt; contours.size(); i++ )&#123; // 根据每个轮廓调节这个参数的大小 const auto epsilon = 0.05*cv::arcLength(contours[i], true); approxPolyDP(contours[i], polys[i], epsilon, true); // if(polys[i].size() == 4)&#123;//// 用这个函数画不出来斜的东西//// rectangle(frame, polys[i][0],polys[i][2], Scalar(0,255,0));// drawContours(frame, polys, i, Scalar(255,0,0), 5, 8, vector&lt;Vec4i&gt;(), 0, Point());// &#125; Rect rect = boundingRect(polys[i]); const int marker_size = rect.area(); const int ImageSize = img_gray.cols * img_gray.rows; const int marker_size_min = int(ImageSize * 0.02); const int marker_size_max = int(ImageSize * 0.95); const int marker_corners_num = 4; const bool is_vaild = (marker_size &gt; marker_size_min) &amp;&amp; (marker_size &lt; marker_size_max) &amp;&amp; (polys[i].size() == marker_corners_num) &amp;&amp; isContourConvex(polys[i]); if (is_vaild == false) continue; // 这样画出来的是随机的颜色的 const Scalar EdgeColor(rand() &amp; 255, rand() &amp; 255, rand() &amp; 255); polylines(frame, polys[i], true, EdgeColor,5); // 下面需要把每个边分成6个部分 for(int j = 0; j &lt; marker_corners_num; ++j)&#123; const int edge_point_num = 6; const int circle_size = 5; circle(frame, polys[i][j], circle_size, Scalar(0,255,0),FILLED); const double dx = (double)(polys[i][(j+1)%marker_corners_num].x - polys[i][j].x)/(double)(edge_point_num + 1); const double dy = (double)(polys[i][(j+1)%marker_corners_num].y - polys[i][j].y)/(double)(edge_point_num + 1); for(int k = 0; k&lt; edge_point_num; ++k)&#123; const double edge_point_x = (double)(polys[i][j].x) + (double)(k+1)*dx; const double edge_point_y = (double)(polys[i][j].y) + (double)(k+1)*dy; Point edge_point((int)edge_point_x,(int)edge_point_y); circle(frame, edge_point, circle_size, Scalar(0,0,255),-1); &#125; &#125; &#125; return frame;&#125; 第一部分运行之后的结果 Ex2. find marker precisely 现在有corner，corner之间的line，这个line还被分成6个部分（~因为大小是6x6？的= 两个边 + 中间四个格？？~） 光检测边缘是不够的，想知道这个边缘实际是什么样子的 现在只知道虚线的部分是什么样子的 并且现在已经把每个边都分部分了 -&gt; 画出来垂直的分块的线，找到这个线和颜色突变的交点，重新画出来新的线（实线） 希望找到颜色突变的地方 但是实际上的颜色不是突变的，是白 -&gt; 灰 -&gt; 黑 操作步骤 预准备 在每个side找六个点 在边上提取三个像素宽度的stride cv::GetQuadrangleSubPix() -&gt; 不会用！ 从输入的array得到四边形 src输入图像 dst提取出来的四边形 变换的矩阵 Sober operator 图像处理里面的常用算子 -&gt; 主要用于边缘检测，用来运算与灰度的相似值 包含两组3x3的矩阵，中间的3x1的0，分别为横向和纵向，另外两面对称 然后与图片做卷积，分别计算x方向和y方向的灰度值 然后把Gx和Gy求平方和的根，最后得出来这个像素点的灰度值（或者有的时候也可以用绝对值来计算，这样计算的消耗小一点） 步骤 在每个stripe里找到最高的change 对这个change和她周围的点（在原来的曲线上），找到一个新的二次曲线 找到这个曲线的顶点（一阶导数） 如何得到图片的subpixel color不是原来的方方正正的，而是沿着那个直线方向的新的方方正正 就是取各个颜色在面积上的平均， 老师的代码 sublixSampleSafe -&gt; 这个函数输入测试的图片（gray和已经得到的subpix点 把这个点求floor，int，得到基准点，然后检查是不是在图片里面（不是的话返回gray，127） 如果在图片里面的话计算出来实际的坐标 确定marker的id corner detection exact sides 找到沿着刚才计算出来6个点的一条线fitLine precise corner 从各个边的交点计算精确的corner（因为各个边已经很精确了） Marker Rectification 创建一个6x6 pix的ID图片 (-0.5,-0.5) to (5.5,5.5) 从原图片的perspective warp到Id image cv::getPerspectiveTransform or cv::warpPerspective 需要找到的是linear transformation]]></content>
      <categories>
        <category>AR</category>
        <category>上课笔记</category>
      </categories>
      <tags>
        <tag>AR</tag>
        <tag>OpenCV</tag>
        <tag>OpenGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Notes for papers about smart kitchen]]></title>
    <url>%2F2019%2F04%2F08%2Fkitchen%2F</url>
    <content type="text"><![CDATA[Choptop: An Interactive Chopping Board2018 abstract 一个可交互的案板 可以给用户菜谱的指导，承重，计时 用户可以通过按案板来进行对画面的操作 上面就是长成上图的样子，案板底下充满了各种传感器。 Intro 针对学生不会自己做饭，不吃新鲜的饭的问题，缺时间 -&gt; 一步一步的把怎么做饭写出来了，包括图片动画等东西 built-in timer用来每一步计时 使用mobile device来提高菜单的交互性 考虑到手的脏等问题，所以不是按屏幕而是按案板（这里考虑能不能像pac pac一样，用手势操作） load sensors，可以解决称重的问题 思路 主要目的是一种新的学习做饭的方法 （作为做饭有天赋的人，我认为这样没有灵魂！） related work smart kitchen Research has aimed to improve the cook- ing process, promote healthier eating and make it simpler to procure ingredients，大家都从不同的角度实现智能厨房 他这个论文的东西成本不是特别高也不是特别大（是在嘲讽我吗） load sensing 之前已经用了很多force的传感器 之前也有用过带重量传感器的案板，以及带扭矩传感器的刀[9] 他们认为camera没有什么用，并且把所有的硬件都藏起来了 装这个senser的方法参考了[12] Design硬件 整个硬件是self-contained的 屏幕是单片机控制的 力量传感系用 检测按压用的是edge detection -&gt; 防止检测到其他东西（原理不是太懂） UI The interface updates based on the information delivered from the recognition engine 成功之后还会有声音 按案板的不同部分就可以往不同的方向移动 user study 找了十三个人，准备一道沙拉 [3]里面找到了一个调查问卷System Usability Scale future SVM训练了74%的测试率（好低），提高正确率 提高自动翻页（？ 用用户的手机来达到屏幕的作用 进一步分user study CookTab: Smart cutting board for creating recipe with real-time feedback2012 abstract 考虑到很多厨师做饭随心所欲，而且不会记录下来精准的用量 一个可以记录下来用量的案板 intro 专门针对切菜部分的记录的软件 记录用的材料的名字，菜量，视频和调味方法，然后会有real-time的feedback related work [3]可以记录视频，声音，用的camera和mic 加重量感应的， [2]四个承重的模块，加速度传安琪 但是他们的系统会有real-time的feedback 不好意思好像就是在pad上切菜 Enabling Nutrition-Aware Cooking in a Smart KitchenCHI 2007（大概只能看个概念了） abstract 目标：health cooking（是不是大家的目标都是那么伟大） sensors，detect cooking activities, and digtail feedback intro 主要目标就是如果人加东西假的过量了或者怎么着，就会提醒 Smart Kitchens for People with Cognitive Impairments: A Qualitative Study of Design RequirementsCHI-2018]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Smart Kitchen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于mac消去分区]]></title>
    <url>%2F2019%2F04%2F08%2F%E5%85%B3%E4%BA%8Emac%E6%B6%88%E5%8E%BB%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[最近刚接手了别人的mbp，他因为装双系统分区之后，windows分区无法消去。试了一圈之后发现只要再新建一个分区，然后再一起合并就可以了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于Python的字典多key，value返回key]]></title>
    <url>%2F2019%2F04%2F05%2F%E5%85%B3%E4%BA%8EPython%E7%9A%84%E5%AD%97%E5%85%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E4%B8%9C%E8%A5%BF%2F</url>
    <content type="text"><![CDATA[多个keys1dict = &#123;(key11,key12): value&#125; 大概就是长这个样子的，key的个数多少没有限制，访问value的时候1dict[(key11,key12)] 多个value1dict = &#123;key1:(value1, value2)&#125; 访问的时候大概就是这么取值12dict[key]dict[key][index] 从value找到key 先通过list(dict.key())获得所有的key，变成一个list list(dict.value())得到所有的value的list 上面这两个list的index相同，先获取value的index，然后再作为key的index去key里找 例子：12dict = &#123;'a': 1, 'b': 2&#125;list(dict.key())[list(dict.value()).index('1')]]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>字典</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Human Computer Interaction笔记]]></title>
    <url>%2F2019%2F04%2F04%2FHCI%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Introduction 50% 出勤和 50% report koike森森还有assignment History 1945 Vannevar Bush memex的概念，二战末提出了一种信息机器的设想（个人图书馆） 这种机器内部用微缩胶卷（microfilm）存储信息，也就是自动翻拍，可以不断往里面添加新的信息；桌面上有阅读屏，用来放大阅读微缩胶卷；还有许多个按钮，每一个按钮代表一个主题，只要按一下，相应的微缩胶卷就会显示出来。每一个胶卷内部还记录着相关的其他胶卷的编号，可以方便地切换，形成同主题阅读。在Bush博士的设想中，这种机器还可以与图书馆联网。通过某种机制，将图书馆收藏的胶卷，自动装载到本地机器上。因此，只通过这一个机器，就可以实现海量的信息检索。from 百度百科 as we may think：同时提出了wearable电脑 1946 Eniac 第一个计算机 没有keyboard和display，只能手动 1951 UNIVAC 可以I/O 先在纸条上打孔，然后再放进去读 Ivan Sutherland 1963 SketchPad，display上面有图像了，并且可以对画面进行操作 可以用光笔操作，不是用键盘操作了 CAD鼻祖 第一个VR设备居然也是他做出来的 Sword of Damocles(1968) Douglas Engelbart 居然出现了鼠标，装了rotatory wheels（竖着的那种），可以在两个方向移动（所以装了两个吗？） Alan Kay PC之父 1972 Dynabook，card board做的，因为CPU和GPu太大了，屏幕也没有，并不是真的 1996年东芝做了个叫这个名字的PC 1973 Xerox Alto real working machine 1981 Xerox Star 出现了桌面系统 GUI Ted Nelson Hypertext Editing System -&gt; pen to jumo to another page Steve Jobs &amp; Bill Atkinson HyperCard 1987，同类的信息都在同一张卡上，然后所以的卡都连在一起，可以在卡中间jump。事情就变得非常简单了 Tim Berners-Lee father of WWW （World Wide Web） Richard Bold put that there可以用手势交互，语言交互 非常大的一个display和projector Mark Weiser father of the concept of Ubiquitous conputing(1991) 预言了以后大家家家有电脑 Jaron Lanier VPL data glove &amp; HMD 1989，手套里面有纤维 I/O的硬件如何组合 -&gt; 新的HCI方式 design &amp; evaluationACM SIGCHI What’s HCI CS design 是一个交叉学科 学会 CHI -&gt; 更注重想法，和转化成实现 UIST -&gt; 更注重implement IEEE/ACM Ubicomp CSCM 重要性 保证安全性，提升生活质量 在商业上的产品化 推荐书 The Design of Everyday things 核心思想Affordence: 人想象的这个东西的用途和实际的用途，让用户看到这个东西就知道是干什么用的 比如傻屌的看见按键不知道按哪个的瓦斯炉 方向不同的车座靠背调节 根本不知道哪个是哪个的电灯开关 想法 mapping ui to real layout design is stupid 七个准则 by Norman 8 golden rules by. Shneiderman consistency（一致性） -&gt; 比如mac的pull down最下面都是quit，大家的位置都差不多 Short-term memory 比如菜单里面的个数 7+-2 magic number Bringing Design to software T.Winograd GUI &amp; hypermediaGUI CUI -&gt; GUI CUI: I: keyboard O: charracters in display GUI I: keyboard + mouse O: bitmap in display desktop like a real office emvironment (metaphor) document,folder,trash 对于没有用过电脑的人来说非常容易理解 visualizing to icons operating mouse Jobs居然copy了这个东西 跟现在的也没有太多概念（standard interface for computer) pros visual by icon，因为视觉看出来的东西比较好理解 direct manipulation interaction abstract by folders cons number of icons make user confused more computing more physical space typing is faster with keyboard 其他的一些想法 room metaphor[henderson86] different romms for different task multiply desk (就像mbp的多个桌面一样) based on user studys &lt;- how they use each applications 并没有变成主流，哭哭 超整理法 super-organizing metaphor 如何整理物理文件 organize by time, not name sequentially, not hierarchically implemented by Freeman -&gt; Lifestream GUI的一个特征WIMPwindow, icon, menu, pointer（like mouse） 整体来看 苹果把pull down在左上角，因为从左往右拉比较容易 windows: 因为不想和apple一样所以扔到底下了 difficult for icon 比如路上的标志设计的就很迷，大家都不知道是干啥的 direct manipulation 比如在删除东西的时候CHI需要自己输入，但是GUI可以直接拖进trash里面 WYSIWYGwhat you see is what you get PUI?I: recognizationO: large/ small displays PUI(perceptual)GUI -&gt; PUI PUI: using various input(sensors) GUI: mouse&amp; keyboard, not intuitive vision-based HCIwhy? natural &amp; intuitive? not special device, unwired multimodel application recognition detection &amp; recognition object &lt;-(detect) - object detection system(find the thing in the real world) object database &lt;-recognition- objection recognition system(know what it is) detect the hand colors（shapes?） infrared camera near infrared -&gt; vedio cameras(capture near infrared light to the object) far infrared -&gt; capture the heat hand location(手指在哪，手在哪) hand regions find the centre -&gt; morphlogical operation fingers -&gt; pattern matching(有很多不同的方法) tracking gesture recognition difficult segmentation of hands/body -&gt; depth camera recognition of 3D pose(occlusion) 如果用户转身了，手会被其他的东西挡住 detecting begin/end of gestures 一个非常重要的问题！ high-speed gestures (systems) pac！pac！ as many hands as possible advantages robust against light conditions real-time with 40 people with 2 hands No instruction necessary 3D gestures(for navigation in VR) 2 cameras recognise hand shapes pattern classification(NN) object recognition tag-based pre-registration of objects difficult to attached on something(unbralla, glove…) unnartual overlook based on color information(‘1991) 3D histogram(RGB) translation/rotation invariant(如果图片改变了方向或者变了，但是颜色信息还没变) 但是颜色相似的时候没法分辨 PTAM(‘2007) recgonize feature positions features &amp; markers gaze recognition infrared cameras &amp; LEDs pattern matching corners of eyes,mouse,shaping triangle -&gt; face direction 4/18interactive surface 例子 handheld:phone, tablet horizantal: desk vertical: wall digital desk(‘93) overhead projector + camera + desk metaDesk(‘97) -&gt; 用两个奇怪的方块，对这个map进行操作 LCD tabletop LCD -&gt; larger, thinner,lighter,higher resolusion, less expensive before that use projectors(dark) use as window? real glass is expensive then LCD principles of polarization 滤光吗，两个方向的（偏振片） 这样可以用来检测手，把手之后的背景光滤掉 非常好用 可以用来检测手 AR marker这样的东西实在是太丑了 design invisiable markers 把偏振光片减成了ar marker的样子，人看不到但是机器可以识别 background &amp; motivation traditional surfaces are planar &amp; regid difficult to make 3D surface photoelasricity -&gt; 透明的材料对不同载荷下颜色不同 -&gt; 也可以用来作为影响偏振光的因素，可以用来按，按下去光就能过去惹 electrical shock 为什么会有这种电人的display啊！（BIRIBIRI） beyond 2D surfaceCaytrick surface(‘18)4/22 information visualization更快，更精准的理解info(shape/ colors -&gt; information) SciVis &amp; InfoVis Sci 用户比较专业 用来理解专业的现象 physical data, measured data, simulation data Info abstract data 给人民群众看的，感觉更加直观 how to layout the data three issues scalability 如果我们想要vis info，如果data的量太大了，有些东西看起来就很复杂(eg.trees) limited display size human cant understand tech layout scalability filiter layout graph drawing 好看，economically（多级化的，中心辐射，引力型 -&gt; 不同关系的相斥，圆环状） tree structure TreeMap 5/9 Cognitive processwhy important 看到的不一定就是真实的 通过改变HCI，可以改变看到的东西的 Seven Stage Model]]></content>
      <categories>
        <category>HCI</category>
        <category>上课笔记</category>
      </categories>
      <tags>
        <tag>HCI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xcode的breakpoint1.1问题以及打开摄像头]]></title>
    <url>%2F2019%2F04%2F04%2FXcodebreakpoint%2F</url>
    <content type="text"><![CDATA[最近上课又要捡起来c++了，半年前才换的mac用xcode没vs顺手，好几次遇到了挺神奇的错误。 thread 1 breakpoint 1.1这个问题其实就是你在代码里面自己加上了断点(breakpoint)，估计是不小心点到的。取消了断点就行 关于Xcode允许相机xcode的相机许可我之前折腾了一个下午才发现怎么搞。 首先，需要有一个允许相机的Info.plist文件，文件内容如下 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;&lt;plist version="1.0"&gt;&lt;dict&gt;&lt;key&gt;NSCameraUsageDescription&lt;/key&gt;&lt;string&gt;Used to capture new image for photo effect&lt;/string&gt;&lt;key&gt;CFBundleName&lt;/key&gt;&lt;string&gt;$&#123;OPENCV_APPLE_BUNDLE_NAME&#125;&lt;/string&gt;&lt;key&gt;CFBundleIdentifier&lt;/key&gt;&lt;string&gt;$&#123;OPENCV_APPLE_BUNDLE_ID&#125;&lt;/string&gt;&lt;key&gt;CFBundleVersion&lt;/key&gt;&lt;string&gt;$&#123;OPENCV_LIBVERSION&#125;&lt;/string&gt;&lt;key&gt;CFBundleShortVersionString&lt;/key&gt;&lt;string&gt;$&#123;OPENCV_LIBVERSION&#125;&lt;/string&gt;&lt;key&gt;CFBundleSignature&lt;/key&gt;&lt;string&gt;????&lt;/string&gt;&lt;key&gt;CFBundlePackageType&lt;/key&gt;&lt;string&gt;FMWK&lt;/string&gt;&lt;/dict&gt;&lt;/plist&gt; 其中，NSCameraUsageDescription这部分就是打开相机的许可。 但是这个文件直接放在项目里是不行的，需要复制下来，打开products的路径，然后复制到这个路径里面，才可以成功的打开相机]]></content>
      <categories>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>Xcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于SVM的理解]]></title>
    <url>%2F2019%2F04%2F03%2F%E5%85%B3%E4%BA%8ESVM%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本文参考支持向量机通俗导论内容 SVM到底是啥support vecttor machine，比如在二维平面上，要把一个东西分成两类，SVM就是平面上的一条直线，并且在这两类的正中间，离两边一样远。换句话说，学习策略是把间隔最大化，从而得到凸二次规划问题的解（虽然不是很理解，但是凸问题应该是比较好求解） 分类标准 logistic regression 线性分类器：x表示数据，y表示类别，分类器则需要在n维找到一个超平面hyper plane，超平面的方程就是W.T 也就是 W.T.dot(x) + b = 0 (令人震惊w居然是超平面的方程) 逻辑回归 逻辑回归就是从特征里面学到一个0/1的分类模型 模型的线性组合作为自变量，取值范围是负无穷到正无穷，所以使用logistic函数（竟然就是simoid函数把他们投影到（0，1）上面，得到的值就是y = 1的概率 线性分类器如果把分类的两类改成 -1和1（只是为了方便选了这个数字），其实就是把wx加了b 这时候的点的位置可以用 f(x) = wx + b表示，如果f(x)等于0，那么这个点在超平面上，如果大于0就是在1的类型里，小于0在-1的类型里 这时候问题变成了寻找间隔最大的超平面 function margin，geometrical margin函数距离 当平面上的点是 wx+b = 0 确定了以后， wx+b的绝对值就是点x到超平面的距离 同时 wx+b 的符号和 y（分类标签）的符号对比，如果一致的话是一个类别，不一致的话是另一个 -&gt; y(wx+ b)的正负来表示分类的正确与否 （也就是两个东西同号得正分类正确） 引出函数间隔的定义（这里的y是乘上对应类别的y，所以能得到绝对值） 在训练集中，所有点到超平面的距离的最小点就是function margin 几何距离 但是如果单纯这么评定，当w和b成比例改变的时候，函数间隔也会改变，所以还需要几何间隔上面的式子乘以y（对应类别的标签）就可以得到绝对值了。 也就是说几何margin的主要部分就是把之前的内容除了一个w的范数，变成了标准化之后的长度 最大间隔分类器 max margin classifier对于一组数据来说，超平面和数据点的距离越大，这个数据的分类确信度（confidence）就越高 最大的间距的目标函数即： max\gama， 其中gama是比所有其他间隔都短的函数间隔 如果让最小的函数间隔等于1（为了方便计算），然后求几何间隔，可以得知需要的目标函数变为最大化 1/||w||，其中w是超平面 深入SVM线性可分和不可分原始问题和对偶问题duality 之前的目标函数是 1/||w||，所以求这个的最大值，就是求1/2*||w||^2的最小值（这里求最大值就是求倒数的最小值，然后1/2和平方都是为了方便加的） 目标函数变成二次的，约束条件是线性的，凸二次问题，可以用QP（一个写的差不多的包） -&gt; 目标最优的时候loss 由于这个问题的结构，可以转换成对偶问题求解 给每一个约束条件加上一个拉格朗日乘子 alpha 把这个融合进入目标函数里面 当所有的约束条件都满足的时候，目标函数的结果就是之前需要求的目标函数。 再对这个目标函数（新的）求最小值，得到的结果就是本来需要求的最小值 最后，因为上面的问题不是很好求解，把它的max和min交换了一下，先求所有的间隔的最小值，然后再求这里面alpha条件可以满足的最大值，这两个问题就是对偶问题 d &lt;= p ，在某些条件满足的情况下这两个值相等，这时候求出来对偶问题就可以求出来原始问题的解 转换对偶问题的原因： 对偶问题更容易求解 可以引入核函数，这样可以直接引入非线性问题 K.K.T条件 上一段说的，满足对偶问题的解等价的条件就是KKT条件 KTT条件的意义：非线性规划问题（nonlinear processing）能有最优化解法的充要条件 这部分没有写证明，但是上面的求最值的问题可以被证明是满足KKT条件的问题，所以可以用解决对偶问题的方式来求解。 对偶问题的求解步骤参考内容 https://www.zhihu.com/question/21094489 https://blog.csdn.net/v_JULY_v/article/details/7624837]]></content>
      <categories>
        <category>图像处理</category>
        <category>Machine Learning</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>分类器</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment1之Image Features]]></title>
    <url>%2F2019%2F04%2F03%2FCS231Nassignment1Feature%2F</url>
    <content type="text"><![CDATA[目标 之前实现的都是写好了一个linear classifier然后直接对输入图片的raw pixel进行分类 这部分是先从raw data得到相应的图片特征，然后再对特征进行分类 前面的简单的setup和load data都和之前的一样。 Extract Features 对每张图片计算HOG以及在HSV的color space上面的hue channel。（这是两个不同的功能） HOG可以提取图片的texture的特征，忽略颜色的影响。而颜色的histogram表示的是颜色而忽略texture，颜色的特征会拉成一个新的vector然后进行分类。 如果我们把这两个东西结合可能会有更好的结果。 在这部分的代码里面，直接给出来了提取hog feature和color histogram的两个function，用这两个直接提取出了特征然后构成了一个新的extract_features，由图片内容和特征组成。 然后预处理了特征，减去平均值，除以std（这样大家都在同一个scale里面），最后加上了一个bias的dim 12345678910111213141516171819202122232425from cs231n.features import *num_color_bins = 10 # Number of bins in the color histogramfeature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]X_train_feats = extract_features(X_train, feature_fns, verbose=True)X_val_feats = extract_features(X_val, feature_fns)X_test_feats = extract_features(X_test, feature_fns)# Preprocessing: Subtract the mean featuremean_feat = np.mean(X_train_feats, axis=0, keepdims=True)X_train_feats -= mean_featX_val_feats -= mean_featX_test_feats -= mean_feat# Preprocessing: Divide by standard deviation. This ensures that each feature# has roughly the same scale.std_feat = np.std(X_train_feats, axis=0, keepdims=True)X_train_feats /= std_featX_val_feats /= std_featX_test_feats /= std_feat# Preprocessing: Add a bias dimensionX_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))]) 训练SVM来处理features用处理多个类别的SVM来给这些特征分类，得到的结果应该比直接分类得到的结果好。大概结果为0.44左右，注意这里面用的是grid search而不是random search 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# Use the validation set to tune the learning rate and regularization strengthfrom cs231n.classifiers.linear_classifier import LinearSVMlearning_rates = [1e-9, 1e-8, 1e-7]regularization_strengths = [5e4, 5e5, 5e6]results = &#123;&#125;best_val = -1best_svm = None################################################################################# TODO: ## Use the validation set to set the learning rate and regularization strength. ## This should be identical to the validation that you did for the SVM; save ## the best trained classifer in best_svm. You might also want to play ## with different numbers of bins in the color histogram. If you are careful ## you should be able to get accuracy of near 0.44 on the validation set. #################################################################################for lr in learning_rates: for rs in regularization_strengths: svm = LinearSVM() svm.train(X_train_feats, y_train, learning_rate = lr, reg = rs, num_iters = 1000, verbose = True) y_pred_val = svm.predict(X_val_feats) y_pred_train = svm.predict(X_train_feats) train_acc = np.mean(y_pred_train) val_acc = np.mean(y_pred_val == y_val) results[(lr, rs)] = (train_acc,val_acc) if val_acc &gt; best_val: best_val = val_acc best_svm = svm################################################################################# END OF YOUR CODE ################################################################################## Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val) 同时也可视化了不是这个类别却被分到这个类别的错误sample：123456789101112131415161718# An important way to gain intuition about how an algorithm works is to# visualize the mistakes that it makes. In this visualization, we show examples# of images that are misclassified by our current system. The first column# shows images that our system labeled as "plane" but whose true label is# something other than "plane".examples_per_class = 8classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for cls, cls_name in enumerate(classes): idxs = np.where((y_test != cls) &amp; (y_test_pred == cls))[0] idxs = np.random.choice(idxs, examples_per_class, replace=False) for i, idx in enumerate(idxs): plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1) plt.imshow(X_test[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls_name)plt.show() （感觉自己训练了一个傻子） 用两层的nerual net试试看 首先去除上文中bias的dim 然后交叉训练，找到最好的参数 这部分半天loss下不去的原因主要是lr选的太小了 1234567891011121314151617181920212223242526272829303132333435363738394041424344from cs231n.classifiers.neural_net import TwoLayerNetinput_dim = X_train_feats.shape[1]hidden_dim = 500num_classes = 10hidden_size = [300,400,500,600]learning_rate = [1,1e-1,1e-2]reg = [1e-4,1e-3,1e-2]# net = TwoLayerNet(input_dim, hidden_dim, num_classes)best_net = Nonebest_acc = -1result = &#123;&#125;################################################################################# TODO: Train a two-layer neural network on image features. You may want to ## cross-validate various parameters as in previous sections. Store your best ## model in the best_net variable. #################################################################################for lr in learning_rate: for hidd in hidden_size: for rs in reg: net = TwoLayerNet(input_size, hidden, num_class) status = net.train(X_train_feats, y_train, X_val_feats, y_val, num_iters=1200, batch_size=400, learning_rate=lr, learning_rate_decay=0.95, reg=rs, verbose= True) val_acc = (net.predict(X_val_feats) == y_val).mean() result[(lr, rs, hidd)] = (val_acc) if val_acc &gt; best_acc: best_acc = val_acc best_net = net# print(result)# for lr, rs, hidd in sorted(result):# val_accuracy = result[(lr, rs, hidd)]# print('lr %e reg %e hidden_units %e val accuracy: %f' % (# lr, rs, hidd , val_accuracy))print('best validation accuracy achieved during cross-validation: %f' % best_acc)print('best parameter is :',list (result.keys()) [list (result.values()).index (best_acc)])################################################################################# END OF YOUR CODE ################################################################################# best validation accuracy achieved during cross-validation: 0.605000best parameter is : (1, 0.0001, 500) 一点感觉 感觉要是lr太小的话，即使增加iteration的次数，后面的改变也不大 lr最基础的范围应该先定下来 最后换了换参数居然训出来了60%的val正确率 test的正确率在55.8左右]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>Image Feature</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231Nassignment1之two_layer_net]]></title>
    <url>%2F2019%2F04%2F02%2FCS231Nassignment1twolayernet%2F</url>
    <content type="text"><![CDATA[目标 Implement a neural network with fc layers for classifiction Test it on CIFAR-10 dataset 初始化auto-reloading external modules 定义relative error123def rel_error(x, y): """ returns relative error """ return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y)))) 这里插入一下np.max和np.maximum的区别 max是求序列的最值，可以输入一个参数，axis表示的是求最值的方向 maximum至少输入两个参数，会把两个参数逐位比较，然后输出比较大的那个结果 但是好像在这里的使用上面，说明x和y不是一个单独的值，应该是两个数组 12345&gt;&gt; np.max([-4, -3, 0, 0, 9])9&gt;&gt; np.maximum([-3, -2, 0, 1, 2], 0)array([0, 0, 0, 1, 2]) 不是很理解这里为什么要除以x+y 设置参数cs231n/classifiers/neural_net.pyself.params储存了需要的参数，参数都被存储在dict里面，一个名字对应一个内容 两层神经网络的参数如下： W1，第一层的weights，（D，H），其中H是第二层的neruon的个数。因为只有一层的时候，D个输入对应C个输出，现在有两层的fc，对应的输出就是第二层的units个数 b1，第一层的bias，（H，） W2，第二层的weights，（H，C） b2，第二层的biasbias都需要初始化为相应大小的0，weights初始化成0-1之间的比较小的数字 Forward pasa计算scores 这部分非常简单，两次Wx+b，并且在第一次之后记得激活就可以了 激活函数用的relu，内容就是score小于0的部分让他直接等于0 计算loss 这里用的是softmax计算loss，和softmax的作业内容一样，将所有的scores exp，求占的百分比，求出来的部分-log，然后把所有的求和 这里用到了boardcasting的问题，注意（100，1）这样的才可以boardcasting，（100，）的是一维数组，需要把它reshape成前面的样子才可以 这里最后的结果还总是差一点，最后发现是因为regularzation的时候多乘了0.5，看题呜呜呜 Backward pass 由于b是线性模型的bias，偏导数是1，直接对class的内容求和然后除以N就是最终结果 对W求导的时候需要用到链式法则，然后直接代码实现一下就行了 这里遇到的主要问题是loss的值会影响他估计的值，因为loss的regularzation改了，所以答案一直对不上。 Training predict 训练和之前写的差不多，训练网络，主要包括写training部分的随机mini-batch和更新weights，记得lr更新的时候要带负号 预测也差不多，算出来scores，找到最大的score就是分类的结果。注意找最大的时候要用argmax，找到的是最大的东西的indice，不然得到的是得分12345678910111213net = init_toy_model()stats = net.train(X, y, X, y, learning_rate=1e-1, reg=5e-6, num_iters=100, verbose=False)print('Final training loss: ', stats['loss_history'][-1])# plot the loss historyplt.plot(stats['loss_history'])plt.xlabel('iteration')plt.ylabel('training loss')plt.title('Training Loss history')plt.show() 使用写好的来训练CIFAR-101234567891011121314input_size = 32 * 32 * 3hidden_size = 50num_classes = 10net = TwoLayerNet(input_size, hidden_size, num_classes)# Train the networkstats = net.train(X_train, y_train, X_val, y_val, num_iters=1000, batch_size=200, learning_rate=1e-4, learning_rate_decay=0.95, reg=0.25, verbose=True)# Predict on the validation setval_acc = (net.predict(X_val) == y_val).mean()print('Validation accuracy: ', val_acc) 这时候得到的准确度应该在28%左右，可以优化 进一步优化 一种可视化的方法是可视化loss function和准确率的关系，分别在训练和val集上面 另种是可视化第一层的weights 两种方法的结果如下： debug模型 问题 loss大体上都是linearly的下降的，说明lr可能太低了 在training和val的准确率上没有gap，说明model的容量太小的，需要增大size 如果容量过大还会导致overfiiting，这时候gap就会很大 tuning hypers 题目里面的建议是tuning几个hyper，还是和之前一样，直接random，search 这里选了三个参数，分别是units的数量，learning rate和reg的强度，随便设置了一下界限 最终计算出来的val准确率是：49.5% 秀秀秀！！ 可视化weigh之后的结果是 震惊，居然最后的测试正确率也达到了49.4！！！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051best_net = None # store the best model into this ################################################################################## TODO: Tune hyperparameters using the validation set. Store your best trained ## model in best_net. ## ## To help debug your network, it may help to use visualizations similar to the ## ones we used above; these visualizations will have significant qualitative ## differences from the ones we saw above for the poorly tuned network. ## ## Tweaking hyperparameters by hand can be fun, but you might find it useful to ## write code to sweep through possible combinations of hyperparameters ## automatically like we did on the previous exercises. ##################################################################################best_acc = -1learning_rates = [1e-3, 1e-2]regularization_strengths = [1e-2, 6e-1]hidden_size = [50, 150]random_search = np.random.rand(30, 3)random_search[:, 0] = random_search[:, 0] * \ (learning_rates[1] - learning_rates[0]) + learning_rates[0]random_search[:, 1] = random_search[:, 1] * \ (regularization_strengths[1] - regularization_strengths[0] ) + regularization_strengths[0]random_search[:, 2] = random_search[:, 2] * \ (hidden_size[1] - hidden_size[0]) + hidden_size[0]for lr, rs, hidd in random_search: input_size = 32 * 32 * 3 hidden = int(hidd) num_class = 10 net = TwoLayerNet(input_size, hidden, num_class) status = net.train(X_train, y_train, X_val, y_val, num_iters=1000, batch_size=200, learning_rate=lr, learning_rate_decay=0.95, reg=rs, verbose=True) val_acc = (net.predict(X_val) == y_val).mean() if val_acc &gt; best_acc: best_acc = val_acc best_net = netprint("best net is with val acc", best_acc)################################################################################## END OF YOUR CODE ################################################################################## 代码部分nerual_net.py部分的完整代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289from __future__ import print_functionimport numpy as npimport matplotlib.pyplot as pltclass TwoLayerNet(object): """ A two-layer fully-connected neural network. The net has an input dimension of N, a hidden layer dimension of H, and performs classification over C classes. We train the network with a softmax loss function and L2 regularization on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture: input - fully connected layer - ReLU - fully connected layer - softmax The outputs of the second fully-connected layer are the scores for each class. """ def __init__(self, input_size, hidden_size, output_size, std=1e-4): """ Initialize the model. Weights are initialized to small random values and biases are initialized to zero. Weights and biases are stored in the variable self.params, which is a dictionary with the following keys: W1: First layer weights; has shape (D, H) b1: First layer biases; has shape (H,) W2: Second layer weights; has shape (H, C) b2: Second layer biases; has shape (C,) Inputs: - input_size: The dimension D of the input data. - hidden_size: The number of neurons H in the hidden layer. - output_size: The number of classes C. """ self.params = &#123;&#125; self.params['W1'] = std * np.random.randn(input_size, hidden_size) self.params['b1'] = np.zeros(hidden_size) self.params['W2'] = std * np.random.randn(hidden_size, output_size) self.params['b2'] = np.zeros(output_size) def loss(self, X, y=None, reg=0.0): """ Compute the loss and gradients for a two layer fully connected neural network. Inputs: - X: Input data of shape (N, D). Each X[i] is a training sample. - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it is not passed then we only return scores, and if it is passed then we instead return the loss and gradients. - reg: Regularization strength. Returns: If y is None, return a matrix scores of shape (N, C) where scores[i, c] is the score for class c on input X[i]. If y is not None, instead return a tuple of: - loss: Loss (data loss and regularization loss) for this batch of training samples. - grads: Dictionary mapping parameter names to gradients of those parameters with respect to the loss function; has the same keys as self.params. """ # Unpack variables from the params dictionary W1, b1 = self.params['W1'], self.params['b1'] W2, b2 = self.params['W2'], self.params['b2'] N, D = X.shape # Compute the forward pass scores = None ############################################################################# # TODO: Perform the forward pass, computing the class scores for the input. # # Store the result in the scores variable, which should be an array of # # shape (N, C). # ############################################################################# # first layer, shape(N,H) X1 = X.dot(W1) + b1 # 这里加了一个第一层之后的relu激活 relu = np.maximum(0, X1) # final result, shape(N,C) scores = relu.dot(W2) + b2 ############################################################################# # END OF YOUR CODE # ############################################################################# # If the targets are not given then jump out, we're done if y is None: return scores # Compute the loss loss = None ############################################################################# # TODO: Finish the forward pass, and compute the loss. This should include # # both the data loss and L2 regularization for W1 and W2. Store the result # # in the variable loss, which should be a scalar. Use the Softmax # # classifier loss. # ############################################################################# num_train = N scores = scores - np.reshape(np.max(scores, axis=1), (num_train, -1)) scores = np.exp(scores) scores_sum = np.sum(scores, axis=1).reshape(N, 1) # scores_sum = np.sum(scores, axis=1, keepdims=True) p = scores / scores_sum loss = np.sum(-np.log(p[np.arange(N), y])) loss /= num_train # 这里不要乘0.5的系数 # loss += reg * np.sum(W1 * W1) + reg * np.sum(W2 * W2) loss += 0.5 * reg * np.sum(W1 * W1) + 0.5 * reg * np.sum(W2 * W2) # ############################################################################# # # END OF YOUR CODE # # ############################################################################# # # Backward pass: compute gradients grads = &#123;&#125; # ############################################################################# # # TODO: Compute the backward pass, computing the derivatives of the weights # # # and biases. Store the results in the grads dictionary. For example, # # # grads['W1'] should store the gradient on W1, and be a matrix of same size # # ############################################################################# dscores = p dscores[range(N), y] -= 1.0 # dscores /= N # shape dW2(CxN) x(NxH) -&gt; (CxH) # dW2 = np.dot(relu.T, p) dW2 = np.dot(relu.T, dscores) # print(dW2) # 每个class会有一个b，对b求导是1 # shape db2 (C,) db2 = np.sum(p, axis=0) # (NxC) x (HxC).T -&gt; (N,H) dW_relu = np.dot(dscores, W2.T) dW_relu[relu &lt;= 0] = 0 # (NxD).T x (N,H) -&gt; (D,H) dW1 = (X.T).dot(dW_relu) db1 = np.sum(dW_relu, axis=0) dW2 /= N dW1 /= N dW2 += reg * W2 dW1 += reg * W1 db1 /= N db2 /= N grads['W1'] = dW1 grads['b1'] = db1 grads['W2'] = dW2 grads['b2'] = db2 ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, grads def train(self, X, y, X_val, y_val, learning_rate=1e-3, learning_rate_decay=0.95, reg=5e-6, num_iters=100, batch_size=200, verbose=False): """ Train this neural network using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) giving training data. - y: A numpy array f shape (N,) giving training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - X_val: A numpy array of shape (N_val, D) giving validation data. - y_val: A numpy array of shape (N_val,) giving validation labels. - learning_rate: Scalar giving learning rate for optimization. - learning_rate_decay: Scalar giving factor used to decay the learning rate after each epoch. - reg: Scalar giving regularization strength. - num_iters: Number of steps to take when optimizing. - batch_size: Number of training examples to use per step. - verbose: boolean; if true print progress during optimization. """ num_train = X.shape[0] iterations_per_epoch = max(num_train / batch_size, 1) # Use SGD to optimize the parameters in self.model loss_history = [] train_acc_history = [] val_acc_history = [] for it in range(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: Create a random minibatch of training data and labels, storing # # them in X_batch and y_batch respectively. # ######################################################################### rand_mini = np.random.choice(num_train, batch_size, replace=True) X_batch = X[rand_mini] y_batch = y[rand_mini] ######################################################################### # END OF YOUR CODE # ######################################################################### # Compute loss and gradients using the current minibatch loss, grads = self.loss(X_batch, y=y_batch, reg=reg) loss_history.append(loss) ######################################################################### # TODO: Use the gradients in the grads dictionary to update the # # parameters of the network (stored in the dictionary self.params) # # using stochastic gradient descent. You'll need to use the gradients # # stored in the grads dictionary defined above. # ######################################################################### self.params['W1'] -= learning_rate * grads['W1'] self.params['W2'] -= learning_rate * grads['W2'] self.params['b1'] -= learning_rate * grads['b1'] self.params['b2'] -= learning_rate * grads['b2'] ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print('iteration %d / %d: loss %f' % (it, num_iters, loss)) # Every epoch, check train and val accuracy and decay learning rate. if it % iterations_per_epoch == 0: # Check accuracy train_acc = (self.predict(X_batch) == y_batch).mean() val_acc = (self.predict(X_val) == y_val).mean() train_acc_history.append(train_acc) val_acc_history.append(val_acc) # Decay learning rate learning_rate *= learning_rate_decay return &#123; 'loss_history': loss_history, 'train_acc_history': train_acc_history, 'val_acc_history': val_acc_history, &#125; def predict(self, X): """ Use the trained weights of this two-layer network to predict labels for data points. For each data point we predict scores for each of the C classes, and assign each data point to the class with the highest score. Inputs: - X: A numpy array of shape (N, D) giving N D-dimensional data points to classify. Returns: - y_pred: A numpy array of shape (N,) giving predicted labels for each of the elements of X. For all i, y_pred[i] = c means that X[i] is predicted to have class c, where 0 &lt;= c &lt; C. """ y_pred = None ########################################################################### # TODO: Implement this function; it should be VERY simple! # ########################################################################### W1 = self.params['W1'] W2 = self.params['W2'] b1 = self.params['b1'] b2 = self.params['b2'] scores = X.dot(W1) + b1 scores[scores &lt; 0] = 0.0 scores = scores.dot(W2) + b2 y_pred = np.argmax(scores, axis=1) ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于Normalization的方法以及实现]]></title>
    <url>%2F2019%2F04%2F01%2FNormalize%2F</url>
    <content type="text"><![CDATA[在处理数据的时候，因为数据的大小差别会比较大，为了避免数据的特征被其他特征吃掉，需要对数据进行normalization的处理 (0,1) 标准化找到最大值和最小值，以最大值为1，最小值为0，计算其他数据在0到1之间的分布。 12def normal0_1(x,Max,Min): return (x-Min)/(Max-Min) 使用np.max()，np.min来找最大值和最小值 正态分布输入原始数据的均值和标准差，对数据处理，处理之后的数据是标准正态分布（均值是0，标准差是1） 12def Normalization(x, mu, sigma): return (x-mu) / sigma 使用np.average()和np.std()找到均值和标准差 Sigmoid函数sigmoid函数关于（0， 0.5）中心对称，在中心附近斜率较大，在负无穷接近0，正无穷接近1 12def sigmood(x): return 1.0/(1+np.exp(-float(x)))]]></content>
      <categories>
        <category>数学问题</category>
      </categories>
      <tags>
        <tag>Normalize</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231N作业assignment1之softmax]]></title>
    <url>%2F2019%2F04%2F01%2FCS231Nassignment1softmax%2F</url>
    <content type="text"><![CDATA[Softmax这部分主要是softmax的loss要如何计算Assignment From: Assignment1 目标 implement a fully-vectorized loss function for the Softmax classifier implement the fully-vectorized expression for its analytic gradient check your implementation with numerical gradient use a validation set to tune the learning rate and regularization strength optimize the loss function with SGD visualize the final learned weights 预处理（和之前一样（ 载入数据 初始化数据 拉长 normalize 分成训练集测试集validation等等 softmax classifiernaive_softmax_loss中心思想：把得到的score（Wx + b）先exp，然后normalize，最后求-log 输入： W：大小(D,C)，weights X：大小(N,D)，输入的mini-batch y：大小(N,)，标签 reg：regularization的系数 输出： loss dW，即改变的gradient 计算loss 先将所有的scores做exp（这一步可以先进行），这样所有的score都会变成正数 然后对不同class的score分别求normalize（虽然说是normalize，实际求的是这个种类的score在所有的score里面所占的比例） 然后将正确的类型所占的比例求log，再求负号，得出来的就是每个图片的loss（这里注意0的log是无穷，计算不出来） 所有图片的loss求和，然后除以图片总数，regularzation，得出来的就是最终的结果 计算dW 可以这样理解 W是一个参数矩阵，这个矩阵的变化由两个部分组成 第一部分是往什么方向变，这个取决于最后算出来的loss的分布 第二部分是变多少合适，这时候还需要乘一个系数X[i] 所以当算出来loss并且y[i] = j的时候，实际上就是这张图正确分类情况下的错误分类的概率，所以W的改变方向应该是这个的反方向 这张图的其他class的loss则应该是改变的方向 这样就可以看出来 SVM和softmax的不同之处了 对于SVM来说，仅仅通过与0比大小得出一个值，相当于一个0，1的开关，只能根据结果得到一个移动的方向 但是对于softmax来说，不仅得到了方向，还得到了这个方向的占比，所以loss越大的数影响就会越大 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def svm_loss_naive(W, X, y, reg): """ Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] # 有多少需要训练的个数 num_train = X.shape[0] loss = 0.0 for i in range(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in range(num_classes): # 如果这个类型是正确的，那就不用管了 if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:, y[i]] += -X[i] dW[:, j] += X[i] # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. # reg是lanbda loss += reg * np.sum(W * W) dW += 2 * reg * W ############################################################################# # TODO: # # Compute the gradient of the loss function and store it dW. # # Rather that first computing the loss and then computing the derivative, # # it may be simpler to compute the derivative at the same time that the # # loss is being computed. As a result you may need to modify some of the # # code above to compute the gradient. # ############################################################################# return loss, dW softmax_loss_vectorized提高计算速度 跟svm部分的计算思路一样，直接使用矩阵运算 在求整个score矩阵的变化的时候，正确分类的loss应该被减掉，但是现在是被加上的，所以需要在正确分类的地方加一个-1 debug了很久的地方是：计算dW的时候不需要计算log，因为没有log之前已经是这个loss所占的百分比了：求log是为了变成凸函数，loss没有求log之前并不是凸函数，但是凸函数容易找到最值的优化问问题，所以要求log。但是在计算dW的时候和log没关系 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def softmax_loss_vectorized(W, X, y, reg): """ Softmax loss function, vectorized version. Inputs and outputs are the same as softmax_loss_naive. """ # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) num_class = W.shape[1] num_train = X.shape[0] ############################################################################# # TODO: Compute the softmax loss and its gradient using no explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# # size（N，C） scores = X.dot(W) scores = np.exp(scores) # 对每行求和 scores_sum = np.sum(scores, axis=1) scores_sum = np.repeat(scores_sum, num_class) scores_sum = scores_sum.reshape(num_train, num_class) # true_divide返回浮点数，普通的返回正数，size（N，C） percent = np.true_divide(scores, scores_sum) # 只有正确种类需要求loss Li = -np.log(percent[np.arange(num_train), y]) loss = np.sum(Li) # 注意这里不需要求log dS = percent.copy() dS[np.arange(num_train), y] += -1 dW = (X.T).dot(dS) loss /= num_train loss += reg * np.sum(W * W) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW 验证，选hyper和SVM的部分一样，随机搜索hyper，验证结果，训练迭代500次，最终的准确率在36%左右1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of over 0.35 on the validation set.from cs231n.classifiers import Softmaxresults = &#123;&#125;best_val = -1best_softmax = Nonelearning_rates = [1e-7, 5e-7]regularization_strengths = [2.5e4, 5e4]################################################################################# TODO: ## Use the validation set to set the learning rate and regularization strength. ## This should be identical to the validation that you did for the SVM; save ## the best trained softmax classifer in best_softmax. #################################################################################hyper_values = np.random.rand(50,2)hyper_values[:,0] = (learning_rates[1] - learning_rates[0]) * hyper_values[:,0] + learning_rates[0]hyper_values[:,1] = (regularization_strengths[1] - regularization_strengths[0]) * hyper_values[:,1] + regularization_strengths[0]for lr, rs in hyper_values: softmax = Softmax() softmax.train(X_train,y_train,lr,rs,num_iters = 500,verbose = True) train_pred = softmax.predict(X_train) train_acc = np.mean(y_train == train_pred) val_pred = softmax.predict(X_val) val_acc = np.mean(y_val == val_pred) results[(lr,rs)] = (train_acc,val_acc) if val_acc &gt; best_val: best_val = val_acc best_softmax = softmax################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val) 可以看出来感觉softmax比SVM的效果好一些？可视化最终的优化的weight123456789101112131415# Visualize the learned weights for each classw = best_softmax.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in range(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i])]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于标量，向量，矩阵求导]]></title>
    <url>%2F2019%2F03%2F30%2F%E5%85%B3%E4%BA%8E%E6%A0%87%E9%87%8F%E5%90%91%E9%87%8F%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[参考文章：https://blog.csdn.net/u010976453/article/details/54381248 关于layout在求导的时候有，因为分子和分母可能的维度不太一样，所以两种不同的布局，分别是分子布局和分母布局假设y（向量）对x（标量）求导： 分子布局，即和原来的y相同 分母布局，为分子布局的tranpose 对标量的导数scalar对scalar求导即最简单的求导 vector对scalar求导比如一个列向量y，对x求导，结果是y里面的每个值都对x求导 matrix对scalr求导矩阵里面的每个值都对x求导 对向量的导数scalar对vector 标量y和向量x，求出来的结果是y对每个x(x1,x2 ….xn)求导 结果为梯度向量，是标量y在空间Rn的梯度，空间以x为基 注意，x是列向量的话，最后求出来的是行的结果 vector对vectory = [y1,y2 …. ym]x = [x1,x2 …. xn]最后求出来的结果是一个m行n列的矩阵，jacobian矩阵 matrix对vector矩阵y =[[y11,y12…y1n],[y21,y22 …y2n],…[yn1,yn2 …ynn]]向量x = [x1,x2…xn]T最终的结果是每一行分别对这个x的向量求导，所以矩阵的列数和向量的行数应该先通 对于矩阵一般只考虑标量对矩阵(剩下的情况和上面类似)最终结果是这个标量对所有的矩阵内容求导，求出来的是梯度矩阵]]></content>
      <categories>
        <category>数学问题</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS231N作业assignment1之SVM部分]]></title>
    <url>%2F2019%2F03%2F29%2FCS231Nassignment1SVM%2F</url>
    <content type="text"><![CDATA[Assignment from: http: // cs231n.github.io / assignments2018 / assignment1/ 目标： a fully - vectorized loss function for the SVM fully - vectorized expression for its analytic gradient use a validation set to tune the learning rate and regularization strength optimize the loss function with SGD visualize the final learned weights Set up部分1234567891011121314151617181920# Run some setup code for this notebook.from __future__ import print_functionimport randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt# This is a bit of magic to make matplotlib figures appear inline in the# notebook rather than in a new window.%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# Some more magic so that the notebook will reload external python modules;# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2 读取CIFAR-10的数据，预处理123456789101112131415161718# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)try: del X_train, y_train del X_test, y_test print('Clear previously loaded data.')except: passX_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print('Training data shape: ', X_train.shape)print('Training labels shape: ', y_train.shape)print('Test data shape: ', X_test.shape)print('Test labels shape: ', y_test.shape) 结果：1234Training data shape: (50000, 32, 32, 3)Training labels shape: (50000,)Test data shape: (10000, 32, 32, 3)Test labels shape: (10000,) 可视化dataset 从类型中1234567891011121314151617# Visualize some examples from the dataset.# We show a few examples of training images from each class.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']num_classes = len(classes)samples_per_class = 7for y, cls in enumerate(classes): idxs = np.flatnonzero(y_train == y) idxs = np.random.choice(idxs, samples_per_class, replace=False) for i, idx in enumerate(idxs): plt_idx = i * num_classes + y + 1 plt.subplot(samples_per_class, num_classes, plt_idx) plt.imshow(X_train[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls)plt.show() 1np.flatnonzero(y_train == y) 返回内容非0的index。这句是返回plane类别里面的（y_train == y）所有非0的内容。然后从这些里面随机选择7个内容，画出来。 结果如下： 进一步分为几部分123456789101112131415161718192021222324252627282930313233343536373839# Split the data into train, val, and test sets. In addition we will# create a small development set as a subset of the training data;# we can use this for development so our code runs faster.num_training = 49000num_validation = 1000num_test = 1000# 用这部分来优化代码num_dev = 500# Our validation set will be num_validation points from the original# training set.mask = range(num_training, num_training + num_validation)X_val = X_train[mask]y_val = y_train[mask]# Our training set will be the first num_train points from the original# training set.mask = range(num_training)X_train = X_train[mask]y_train = y_train[mask]# We will also make a development set, which is a small subset of# the training set.mask = np.random.choice(num_training, num_dev, replace=False)X_dev = X_train[mask]y_dev = y_train[mask]# We use the first num_test points of the original test set as our# test set.mask = range(num_test)X_test = X_test[mask]y_test = y_test[mask]print('Train data shape: ', X_train.shape)print('Train labels shape: ', y_train.shape)print('Validation data shape: ', X_val.shape)print('Validation labels shape: ', y_val.shape)print('Test data shape: ', X_test.shape)print('Test labels shape: ', y_test.shape) 12mask = range(num_test)X_test = X_test[mask] 感觉这是一种从一个整体中选取其中一部分的代码 将image拉成row1234567891011# Preprocessing: reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1))X_val = np.reshape(X_val, (X_val.shape[0], -1))X_test = np.reshape(X_test, (X_test.shape[0], -1))X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))# As a sanity check, print out the shapes of the dataprint('Training data shape: ', X_train.shape)print('Validation data shape: ', X_val.shape)print('Test data shape: ', X_test.shape)print('dev data shape: ', X_dev.shape) 当想把无论任何大小的东西拉成一整行的时候，用a.reshape(x, -1)。 X_train.shape[0]行，列数未知，但是拉平了 如果想拉成一整列的时候，用a.reshape(-1, x)。 列数为x，每列有多少东西未知 预处理部分：减去mean image 第一步，求出训练集的mean并且可视化 12345678# Preprocessing: subtract the mean image# first: compute the image mean based on the training datamean_image = np.mean(X_train, axis=0)print(mean_image[:10]) # print a few of the elementsplt.figure(figsize=(4, 4))plt.imshow(mean_image.reshape((32, 32, 3)).astype( 'uint8')) # visualize the mean imageplt.show() 第二步，从train和test里面减去平均数据 12345# second: subtract the mean image from train and test dataX_train -= mean_imageX_val -= mean_imageX_test -= mean_imageX_dev -= mean_image 第三步，把预处理好的所有图片的末尾（拉成行之后的最后）加了一个1（bias的dim） 12345678# third: append the bias dimension of ones (i.e. bias trick) so that our SVM# only has to worry about optimizing a single weight matrix W.X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape) np.hstack()，沿着水平方向把数组叠起来。于此相同，np.vstack()，是沿着垂直方向把数组叠起来。 SVM classifier1cs231n / classifiers / linear_svm.py. svm_loss_naive 有三个输入 X：一个有N个元素的minibatch，每个元素的内容是D(N, D) W: weights，(D, C), 图片的内容是D，一共C个class，所以用的时候跟普遍想法的W是tranpose的 y: 标签，大小(N,) 一共N张照片，每张照片有一个标签 最终结果 一个float的结果：loss W的gradient dW 注意，Wx求出来的就是不同分类的积分 dW的计算(https://blog.csdn.net/zt_1995/article/details/62227201) 形状很奇怪的1(x)指的是，当x为真的时候结果是1，当x为假的时候结果取0 第一个式子表示第i个被正确分类的梯度 有多少个Wj让这个边界值不被满足，就对损失起了多少贡献 乘以xi是因为xi包含了样本的全部特征，所以前面乘以一个系数1就可以了 符号是因为SGD采用负梯度运算 第二个式子表示不正确分类的梯度，只有在yi == j的时候才有贡献，所以没有求和。但是注意，在每张图里面，这个都会在j == yi的时候发生一次，所以每张图的j部分需要加上这个值 最终的结果需要，除以N 别忘了正则化！而且用2\lanmdaW来正则化的效果更好一些 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def svm_loss_naive(W, X, y, reg): """ Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] # 有多少需要训练的个数 num_train = X.shape[0] loss = 0.0 for i in range(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in range(num_classes): # 如果这个类型是正确的，那就不用管了 if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:, y[i]] += -X[i] dW[:, j] += X[i] # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. # reg是lanbda loss += reg * np.sum(W * W) dW += 2 * reg * W return loss, dW svm_loss_vectorized通过向量化来提高计算速度 计算loss部分 W是一个(D, C)的向量，X是(N, D)的，所以两者相乘可以得到一个(N, C)的矩阵，N为图片数量，C是每张图片对于不同分类的score 在score中取每一行的y中label部分就是这张图正确类型的评分 把整体的score矩阵的所有项减去正确评分的矩阵（应该可以广播但是我刚开始用repeat和reshape复制了一下），减去的结果就是svm中需要和0比的值（margin） 为了求loss，把小于0的项目和正确的项除去（都设置成0） 然后行求和，列求和，除以整体的个数，regularzation 计算dW部分 X.T点乘margin得到的就是最终的loss，所以需要把每个margin里面符合条件的数对了 所有比0大的时候都算1（根据导数的计算结果） 当应该判断正确的类型比0大的时候，这个东西会在每次计算导数的时候都算上一次，所以是行的合 最后乘完之后除以总的个数，再regularzation1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation. Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero ############################################################################# # TODO: # # Implement a vectorized version of the structured SVM loss, storing the # # result in loss. # ############################################################################# num_train = X.shape[0] num_classes = W.shape[1] scores = X.dot(W) # 这里是取第N行（图片行）的第C个（class列），得到的是（500，）的正确类的score的矩阵 correct_class_score = scores[np.arange(num_train), y] # correct_class_score = np.repeat(correct_class_score, num_classes) # correct_class_score = correct_class_score.reshape(num_train, num_classes) # DxC margin = scores - correct_class_score + 1.0 margin[np.arange(num_train), y] = 0.0 margin[margin &lt;= 0] = 0.0 loss += np.sum(np.sum(margin, axis=1)) / num_train # loss /= num_train loss += 0.5 * reg * np.sum(W * W) margin[margin &gt; 0] = 1.0 calculate_times = np.sum(margin, axis=1) margin[np.arange(num_train), y] = - calculate_times dW = np.dot(X.T, margin) / num_train dW += 2 * reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW 现在得到了dW和loss，使用SGD来减少loss训练 将整体分成不同的minibatch，使用np.random.choice，注意后面的replce可以选True，这样会重复选择元素但是结果速度好像是更快了 将minibatch的结果计算loss和gradient，然后grad * learning rate来update数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): """ Train this linear classifier using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label 0 &lt;= c &lt; C for C classes. - learning_rate: (float) learning rate for optimization. - reg: (float) regularization strength. - num_iters: (integer) number of steps to take when optimizing - batch_size: (integer) number of training examples to use at each step. - verbose: (boolean) If true, print progress during optimization. Outputs: A list containing the value of the loss function at each training iteration. """ num_train, dim = X.shape # assume y takes values 0...K-1 where K is number of classes num_classes = np.max(y) + 1 if self.W is None: # lazily initialize W self.W = 0.001 * np.random.randn(dim, num_classes) # Run stochastic gradient descent to optimize W loss_history = [] for it in range(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: # # Sample batch_size elements from the training data and their # # corresponding labels to use in this round of gradient descent. # # Store the data in X_batch and their corresponding labels in # # y_batch; after sampling X_batch should have shape (dim, batch_size) # # and y_batch should have shape (batch_size,) # # # # Hint: Use np.random.choice to generate indices. Sampling with # # replacement is faster than sampling without replacement. # ######################################################################### indices = np.random.choice(num_train, batch_size, replace=True) X_batch = X[indices] y_batch = y[indices] ######################################################################### # END OF YOUR CODE # ######################################################################### # evaluate loss and gradient loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) # perform parameter update ######################################################################### # TODO: # # Update the weights using the gradient and the learning rate. # ######################################################################### self.W += - learning_rate * grad ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print('iteration %d / %d: loss %f' % (it, num_iters, loss)) return loss_history 预测结果 已经有了前面的到的训练过的W（self.W） Wx算出来的就是分数 从每一行里面选择最大的分数就是预测的结果123456789101112131415161718192021222324252627def predict(self, X): """ Use the trained weights of this linear classifier to predict labels for data points. Inputs: - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D. Returns: - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional array of length N, and each element is an integer giving the predicted class. """ y_pred = np.zeros(X.shape[0]) ########################################################################### # TODO: # # Implement this method. Store the predicted labels in y_pred. # ########################################################################### scores = X.dot(self.W) y_pred = np.argmax(scores, axis=1) # print(labels.shape) # print(labels) ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred 交叉验证 在作业里，需要选择两个hyper的值，分别是学习率和regularzation的参数，没有采用交叉验证，但是采用了随机搜索，会比grid search更准确一些 采用不同的参数组合分别训练这个模型，然后得到各自在validation上面的准确率，这个得到准确率最大的组合的参数 注意，在验证的过程中应该选择iter的次数少一点，不然训练的时间会非常长 在这个代码里用了rand来得到0到1之间的随机数，这个数乘以hyper的范围的差，然后再加上下限，就是随机得到的最终结果 1234567891011121314rand_turple = np.random.rand(50,2)rand_turple[:,0] = rand_turple[:,0]*(learning_rates[1]-learning_rates[0]) + learning_rates[0]rand_turple[:,1] = rand_turple[:,1]*(regularization_strengths[1]-regularization_strengths[0])+regularization_strengths[0]for lr,rs in rand_turple: svm = LinearSVM() svm.train(X_train, y_train, learning_rate=lr, reg=rs,num_iters=1500, verbose=True) y_train_pred = svm.predict(X_train) train_acc = np.mean(y_train == y_train_pred) y_val_pred = svm.predict(X_train) val_acc = np.mean(y_train == y_val_pred) results[(lr,rs)] = (train_acc,val_acc) if (val_acc &gt; best_val): best_val = val_acc best_svm = svm 结果可视化123456789101112131415# Visualize the learned weights for each class.# Depending on your choice of learning rate and regularization strength, these may# or may not be nice to look at.w = best_svm.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in range(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i])]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
        <category>CS231n作业</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>SGD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TypeError:'method' object is not subscriptable]]></title>
    <url>%2F2019%2F03%2F26%2Fmethod%E4%B8%8D%E6%98%AFsubscripatable%2F</url>
    <content type="text"><![CDATA[遭遇问题TypeError: ‘method’ object is not subscriptable是因为我本来写了一个class的method123def get_page(self, num):num = int(num)return self.pages[num] 但是在调用的时候我用了12get_page[i]get_page(i) #这样才是正确的 找到报错改括号就行了！]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[enumerate枚举]]></title>
    <url>%2F2019%2F03%2F25%2Fenumerate%E6%9E%9A%E4%B8%BE%2F</url>
    <content type="text"><![CDATA[enumerate()枚举对可迭代的数据进行标号并将其里面的数据和标号一并打印出来。1enumerate(iterable, start=0) iterable: 可迭代的数据，比如list start: 打印的初始值，默认从0开始打印 123test = [[11], [21], [31], [41]]for i, cnt in enumerate(test):print(i, cnt) 结果为12340 [11]1 [21]2 [31]3 [41]]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python的None和if的理解]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E7%9A%84None%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[python对变量None的判断None是一种数据类型！！！12&gt;&gt;&gt;type(None)&lt;class 'NoneType'&gt; 说明该值是一个空的对象，是Python里面的特殊的值，跟NULL不一样，跟0也不一样 123456a = Noneb = []if a is None or b is None:print("yahaha")else:print("wocao") 结果为“yahaha” 注意：在if的情况下，使用None有时候可以起到很好的作用1if a is None: 与这个差不多的用法是1if not a: 在python里面，None，空列表[]，字典{},tuple()，0等都会被转化成false，剩下的为true比如：12345a = Noneif a:print("yahaha")else:print("wocao") 这时候的输出是wocao，因为a被认为是false]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[np.delete删除数组内容]]></title>
    <url>%2F2019%2F03%2F25%2Fnp-delete%E5%88%A0%E9%99%A4%E6%95%B0%E7%BB%84%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[np.deletenumpy.delete(arr, obj, axis=None) 返回一个新的array，删除掉obj，沿着axis方向 axis : int, optional The axis along which to delete the subarray defined by obj. If axis is None, obj is applied to the flattened array.(如果不加上axis的话会自动把这个array拉平) axis = 0：删除数组的行 axis = 1: 删除数组的列 axis = none: 把整个数组平铺之后按索引删除 123456789101112import numpy as npids = [[3], [34], [5]]ids_o = [[3], [31]]remove_list = filter(lambda i: i not in ids, ids_o)# print(np.asarray(ids))for i in remove_list:index = np.where(np.asarray(ids_o) == i)[0]print("index = ", index)ids = np.delete(ids, index, axis = 0)print("new ids = \n", ids) 结果：1234index = [1]new ids = [[3][5]] 如果把上面改成1ids = np.delete(ids, 0, axis = 1) 即为删除数组的第0列，结果是 [ ] （因为只有一列） 如果改成1ids = np.delete(ids, index, axis = None) 结果为：12new ids = [3 5]]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[np.where查找索引]]></title>
    <url>%2F2019%2F03%2F25%2Fnp-where%E6%9F%A5%E6%89%BE%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[np.wherenp.where(condition, x, y)满足条件(condition)，输出x，不满足输出y。123np.where([[True,False], [True,True]], # 官网上的例子[[1,2], [3,4]],[[9,8], [7,6]]) 输出12array([[1, 8],[3, 4]]) 上面这个例子的条件为[[True,False], [True,False]]，分别对应最后输出结果的四个值。第一个值从[1,9]中选，因为条件为True，所以是选1。第二个值从[2,8]中选，因为条件为False，所以选8，后面以此类推这里的true指的就是选前面的，false就是指选后面的 1234567&gt;&gt;&gt; a = 10&gt;&gt;&gt; np.where([[a &gt; 5,a &lt; 5], [a == 10,a == 7]],[["chosen","not chosen"], ["chosen","not chosen"]],[["not chosen","chosen"], ["not chosen","chosen"]])array([['chosen', 'chosen'],['chosen', 'chosen']], dtype='&lt;U10') np.where(condition)只有条件 (condition)，没有x和y，则输出满足条件 (即非0) 元素的坐标（注意这里返回的是坐标）12345&gt;&gt;&gt; a = np.array([2,4,6,8,10])&gt;&gt;&gt; np.where(a &gt; 5) # 返回索引(array([2, 3, 4]),) &gt;&gt;&gt; a[np.where(a &gt; 5)] # 等价于 a[a&gt;5]array([ 6, 8, 10]) 123456789101112131415161718&gt;&gt;&gt; a = np.arange(27).reshape(3,3,3)&gt;&gt;&gt; aarray([[[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8]],[[ 9, 10, 11],[12, 13, 14],[15, 16, 17]],[[18, 19, 20],[21, 22, 23],[24, 25, 26]]])&gt;&gt;&gt; np.where(a &gt; 5)(array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]),array([2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2]),array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])) 注意这里的最终结果的坐标是要竖着看的，即（0，2，0），（0，2，1）…. 这个方法只能用在array上面，如果需要list的话需要np.asarray 12345678910import numpy as npids = [[3], [34], [5]]ids_o = [[3]]remove_list = filter(lambda i: i not in ids_o, ids)print(np.asarray(ids))for i in remove_list:index = np.where(np.asarray(ids) == i)print(index) 结果123456[[ 3][34][ 5]](array([1]), array([0]))(array([2]), array([0]))[Finished in 0.2s]]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python的filter函数]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E7%9A%84filter%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[python filterfilter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。 该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。 返回值是fliter的类型1remove_list = filter(lambda i: i not in ids_o,ids_u) 对于不在ids_o里面的i，是不是在ids_u里面，如果是的话就需要remove这部分东西]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tuple,array,list的大小问题]]></title>
    <url>%2F2019%2F03%2F22%2Ftuple-array-list%E7%9A%84%E5%A4%A7%E5%B0%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[每次在使用这群乱七八糟的数据结构的时候我都不明白到底用哪个函数求长度，而且各个结构的表示方法每次都让我感觉很迷茫，所以有了这篇文章。 好像只有array可以用shape来求！其他的都没有shape，array的shape可能是多维的。 数组array 数组的表示方法为最外面是括号，里面是方括号，不同的方括号代表不同的维度，np操作的都是array的部分 如果是一维数组，显示出来的size应该是(1,)这个样子的 size方法1a.size 1np.size(a) len不可以得到整个的大小，但是可以得到数组的行数，相当于a.shape[0]1len(a) 1a.shape[看看求的是第几维] 列表 列表最外面是方括号，不是圆括号！ 不可以直接用 a.size 求，’list’ object has no attribute ‘size’ 1np.size(List) 1len(List) 元组 元组的最外面是圆括号 不可以通过 t.size 来访问 可以通过 Tuple[]直接访问元素 1np.size(Tuple) 1len(Tuple) 字典 外面是大括号，里面是value-key的配对 size不可以用，np.size无法获得真实的大小 1len(Dict)]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>data structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在python中调用另外的文件]]></title>
    <url>%2F2019%2F03%2F22%2F%E5%9C%A8python%E4%B8%AD%E8%B0%83%E7%94%A8%E5%8F%A6%E5%A4%96%E7%9A%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[关于如何在python中调用其他的文件在cpp里面是使用头文件来导入的，但是提到python突然没想起来是怎么导入的。假设有文件a.py和b.py 在同一目录下12import aa.func() 或者引用模块中的函数123from a import funcfunc() ` 注意：前面一种方法导入的时候需要加上模块的名称限定，但是后面的导入就不用。如果怕麻烦可以导入的时候使用1from a import * 在不同目录下sys.path获取指定模块搜索路径的字符串集合，可以将写好的模块放在得到的某个路径下，就可以在程序中import时正确找到1234import syssys.path.append('a所在的路径')import aa.func() sys是什么 sys是python程序用来请求解释器行为的interface，比如调试，实时运行环境等 sys.argv 从外部向程序内部传递参数12345#!/usr/bin/env pythonimport sysprint sys.argv[0]print sys.argv[1] 运行：123# python sys.py argv1sys.pyargv1 sys.exit() 需要中途退出的时候可以调用，可以返回参数（0是正常退出，其他是异常）12345678910111213141516#!/usr/bin/env pythonimport sysdef exitfunc(value): print value sys.exit(0)print "hello"try: sys.exit(1)except SystemExit,value: exitfunc(value)print "come?" 123# python exit.pyhello1]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于hexo和gitpage的博客搭建以及设置]]></title>
    <url>%2F2019%2F03%2F20%2F%E5%85%B3%E4%BA%8Ehexo%E5%92%8Cgitpage%E7%9A%84%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E4%BB%A5%E5%8F%8A%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在commit了40多次之后终于把自己的博客搭好了，中间画遇到了一些奇怪的问题记录一下 github部分 在一些地方看到的说网站的名字必须和github的名字一样，不知道是不是必须的但是还是这么设置了 网站需要选择在master hexo部分基本功能：生成网页1hexo g 传到github上面1hexo d 生成新的md1hexo new &lt;title&gt; 需要把生成的全部清除1hexo clean 添加主题 把相应的主题clone下来，然后修改博客根目录的 _config.yml 文件 遇到404或者不显示模板的时候基本就是没套对 主题内容在主题的config修改这部分遇到的主要问题是两个：根目录config忘记添加一部分123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true hexo的url和root部分设置不对 github的deploy 地址应该是clone时候的网址 url部分应该是https://bigphess.github.io/，root部分是/ md文件增加图片在config里面设置，生成新的文章的时候就会生成对应的文件夹1post_asset_folder: true 然后把相应的图片放在文件夹里，引用的时候直接md格式引用：1![图片的名字](相对路径)]]></content>
      <categories>
        <category>博客相关</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[StanfordCS231N笔记]]></title>
    <url>%2F2019%2F03%2F20%2FStanfordCS231N%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Introduction Image Classification pipelinechallenges 图片是由无数数字块组成的 视角的转变，亮度的变化，变形都会产生非常大的变化 viewpoint illumination deformation occlusion background clutter intraclass variation image classifer input:image output: class_label data-driven approach 其他方法不行 attempts: 边缘检测，纹理等等（但是太过具体） 以数据为导向的方法 def train(image, label) def predict(model, test_image) KNNNN 对于每一个测试的data，在数据库里面找到离他最近的图片（选择一共找多少张，这么多张里面投票） 定义距离（hyperparameter） 曼哈顿距离 L1: 两张图相减求绝对值，然后把整张照片求和 欧几里得距离 L2: 距离的平方和开方 实现 training：记住每个图片的内容和label image：N✖D，每行是一张图片（拉成一行），一共N张 label：1-d数组，sizeN predict：计算距离找到最小的角标（np.argmin) 速度：linearly to size of dataset 缺点： 预测的时间太长了（expensive） 但是我们希望训练的时间长但是测试的时间短（CNN） KNN 找到最近的K个，投票 当K增加的时候，整个图片的边缘变得平滑了 K的数量也是一个hyperparameter 需要选择的hyper（并不能很好的找到最优解） K 用什么distance 如何选择最好的参数 总不能尝试所有的参数吧2333 不能使用test data，请在训练的时候忘记自己拥有它 把train data fold成不同的部分，把其中的一部分当成测试数据（validation data），然后测试训练的结果寻找hyper 交叉验证（cross-validation），循环当validation fold然后average result 但是根本不用呢 在test time的performance太差了 两个图片之间的距离太不直观了，你根本不知道图片间的距离会怎么变 linear classificationparametric approach 输入：32x32x3的图片，array of numbers 0,1,…3072 f(x,W) = Wx + b （在线性分类的情况下） （10x1） x: image （3072x1 -&gt; 拉直了） W: parameters，weights （10x3027） b： bias （10x1），不是这个函数的参数，只是用来决定比如猫的数量特别多，偏向猫的bias可能就比较大 输出：10个数字，表示每个class的scores 注意 W是把不同分类的classifer拼在了一起（乐高一样），每一行都是一个不同的class的分类器，点乘这个图片上面的像素，加上bias就是这个图片最终的得分 resize所有的图片到一个大小（目前） 实际上每个class的score就是图片里面每个点的加权求和，可以想象成在数每个不同地方的点的颜色。如果把W矩阵还原，还原出来的就是这个class的感觉上的颜色 可以想象在一个巨高d的space里面，用线性分类 hard part 都用灰度图会有问题 相似的texture（？ loss function optimizationtodo： 定义一个loss function来定义这个score的好坏 找到一个efficiently way去找到minimize 这个loss SVM loss定义 假设如果只有三个种类，一张图片对三个class分别会有不同的score。每张图片都可以计算出一个对应的loss SVM loss Li = sum max（0，sj - si + 1） si: 想要计算这个的loss function 的class的评分（也就是label标注的class的评分） sj: 这张图对于所有其他种类（除了i）的评分 Li: 最终这张图片的loss 1: 是一个safety margin（也是一个hyper parameter）。可以选择其他正数，但是选0会出问题 Li的每一项都在0和差值之间找最大值，然后把每一项的加起来求和 如何理解这个式子：既然对于不同class的评分越高就是越可能，那么评分是负数的话就说明不可能，这样就直接用0把这种可能性抹去了。如果其他种类在正的方面评分越高，说明这个种类跑偏了，loss越大 ###注意点 在上面这张图里，因为车的评分已经是最高了，计算出来的loss就是0 最后再把所有类型的loss求和，除以种类得到最终的loss 用的是求和而不是mean也是取决于自己的决定 也有的SVM里面用的是max之后平方，但是不平方的用的更多一点，也是一个hyper parameter scale 最小：0 最大：infinite bug 在实际应用里面没有那么好的效果 W不是唯一的，比如把这个W加倍，如果loss是0的时候是一样的 -&gt; 需要得到唯一的W weight regularization（解决上面这个问题） 在之前的loss的基础上加上了 \lambda R(W) \lambda是一个hyper parameter，是取决于自己的选择的 R是一个regularization函数，这个函数的作用是抵抗之前的loss。因为之前的loss是从训练集上得到的，比较吻合训练集，所以需要一个比较特别的W来和之前的fight，这样的话结果可能会在实际使用的时候更好一些 主要分类 L2 regularization：W里面的所有项平方然后求和（最常见） L1 regularization：W里面所有项绝对值然后求和 -&gt; 在一些其他地方使用 elastic net（L1+L2）：所有项平方乘参数加绝对值求和 max norm regularization -&gt; 后面讲 dropout 理解L2 比如X是[1,1,1,1],两个W分别是[1,0,0,0]和[0.25,0.25,0.25,0.25] 这样乘出来的最终结果都是一样的，都是1。 但是如果加上了L2的regularization之后就发现第二种方法的loss更少一点。因为他用到了更多的维数，在实际应用之中效果更好。 softmax（用起来更好）（multinomial logistic regression）定义 scores：unnormalized log probabilities of the class 需要把score先exp回来(这样所有的数都变成正数了) 再normalize（除以所有exp之后的的和） 最终，对于正确class的最终处理完的score来说，max这个log或者min（loss function）- log会得到最终最好的结果 最终处理完的score就是每个类型推测出来的占比可能性（和为1） 这里求完-log（p）其实就是信息熵，代表对不确定度的度量 直接比较可能性和log之后比较可能性在本质上是没有区别的 但是数学上一般log之后的数据会看起来好一些实际操作如下 一些问题 极值 Li最小值：0 -&gt; 如果正确类型的可能性是1，求出来的最终值就是0 Li最大值：infinite，可能性非常低非常接近于0 当W的初始化很小，所有score都接近于0： score求exp之后都是1，normalize之后是1/num（class），最后再求log 可以用于开头的检验 SVM和softmax 如果输入是[10,-100,-100]，在这个范围里微小变化，第一个是正确的class 对于SVM来说，后面两个负值都非常小了，根本不会去管后面的两个东西，-100和-200没啥区别 对于softmax来说，后面的-100还是-200还是会对loss最终的值产生影响，softmax希望所有的值都在正确的class上面，后面啥都没有。所以更具有robustness。 SVM会有一个你需要的区域，剩下的根本不考虑；而softmax会考虑所有的区域 上方区域总结 x：训练集里面的数据，放在图片里就是把一个图片拉成一个1xN的向量 y：训练集的标签，用来和最终的结果比对 W: weights，需要优化的部分 L：loss，用来权衡W优化结果的好坏 基本过程 Wx+b得到目前的分类器的score（score function） y是目前分类应该有的结果（label） R（W）得到regularzation的值 分类器得到score，y知道正确的分类，通过softmax或者SVM得到这个分类器目前的loss，再加上R（W）的部分增加robustness最终得到整个分类器的loss optimization lossfollow the slope 通过计算gradient来找到最低点 最基础的想法：（从数学上入手） 因为梯度是lim f(x+,h)-f(x)/h 把W上面的每一个点都加上一个0.00001（接近于0）然后再求上面的式子，就能得到第一次操作的梯度 silly 每一步都需要每一个维度都算一下，在CNN里面参数高达百万个，计算太慢了 因为用的0.00001，其实并不准确 感谢牛顿莱布尼兹发明了微积分 -&gt; 如何具体计算在下一节课 把loss的gradient改成了一个式子 快速，准确，然是容易发生bug（error-prone） practice需要进行gradient check 在写代码的时候用的肯定都是analytic gradient 但是需要在应用之前用numerical gradient检查一下，确保两者的结果是一样的，为了保证代码里面写的积分是正确的 gradient descent mini-batch 在实际应用的时候，不会把整个的训练集都拿来优化W，而是会把一部分拿出来（sample examples） 一小点一小点的拿结果不会非常准确，但是可以step很多次，在实际应用里面一般都不会用整个training set，不是很现实而且效果不是很好。 选择的数量上 32/64/256，这个不是一个很重要的hyperparameter，主要是根据GPU的性能来决定的 最终结果的loss是会下降的，虽然noise很多但是最终会go dowm learning rate 图片中使用linear classifier因为图片像素太多了，不可能对每个像素都用线性分类，所以一般会先提取一些特征然后得到最终的分类结果 color histogram 先得到一张图片的颜色特征分布 然后把整个特征分布拽成一个长的vector进行分类 HOG/SIFT 找到边缘特征，在图片的哪个部分有那种样子的edge bag of words 先把图片里面的一些特征当作一个vocabulary，然后放进一个词典里面 找到词典里每个词出现的频率然后拽成vector 线性分类 总结一般都是先进行特征提取然后再进行线性分类 深度学习特征都是自己提取 Backpropagation &amp; neural network目的：求出来loss function的gradient backpropagation最右边的点因为是df/df所以结果就是1 forward pass：知道开始然后一直顺到结束 在一个node上面，收到了x和y的input，对他们进行f操作，得到最终的结果z z再往后操作得到最后的loss（不知道什么操作） backward pass：从后到前，通过链式法则倒回来 虽然不知道loss对x或者y的gradient，但是可以求出来dz/dx和dz/dy（只和这个点有关） 可以得到dL/dz，然后乘以local gradient local gradient 每一个node上面的gradient往前推的时候，都可以通过链式法则（chain rule）变成这个点输入的gradient和这个点到上一个点的gradient的乘积。 算local的时候，乘的参数是输入进去的参数啊。比如dL/dx = dL/dz（这个带这个点back回来的数字） * dz/dx （这个里面的x带这个点输入进来x的值） 想不明白的时候把不同的点假设成不同名字然后求导！ 在这个网络里面，如果gate是加法（x + y）的话不是求偏导，如果求x的导数的话y并不是参数而是常数，所以求出来的结果是1，所以加法的gate就是直接把这个值相等的分开 加gate是一个gradient distributor，当一个gradient进来的时候会被相同的分开成了两份 也可以把一些gate组成一个大的gate，比如sigmoid 注意，求出来的gradient如果是正的，说明这个点对最终的loss有positive的作用 patterns add：gradient distributor max：router 假设f是max（x，y） local gradient对最大的那个就是1，对其他的都是0 因为如果没能通过max的gate的话根本对后门的loss没有影响。back的时候走最大的点就可以了，其他的都不用管了 multiply：switcher，真，两极反转 当往回的时候，两个点指向一个点，gradient需要相加（如下图） Implementationpsuedocode graph or net object forward: 把input pass进这个gate里面（必须在代码里面记住input） 把整个computational的garph往前推动 最后一个gate会return这个网络的loss backward 输入dz，然后乘不同的x和y 不同的gate分别是不同的文件（API），每个文件里面包括初始化，forward和backward 每次update的时候都需要进行forward和backward，forward得到gradient，backward再回来求最终的loss vectorized 在实际的计算中x，y，z都是矩阵，dz/dx是jacobian矩阵（全部都由偏导组成的矩阵） 比如一个max的门，如果输入是1x4096，输出也是1x4096，但是求偏导出来的矩阵是4096x4096（太大了），矩阵中间只有对角线部分的是需要考虑的（还会有很多0） 然后如果用了minibatch的100，得到的结果就是409600了，更可怕了 所以在每次API的时候，肯定不能写出来所有的链式法则，只用其中的一部分 作业的重点就是如何让这个东西计算出来效率高 neural network两层的NN 输入是图片一共的坐标数量 先通过第一层（max）得到100的中间层（hidden layer）-&gt; 100是hyperparameter，自己定的，但是越多越好吧 然后通过W2得到最终的分类结果（分10类） 其实具体里面是什么东西真的是不知道的？ 神经元 每个神经元的输入是Wx+b，然后经过激活函数 输出 激活函数 activation function sigmoid tanh ReLU 层状 -&gt; 可以更加efficient Neural network 2（training part1）前方提示： 小的dataset也可以有结果 电脑的性能有限 回顾一下历史 perceptron -&gt; 激活函数：0或者1，不能back madaline··· activation function（一个hyerparameter）sigmoid 特点： 把所有的数值都压到了0到1之间 曾经非常受欢迎，因为satrating的效果比较好 问题： 在saturate的情况下（非常接近0或者1），会杀死gradent -&gt; 看函数的图就能感觉出来-10做哟的导数就是0了，back回来没有意义 output不是以0为中心的（预处理的时候希望是0中心的） 不是0中心的问题：如果所有输入的x都是positive的话，得到的gradient要不都是positive要不都是negative 最后走出来的路径都是zig zag的 exp（）在计算上比较expensive tanh 把数字从-1到1之间分布，是一个以0为中心的sigmoid（0-centered），所以sigmoid的缺点（saturated的点会kill gradient）的缺点还在 ReLU 输入是正数的时候直接pass这个值，输入是负数的时候直接kill 可能的优点：（实际应用的时候效果非常好但是具体解释起来也没有那么知道为什么） 不会saturate（不会消失gradient） 计算效率高 更容易相交 问题 不是0-centered 如果x小于0（没有激活） -&gt; kill gradient） 死的时候会死一大片 -&gt; 所以一般的时候会把relu初始化的时候加上一个slightly positive bias 注意learning rate，选不好容易死 leaky ReLU 在小于0的时候会有一个微小的值，所以不会die 在使用的时候converges的速度比sigmoid和tanh快很多 加上了一个参数，可以在back的时候学到，这个值可以确定他是不是ReLU或者其他的 Maxout neuron 把ReLU和leaky ReLU组合了起来，有两个参数。算出来两个分别的值然后取其中大的那个 不会发生saturate或者die的问题 问题在于参数需要计算两次 步骤： 预处理数据 -&gt; 选择architecturedata preprocessingML 处理数据的时候首先需要0-center -&gt; 减去平均值（不是特别需要normalize，ML需要） PCA，Whitening，其实都在DL里不怎么常用 实际应用里：只需要center 比如一张图是32x32x3的 减去mean image（32x32x3） 减去per-channel mean （每个channel的mean，一共是三个数字） weight initialization（重要）请不要这么做：set所有w都是0，得到的结果就是每个神经元的功能都是一样的 small random numbers 0.01* np.random.randn(D,H) 问题： 在比较小的net里可以使用 在layer之间会发生non-homogeneous distribution of activation的问题 所有的activations会变成0 在back的时候所有的gradient都会变成0 如果把0。01变成了1，这时候发现所有的neurons全都是1或者-1 -&gt; gradient也全都是0，死亡 其他的一些论文也讨论过其他方法 Xavier 2010 除以input的sqrt ReLU， non-liear，会breaking。每回relu都会杀掉一半的东西，set到0 He 2015 把input除以2以后sqrt了 在实践中很有用 batch normalization -&gt; 实际中解决w初始化的方法 核心思想：x越来越接近0的原因是因为越乘越小（或者越大），这个时候我们就希望可以normalize这个x的input。因为gaussian的normaliztion是可以积分的，所以可以放回到back里面，在整个的网络里面插入一些normalize的部分就可以了 插在FC或者CNN之后，然后放在激活函数之前 优点 提高net里面的gradient flow 允许更高的学习率 减少对初始化参数的影响 form of regularization -&gt; 可能可以减少dropout的需求 babysitting &amp; learning process检查loss算的对不对 初始化这个net，去掉regularization，检查最后返回的loss 因为什么都没做呢，所以loss应该是最终知道的值（10 class是2。3） 再加上regularization，结果应该小小的变化 尝试训练 overfit一个非常小的dataset，关掉reg，得到非常小的loss和很高的accuracy 一个可能性：建议以一个小的reg开始，找到让loss变小的learning rate（如果不变小可能是rate太小了） cost：NaN，可能是learning rate高了 建议范围： 1e-3 ~ 1e-5 hyper optimization交叉验证 找到准确率高的部分，使用其中的hyper 最好set到log的space 再调整parameter，找到更准确的值 如果结果特别好可能也不对，可能是已经到了boundary了 参数的选择sample randomly的结果更好，不要固定一个选另一个，可能一个参数比另外一个重要很多 如果训练和验证之间的gap太大，说明overfitting，需要增加reg的力度。如果太小可能需要增加model的容量 ratio between the values and updates: ~ 0.0002 / 0.02 = 0.01 (about okay) 需要选择的hyper net architecture learning rate. decay schedule and update type regularization(L2/Dropout) ##总体summary training Neural Net2parameter updateSGD 以前是直接用gradient来update，现在希望变得复杂一点 -&gt; SGD太慢了 为什么SGD太慢： 如果在一个loss的分布上，一个维度特别密集，另一个维度特别稀疏，直接用gradient改变就会在一个方向跑大了 最后就会形成那种zag的形状 momentum update 在计算的时候引入了速度v = mu v - learning_rate dx （v初始化为0） 假设路线就是一个球在loss的圆弧里面运动，mu是～0.5，0.9，0.99（只使用一个值，single number，hyper） 形态，从初始点开始走一个大的圆弧，会跑过了，但是会再快速的converge回去 优点 引入了速度，可以在比较shallow的方向上速度逐渐增加 在比较深的维度上面，就像球在圆弧里面来回滑动 理解 是对这个update一点物理上比较直观的理解（其实名字叫做动量） 可以理解为这个东西是在一个平原上跑的一个球，我们需要求的w是这个球的速度，得到的dw是这个球的加速度，而这个球的初速度是0 可以理解为这个球找最低点的时候，除了每步按dw update，还在上面加上了前面速度的影响，也就是加上了惯性！123# Momentum updatev = mu * v - learning_rate * dx # integrate velocityx += v # integrate position nesterov momentum update 在上面的方法之后 look a head 了一步，得到的是两个向量之间的差 在实际走的过程当中，弧度会比monnument的更大一些，跑过的会更小一些 理解 Nesterov Momentum(NAG) 在原来的基础上：真实移动方向 = 速度的影响（momentum）+ 梯度的影响 （gradient） 现在：既然我们已经知道了要往前走到动量的影响的位置，那么我根据那个位置的梯度再进行update，岂不是跑的更快！ 总的来说就是考虑到了前面的坡度（二阶导数），如果前面的坡度缓的话我就再跑快点，如果陡的话就跑慢点123v_prev = v # back this upv = mu * v - learning_rate * dx # velocity update stays the samex += -mu * v_prev + (1 + mu) * v # position update changes form adaGrad （parameter-adaptive） 首先定义了一个cache，这个cache是gradient的平方的和，只是positive，和parameter的维度是一样的 然后把SGD的学习率（全局的learning rate）scale了一个这个数 -&gt; 这样得到的是不同参数的学习率 “ Added element-wise scaling of the gradient based on the historical sum of squares in each dimension” 结果：在越密集的维度上，update的步伐越小，越稀疏的上面update越大（因为平缓的地方历史gradient的平方和更小，所以update会更大） 问题 step size：时间越长learning rate会最终变到0，然后就停止学习 RMSProp（上面一个的变形） 把cache的定义改变了，增加了一个decay rate（hyper） adaGrad会计算的是所有梯度的平方的和，而这个计算的是gradient对应的平均值，这样的话learning rate的下降会更慢 依然能保持各个维度上面的平衡，但是不会让learning rate变到0 adam -&gt; 另一种自适应学习率的算法 beta都是hyper 结合了上面的两种方法 利用梯度的一阶矩和二阶矩估计动态调整每个参数的学习率 -&gt; 每次迭代学习率会有一个范围，让参数比较平稳 对梯度的一阶和二阶估计（期望的近似） 实际使用 默认用adam 刚开始使用高的learning rate -&gt; 这样进展会非常快 decay over time -&gt; 在进行到一定程度的时候会没有办法更细致的逼近minimum step decay: 比如过一些epoch之后就把lr减少到一半 exponential decay 1/t decay secend order optimization method（ml） 在计算的时候不仅需要gradient，还需要hessian来告诉你曲面的curve程度，以此来确定如何前进（牛顿method） 速度更快，hyper更少 但是在deep nets里面不太能使用，因为参数太多惹 BFGS（approximate inverse Hessian with rank 1 updates over time (O(n^2) each). L-BFGS work well in full batch mini-batch不是很适用 evaluation：model ensembles 可以不用训练很多个model，而是训练一个然后在其中选取不一样的check point track一个参数vector的running average可能会得到更好的效果 regularization（DROPOUT） 在forward的时候，随机的把一些neruon的值设置成0（比如杀掉一半） 为什么要使用： 为了求出来的结果更加的准确，每个特定的特征都不能完全依赖，因为这个feature可能就被drop掉了 计算一个大的net的其中一小部分，被drop掉的部分在back的时候也不会再计算了，就彻底关掉了。相当于在net里面取了一部分sample test time 在测试的时候希望可以把所有的neuron都打开（就进行一次） scale！！！！ 需要注意的问题： 计算训练时候的期望，就发现dropout之后的期望是测试的实际值的1/2（因为drop了一半） 因为以前net没见过这么大的output，会直接死掉，所以需要把测试时候的结果再缩小一半（或者drop的比例，* p） 最终结果：测试时候的输出 = 训练时候的期望输出 另一种方法 inverted dropout 在train的时候 / p 在测试的时候就不用改变了 gradient checking（并没有讲）CNN开始啦卷积层（核心部分） 对一张图片操作： 拥有一张图像32x32x3 拥有卷积核5x5x3（这两个东西必须维度一样） -&gt; 奇数尺寸的效果更好 kernal做卷积（所有的channel），得到一个28x28x1的activaton map 再对这张图片使用下一个不同的卷积核（卷积核的数量是一个hyper） 这样一个32x32x3变成了一个28x28x6（6是选择的hyper的数量） 当把这些层可视化了之后，发现越深图片的feature越高级（从上一级的特征得到的新的特征） 大致布局 卷积层 RELU层 -&gt; 黑白化 pooling层 最后加上fc层 具体计算stride 每次卷积核移动的时候的步长 注意在不同图片大小，不同卷积核大小和不同步长可能不匹配 （图片 - 卷积核）/步长 + 1 是不是整数，结果是输出图片的尺寸 padding 可以在图片周围一圈加上一圈0，这样图片卷积之后的大小就不变了 0-padding的大小和卷积核的大小有关，大小是（卷积核 -1）/2 如果不进行padding，图片会越来越小 参数数量 对一个卷积核：卷积核的大小 * 深度 + 1 （加一是加了一个bias） 一层的参数： 卷积核数量 * 一个卷积核 四个hyper： K：filter的数量，2的指数 -&gt; 计算效率高 S：步长 F：卷积核大小 P：0-padding 1x1的卷积 1x1的卷积层（stride也是1）会有比较好的效果 比如输入是56x56x64，filter是32个1x1x64。因为数据是有深度的，1x1的时候是有意义的（在二维上面没有意义） 现在处理的东西都是方形的从神经元的角度来看CNN 可以把filter认为成一个固定位置的神经元，这个神经元只看到了图片上面的一小部分，没有和全部的图片相连，然后进行了wx+b的运算 当slide这个filter的时候，weight是不变的，可以假设成一圈共享参数的神经元 对同一张图片的不同的filter可以认为成他们是在三维上面排列的一组神经元，每一层神经元都和这一层共享参数（不希望全部都是全联接，因为浪费了很多参数） pooling 在卷积的时候是不会改变图片的大小的 改变图片大小的操作在pooling layer里面实现 长宽缩短，深度不变 max pooling 2x2pool，stride2 -&gt; 每4个格子里面选择一个最大的表示 两个参数 pooling size F 2，3 stride S 2，2 不会改变图片的深度 fully connected 就跟普通的神经网络一样，所有神经元之间都会连接 把最后的图片变成一个列，放进去开始计算 实际应用LeNet-5AlexNet 两天不同的线，因为当时的GPU的效果不够 优点： 第一次使用ReLU 把data normalization了，但是现在看其实并不需要 data augumenation -&gt; 有用！ dropout 0.5，最后几层 ZFNet 在第一层上比alex的stride短，因为alex的步长4跳过了太多图片信息，这里改成了步长2 fliter的数量更多 VGGNet 只有3x3 s1 p1的卷积核，和2x2 s2的max pooling 结果还特别好 图像的尺寸越来越小，但是深度越来越高 需要的计算量：93MB/image（forward） -&gt; 200m/image(所有的计算加起来) 大部分的memory都在前期的层里，大部分的参数都在最后的全链接层里面（最后的计算量太大，后面有更好的方法） VGG也有位置确定，他比overfeat的层数更深 GoogLeNet 是一个一个的小结构组成出来了 参数的数量非常少 5million，取消了fc层 使用了average pool，把7x7x1024变成了1x1x1024 :把每个activate map取平均值 用VGG的人更多因为VGG的结构比较好想2333 ResNet t5 error降到了3.多 平常的加深层数训练集和测试集上边的准确率变化结果不统一，但是res做到了统一 虽然层数特比多，但是速度还是快 -&gt; 加入了skip的部分，把输入跳过了卷积又加了回去，这样back的时候就会分流 top-5 error 在看结果的时候不光看准确率，还会看分类器认为的前5个可能性（可能有几千个分类），如果这5个可能性都不对的话就是求出来的就是top-5 error spatial localization and detection这章的主要内容是识别出来这个东西之后用框框框出来 分类+定位：Localization as Regression 实际上就跟regression差不多 neurral net的输出是bounding box（4个数字），左上角的坐标和长宽 实际的图片标注的内容也有左上角坐标和长宽，求出这两个部分的L2 distance作为loss 步骤 训练（下载）一个分类的model 在net里面加上fc的regression head 用SGD和L2loss训练regression head部分 test的时候分类和regression都用 类别 平常的分类：最终的数量和class的数量相同 一个box里面会有4个数字，一共Cx4个数字 加在什么地方 conv layer之后 fc之后 多个目标的检测（Aside） 知道准确的检测目标的数量k，那么最终的分类数量就是4 * k 应用：人的动作检测 -&gt; 得到关节的位置 分类 + 定位：sliding window：overfeat 核心idea：在检测的时候直接process图片，但是对一张图片在不同的地方进行多次操作 操作步骤： 首先对图片进行conv和pooling，然后对得到的结果进行两个不同的fc， 得到的是1000个的分类种类 另一个的到的是1000x4的bounding box坐标 在一张大的图片上，在不同区域找到需要寻找的东西（比如分成四部分，这四部分是有重叠的，不是pooling那个样子） 得到每个部分对于这个分类的得分，以及相应部分对应的bounding box 最后用没怎么讲的办法merge了这些框，得到了最终结果 进一步优化 因为要对这个图片的不同crop做cnn，计算量会非常大 在fc层其实只是一个向量1x4096，把这个玩意拉成了一个cnn，4096x1x1，然后直接conv1x1的卷积核 现在net里面就只有conv和pooling了，所以就可以处理不同尺寸的图片了（不同尺寸的方形） 而且在处理不同区域的时候是参数是share的 目标检测 主要不同：不能确定图片里面物体的数量 思路： 尝试所有可能的window然后用classifcation找到需要的部分 问题：需要很多次分类 历史解决方法：用非常快的分类器，尝试所有（linear classifier） 更想用的方法：用cnn，只测试tiny subsets of possible locations region proposals 输入一张图片，输出所有可能有物体的区域 不在意到底是什么类型 不在意精确度 但是速度很快 selective search 从一个pixel开始，如果相邻的pixel有一样的颜色或者texture，merge 形成连接区域，再连接不同区域，这个区域也可以再打散 还有很多其他方法：edge boxes（推荐） RCNN（region based CNN） 从输入图片里面用region proposal的方法得到一系列的boundings（不同的位置和scale） 对每个区域crrop和wrap这个区域到fixed size cnn分类，regression head &amp; classifcation head 过程 下载model fine-tune for detection：改变分类的种类等 extract features 为每个class训练一个SVM（看这个区域是否包括寻找的东西） box regression：对每个种类，训练一个linear regression来纠正位置的偏差（太左太右，空隙太多）（dx，dy，dw，dh） datast PASCAL VOC 比较小 ImageNet 不是事很好操作，但是一张图一半只有一个东西 MS-COCO 一张图多个内容 fast RCNN （提速） 在测试时的速度比较慢 -&gt; 一张图里，在不同的proposals之间share conv的计算 训练时不是一起训练的，训练的pipeline也很复杂 -&gt; 把整个系统端对端对的训练一次 ROI pooling 在用的时候希望感兴趣区域的分辨率比较高，fc层希望更低的conv feature 在conv feature map上面投影高分辨率的region proposal 把这个区域分成小格，然后对每个格子进行max pooling(back的时候也是这么回来) 训练8倍！测试146倍！结果更准确！ faster RCNN（再提速） 之前的测试速度计算都没有算region proposal的时间，所以把这个问题也交给conv去干 在最后一层conv后面加入region proposal net 在feature map上面的移动实际就是卷积 训练一个小的网络判断是不是一个物体并分类，以及regression框的位置 在每个位置使用了N anchor boxes，不同的anchor有score来判断他是否属于一个object，在不同的形状上有不同的可能性（？ 后续的paper里面可以一口气train了 yolo 把detection变成了regression的问题 分成不同的小块，在每个块里面都加入 visualization, deep dream, neural style可视化：观察神经网络如何工作 可视化不同位置 可视化activation的神经元 -&gt; 大量的图片扔进神经元里面，找出来一个神经元最感兴趣的部分 可视化fliter -&gt; 只能在第一层进行（其他的层可以但是意义不大） 但是啥算法都会得出来长得差不多的 可视化特征（全联接层的特征向量） -&gt; t-SNE：Embed high-dimensional points so that locally, pairwise distances are conserved，特征相似的东西会聚类 对图片进行遮罩，可以看出来遮住不同地方这张图片被识别出来的概率 deep conv和optimazation的可视化工具：http://yosinski.com/deepvis deconv实现问题1:如何计算任意一个神经元的梯度（代码实现） 找到想要的神经元，forward的时候就停在这里 然后进行back，把所有其他的神经元的都设置成0，只把感兴趣的神经元设置成1，然后计算back出来的结果 最后的结果看起来并不是很好理解，所以改变了back的方法，得到更好的结果（“guided back”） guided back的计算方法 在普通的计算中，back的时候使用relu，会把所有负值都转化成0 在guide的计算里，在激活之后的东西back回去的时候，如果input的东西是负数的话，也会把这个东西kill成0，也就是说一个是block back的时候的gradient的值，另一个还会附加block输入进来的值 发生了什么：把输入进去ReLU的负的影响也取消掉了。如果不取消的话，这些正负就会互相fight，呈现更奇怪的图片。但是去掉负的之后变得就更清晰了。 deconv：直接无视掉relu的存在了 第二个问题：图像优化 how to find an image maximize some class score，但是整个网络不变 输入一张全0的图片 在back的时候把score设置成[0,0,0,1,0,0,…]，只有感兴趣的是1 back回去，找到对图片会产生什么影响 不停的重复这个步骤，更新的是图片不是weight 效果 找到可以让一个类型分数最高的图片（图片是根据网络生成的） 可视化data的梯度 -&gt; 得到了一个类似热量图的东西，这样对黑色的部分改变对这个东西的分类没有很大的影响 上面的步骤可以对任何的神经元进行（生成一张图片） 更好的regular 忽视了惩罚，只max神经元 但是更新之后blur了一下图像，这样可以阻止图片进行高频率积累 第三个问题，CNN的code包含多少信息 是否可以通过net还原出来原来的图片（涉及到隐私泄露的问题） 越往后的时候预测的准确度越低 deep dream 一个非常简单的过程，只有几百行代码，就是optimazation image 每次调用make_step图片都会发生微小的改变 把网络forward到一个位置 把gradient设置成activation设置成一样的 再往回传回去 可以强调对图片贡献最多的部分，不管激活了什么，都会把这个激活加强 deepart 把目标的content传进CNN 把style contet也传进CNN 把目标的loss和style的loss匹配，然后得到相应的opt image 是否可以用生成的图片去fool CNN 把图片的gradient设置成其他的东西，本来希望可以得到混合的结果，但是实际上图片的distort根本看起来不变 有些图片人类看起来差不多，但是gradient（或者HOG）之类的可能彻底是其他的东西 原因： 图片有很高维度的空间 实际训练的图像有一小部分被约束，放了一个线性分类之类只调整了其中的一小部分 在线性分类里，如果在每个维度上面都改变了一点点，实际上的置信区间会发生特别大的改变（大规模的点积运算）.下图只加进去了一点点的金鱼的噪音，分类就变成了100%的金鱼 这个现象不仅仅发生在图片里面，也发生在其他的地方 RNN（recurrent neural network）普通的nets：大小都是固定的 one to oneRNN：可以有灵活的对应结果 一系列的词来描述这张图 machine translation：seq of words -&gt; seq of words frame level的视频classification RNN是什么 可以在任何时间接受一个input（vector），然后对于不同的state产生不同的预测结果，然后需要在一些时间中预测出来vector。只需要特定的情况，其他的情况虽然有但是没有记录下来 过去的状态 + 新的input + 参数w -&gt; 预测出来新的state 注意：同样的function里面的weight是固定的，在不同时间使用但是weight是一样的 比如例子：https://gist.github.com/karpathy/d4dee566867f8291f086 输入一个字母的序列h e l o 预测下面的字母是什么，训练的模型是hello 把每个字母分别feed进去，顺着这个字母顺序来优化参数的序列，因为知道下一个的结果是什么了，就可以朝着这个目标来努力 竟然可以生成句子数学公式甚至代码 在图片中开始使用 从一张图得到一系列的文字 两部分组成 CNN：把test图片输入到CNN，一直到最后的fc，但是然后不进行分类，输入RNN RNN：RNN不仅是现在的输入，还会加入了CNN里面出来的输出。然后得到的结果（得到了没准一个词）进入下一个循环（就跟生成语义的时候一样） 直到在RNN里面找到的token，然后结束RNN LSTM long short term memory（大概是个生物里面的东西） RNN有好多层，每层还有很多个参数来决定这层往哪走 有两个输入x和h，组合到w上面，然后不同的东西乘不同的激活函数 x来自below h是从上一回来的 基于gate和function（forget gate）的类型，会更新c的值（反正都是参数的） 进行这些奇怪的操作的原因就是找到一个平衡和更好的结果 比较 每次普通的RNN都要经过f gate，会彻底改变。back的时候gradient会消失或者爆炸 LSTM里面用加法跳过了这个门，有一定的影响但是没有彻底改变，gradient的消失问题会被控制住（因为只用了加，不会die） gradient clipping可以控制住gradient爆炸 作业相关内容安装anaconda！！！ conda activate cs231npython3 -m IPython notebook 打开！！assignment1knn 两次循环计算距离 不需要一个像素一个像素的计算，用X直接表示i对应的那行的像素值的和，直接做差（每一项之间，平方（每一个，求和（所有项），开方。会快很多！！！！ 12#dists是一个500x5000的矩阵（测试数量和训练数量）dists[i,j] = np.sqrt(np.sum(np.square(X[i] - self.X_train[j]))) 初始化数组的方法是 np.array([[],[]]) 如果一个像素一个像素的循环结果简直太可怕了，害怕]]></content>
      <categories>
        <category>图像处理</category>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
</search>
