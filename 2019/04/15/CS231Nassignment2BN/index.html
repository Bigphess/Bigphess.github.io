<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.0.1',
    sidebar: {"position":"right","display":"always","offset":12,"onmobile":true,"dimmer":true},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="target 之前的内容讲了lr的优化方法，比如Adam，另一种方法是根据改变网络的结构，make it easy to train -&amp;gt; batch normalization 想去掉一些uncorrelated features(不相关的特征)，可以在训练数据之前preprocess，变成0-centered分布，这样第一层是没有问题的，但是后面的层里还是会出问题 所以把normaliz">
<meta name="keywords" content="DL,Batch Normalization,Layer Normalization">
<meta property="og:type" content="article">
<meta property="og:title" content="CS231Nassignment2之Batch Normalization">
<meta property="og:url" content="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/index.html">
<meta property="og:site_name" content="今天开始努力学习">
<meta property="og:description" content="target 之前的内容讲了lr的优化方法，比如Adam，另一种方法是根据改变网络的结构，make it easy to train -&amp;gt; batch normalization 想去掉一些uncorrelated features(不相关的特征)，可以在训练数据之前preprocess，变成0-centered分布，这样第一层是没有问题的，但是后面的层里还是会出问题 所以把normaliz">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/2.jpg">
<meta property="og:image" content="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/1.jpg">
<meta property="og:image" content="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/3.jpg">
<meta property="og:image" content="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/4.jpg">
<meta property="og:image" content="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/5.jpg">
<meta property="og:image" content="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/6.jpg">
<meta property="og:image" content="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/7.jpg">
<meta property="og:updated_time" content="2019-04-18T00:52:30.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS231Nassignment2之Batch Normalization">
<meta name="twitter:description" content="target 之前的内容讲了lr的优化方法，比如Adam，另一种方法是根据改变网络的结构，make it easy to train -&amp;gt; batch normalization 想去掉一些uncorrelated features(不相关的特征)，可以在训练数据之前preprocess，变成0-centered分布，这样第一层是没有问题的，但是后面的层里还是会出问题 所以把normaliz">
<meta name="twitter:image" content="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/2.jpg">






  <link rel="canonical" href="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>CS231Nassignment2之Batch Normalization | 今天开始努力学习</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">今天开始努力学习</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">面向谷歌编程选手许若芃</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories<span class="badge">24</span></a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives<span class="badge">40</span></a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  
  
  

  

  <a href="https://github.com/Bigphess" class="github-corner" title="垃圾代码都在我的github！" aria-label="垃圾代码都在我的github！" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" style="fill: #222; color: #fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://bigphess.github.io/2019/04/15/CS231Nassignment2BN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="RUOPENG XU">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="今天开始努力学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">CS231Nassignment2之Batch Normalization

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-04-15 14:21:16" itemprop="dateCreated datePublished" datetime="2019-04-15T14:21:16+09:00">2019-04-15</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-18 09:52:30" itemprop="dateModified" datetime="2019-04-18T09:52:30+09:00">2019-04-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/图像处理/" itemprop="url" rel="index"><span itemprop="name">图像处理</span></a></span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/图像处理/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/图像处理/Deep-Learning/CS231n作业/" itemprop="url" rel="index"><span itemprop="name">CS231n作业</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="target"><a href="#target" class="headerlink" title="target"></a>target</h2><ul>
<li>之前的内容讲了lr的优化方法，比如Adam，另一种方法是根据改变网络的结构，make it easy to train -&gt; batch normalization</li>
<li>想去掉一些uncorrelated features(不相关的特征)，可以在训练数据之前preprocess，变成0-centered分布，这样第一层是没有问题的，但是后面的层里还是会出问题</li>
<li>所以把normalization的部分加入了DN里面，加入了一个BN层，会估计mean和standard deviation of each feature，这样重新centre和normalized</li>
<li>learnable shift and scale parameters for each feature dimension</li>
<li><strong><em>核心思想：粗暴的用BN来解决weights初始化的问题</em></strong></li>
</ul>
<p>ref：<a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html</a><br><a id="more"></a></p>
<h2 id="Batch-normalization-forward"><a href="#Batch-normalization-forward" class="headerlink" title="Batch normalization: forward"></a>Batch normalization: forward</h2><p><strong>这个东西的要义就是NN里面的一层，不对维度改变，但是会改变这些值的分布</strong></p>
<p>首先setup，并且载入好了preprocess的数据<br><code>cs231n/layers.py</code> -&gt; <code>batchnorm_forward</code></p>
<ul>
<li>keep exp decay 来运行mean &amp; variance of each feature -&gt; 在test的时候去normalize data</li>
<li>test-time: 计算sample mean和varience的时候用大量的训练数据而不是用所有图片的平均值，但是在作业里面用的是平均值，因为可以省去一步estimate（torch7 也用的是平均值）</li>
</ul>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">running_mean</span> = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</span><br><span class="line"><span class="attr">running_var</span> = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</span><br></pre></td></tr></table></figure>
<h3 id="I-O"><a href="#I-O" class="headerlink" title="I/O"></a>I/O</h3><ul>
<li>input<ul>
<li>x，data(N,D)</li>
<li>gamma：scale parameter(D,)</li>
<li>beta：shift parameter(D,)</li>
<li>bn_param: 一个dict<ul>
<li>mode：‘train’ or ‘test’</li>
<li>eps：为了数字上的稳定性的一个常数</li>
<li>momentum：在计算mean和variance上面的一个常数</li>
<li>running mean：(D,)，是running mean</li>
<li>running var：(D,)</li>
</ul>
</li>
</ul>
</li>
<li>output<ul>
<li>out：(N,D)</li>
<li>cache:在back的时候用</li>
</ul>
</li>
</ul>
<h3 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h3><ul>
<li>用minibatch的统计来计算mean和variance，用这两个值把data normalize，并且用gamma和beta拉伸这个值，以及shift这些值的位置</li>
<li>在分布的上面，虽然求得是running variance，但是需要normalize的时候考虑的是standard（也就是平方根）<br><img src="/2019/04/15/CS231Nassignment2BN/2.jpg" alt="image_2"></li>
</ul>
<h3 id="implement"><a href="#implement" class="headerlink" title="implement"></a>implement</h3><ul>
<li>其实是和如何计算息息相关的，知道输入，求这个玩意的normal的步骤如下（其中的x就是这个minibatch的全部数据）<ul>
<li>求mu，也就是x的mean（注意这里要对列求mean，也就是把所有图片的像素均匀分布，最后得到的结果是D个不是N个）</li>
<li>求var，知道这个东西，可以直接用 np.var(x, axis = 0)来求方差</li>
<li>求normalize： x - x.mean / np.sqrt(x.var + eps)<ul>
<li>其中刚开始求出来的var就是方差，也就是标准差的平方</li>
<li>eps是偏差值，这个值加上方差开方是标准差</li>
</ul>
</li>
<li>scale和shift，乘scale的系数，加shift的系数</li>
</ul>
</li>
<li>最后需要计算什么cache和back的推导息息相关</li>
</ul>
<h2 id="Batch-normalization-backward"><a href="#Batch-normalization-backward" class="headerlink" title="Batch normalization: backward"></a>Batch normalization: backward</h2><ul>
<li>可以直接画出来计算normal的路径，然后根据这个路径back<br><img src="/2019/04/15/CS231Nassignment2BN/1.jpg" alt="image_1"></li>
<li>要义就是一步一步的求导！一步一步的链式法则</li>
<li>注意的就是求mean回来的导数，理解上来说就是这个矩阵在求导的过程中升维了，从(D,)变成了(N,D)，而在最开始求得时候所有的数字的贡献都是1，所以往回走的时候乘一个（N，D）的全是1的矩阵，并且1/N的常数还在</li>
</ul>
<h2 id="Batch-normalization-alternative-backward"><a href="#Batch-normalization-alternative-backward" class="headerlink" title="Batch normalization: alternative backward"></a>Batch normalization: alternative backward</h2><ul>
<li>在sigmoid的back的过程中有两种不同的方法<ul>
<li>一种是写出来整体计算的图（拆分成各种小的计算），然后根据这张图的再back回去</li>
<li>另一种是在纸上先简化了整体的计算过程，然后再直接实现，这样代码会比较简单</li>
</ul>
</li>
<li>ref:<a href="https://kevinzakka.github.io/2016/09/14/batch_normalization/" target="_blank" rel="noopener">https://kevinzakka.github.io/2016/09/14/batch_normalization/</a></li>
</ul>
<h3 id="最终目标"><a href="#最终目标" class="headerlink" title="最终目标"></a>最终目标</h3><ul>
<li>f: BN之后的整体输出结果</li>
<li>y：对normal之后的线性变换（gamma + beta）</li>
<li>x’：normal的input</li>
<li>mu：batch mean</li>
<li>varbatch vatiance</li>
<li>需要求 df/dx,df/dgamma,df/dbeta -&gt; 最终结果整体速度比以前快了x2.5左右，这一步的主要目的就是用来提速的</li>
</ul>
<p>可以把整体的计算分为以下的三个步骤<br><img src="/2019/04/15/CS231Nassignment2BN/3.jpg" alt="image_3"></p>
<h2 id="这三部分的代码如下"><a href="#这三部分的代码如下" class="headerlink" title="这三部分的代码如下"></a>这三部分的代码如下</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></span><br><span class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></span><br><span class="line"><span class="string">    data at test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></span><br><span class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: 'train' or 'test'; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mode = bn_param[<span class="string">'mode'</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the training-time forward pass for batch norm.      #</span></span><br><span class="line">        <span class="comment"># Use minibatch statistics to compute the mean and variance, use      #</span></span><br><span class="line">        <span class="comment"># these statistics to normalize the incoming data, and scale and      #</span></span><br><span class="line">        <span class="comment"># shift the normalized data using gamma and beta.                     #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should store the output in the variable out. Any intermediates  #</span></span><br><span class="line">        <span class="comment"># that you need for the backward pass should be stored in the cache   #</span></span><br><span class="line">        <span class="comment"># variable.                                                           #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should also use your computed sample mean and variance together #</span></span><br><span class="line">        <span class="comment"># with the momentum variable to update the running mean and running   #</span></span><br><span class="line">        <span class="comment"># variance, storing your result in the running_mean and running_var   #</span></span><br><span class="line">        <span class="comment"># variables.                                                          #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># Note that though you should be keeping track of the running         #</span></span><br><span class="line">        <span class="comment"># variance, you should normalize the data based on the standard       #</span></span><br><span class="line">        <span class="comment"># deviation (square root of variance) instead!                        #</span></span><br><span class="line">        <span class="comment"># Referencing the original paper (https://arxiv.org/abs/1502.03167)   #</span></span><br><span class="line">        <span class="comment"># might prove to be helpful.                                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        mean = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">        xmu = x - mean</span><br><span class="line">        sq = np.square(xmu)</span><br><span class="line">        var = np.var(x, axis=<span class="number">0</span>)</span><br><span class="line">        sqrtvar = np.sqrt(var + eps)</span><br><span class="line">        ivar = <span class="number">1.</span> / sqrtvar</span><br><span class="line">        normalize_raw = xmu * ivar</span><br><span class="line"></span><br><span class="line">        normalize_result = gamma * normalize_raw + beta</span><br><span class="line">        out = normalize_result</span><br><span class="line"></span><br><span class="line">        running_mean = momentum * running_mean + \</span><br><span class="line">            (<span class="number">1</span> - momentum) * mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * var</span><br><span class="line"></span><br><span class="line">        cache = (normalize_raw, gamma, xmu, ivar, sqrtvar, var, eps)</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test-time forward pass for batch normalization. #</span></span><br><span class="line">        <span class="comment"># Use the running mean and variance to normalize the incoming data,   #</span></span><br><span class="line">        <span class="comment"># then scale and shift the normalized data using gamma and beta.      #</span></span><br><span class="line">        <span class="comment"># Store the result in the out variable.                               #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        x_normalize = (x - running_mean) / (np.sqrt(running_var + eps))</span><br><span class="line">        out = x_normalize * gamma + beta</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you should write out a computation graph for</span></span><br><span class="line"><span class="string">    batch normalization on paper and propagate gradients backward through</span></span><br><span class="line"><span class="string">    intermediate nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for batch normalization. Store the    #</span></span><br><span class="line">    <span class="comment"># results in the dx, dgamma, and dbeta variables.                         #</span></span><br><span class="line">    <span class="comment"># Referencing the original paper (https://arxiv.org/abs/1502.03167)       #</span></span><br><span class="line">    <span class="comment"># might prove to be helpful.                                              #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    normalize_raw, gamma, xmu, ivar, sqrtvar, var, eps = cache</span><br><span class="line">    N, D = dout.shape</span><br><span class="line"></span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgammax = dout</span><br><span class="line"></span><br><span class="line">    dgamma = np.sum(dgammax * normalize_raw, axis=<span class="number">0</span>)</span><br><span class="line">    dnormalize_raw = dgammax * gamma</span><br><span class="line"></span><br><span class="line">    divar = np.sum(dnormalize_raw * xmu, axis=<span class="number">0</span>)</span><br><span class="line">    dxmu = dnormalize_raw * ivar</span><br><span class="line"></span><br><span class="line">    dsqrtvar = <span class="number">-1.</span> / (sqrtvar ** <span class="number">2</span>) * divar</span><br><span class="line"></span><br><span class="line">    dvar = <span class="number">0.5</span> * <span class="number">1.</span> / np.sqrt(var + eps) * dsqrtvar</span><br><span class="line"></span><br><span class="line">    dsq = <span class="number">1.</span> / N * np.ones((N, D)) * dvar</span><br><span class="line"></span><br><span class="line">    dxmu2 = <span class="number">2</span> * xmu * dsq</span><br><span class="line"></span><br><span class="line">    dx1 = (dxmu + dxmu2)</span><br><span class="line">    dmu = <span class="number">-1</span> * np.sum(dxmu + dxmu2, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    dx2 = <span class="number">1.</span> / N * np.ones((N, D)) * dmu</span><br><span class="line"></span><br><span class="line">    dx = dx1 + dx2</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward_alt</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Alternative backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation you should work out the derivatives for the batch</span></span><br><span class="line"><span class="string">    normalizaton backward pass on paper and simplify as much as possible. You</span></span><br><span class="line"><span class="string">    should be able to derive a simple expression for the backward pass. </span></span><br><span class="line"><span class="string">    See the jupyter notebook for more hints.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: This implementation should expect to receive the same cache variable</span></span><br><span class="line"><span class="string">    as batchnorm_backward, but might not use all of the values in the cache.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs / outputs: Same as batchnorm_backward</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for batch normalization. Store the    #</span></span><br><span class="line">    <span class="comment"># results in the dx, dgamma, and dbeta variables.                         #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># After computing the gradient with respect to the centered inputs, you   #</span></span><br><span class="line">    <span class="comment"># should be able to compute gradients with respect to the inputs in a     #</span></span><br><span class="line">    <span class="comment"># single statement; our implementation fits on a single 80-character line.#</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    normalize_raw, gamma, xmu, ivar, sqrtvar, var, eps = cache</span><br><span class="line">    N, D = dout.shape</span><br><span class="line"></span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(dout * normalize_raw, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># intermediate partial derivatives</span></span><br><span class="line">    dxhat = dout * gamma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># final partial derivatives</span></span><br><span class="line">    dx = (<span class="number">1.</span> / N) * ivar * (N * dxhat - np.sum(dxhat, axis=<span class="number">0</span>)</span><br><span class="line">                            - normalize_raw * np.sum(dxhat * normalize_raw, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<h2 id="Fully-Connected-Nets-with-Batch-Normalization"><a href="#Fully-Connected-Nets-with-Batch-Normalization" class="headerlink" title="Fully Connected Nets with Batch Normalization"></a>Fully Connected Nets with Batch Normalization</h2><p>in <code>cs231n/classifiers/fc_net.py</code>, add the BN layers into the net.</p>
<ul>
<li>应该在每个relu之前加上BN，所以在这里不能直接用之前的affine，relu的过程，因为中间又插了一个新的BN层，所以要写一个新的function</li>
<li>最后一层之后的输出不应该BN(应该是涉及到循环的问题)</li>
</ul>
<h3 id="实现中遇到的问题"><a href="#实现中遇到的问题" class="headerlink" title="实现中遇到的问题"></a>实现中遇到的问题</h3><ul>
<li>self.bn_params的参数类型不是dict而是list，代表的是所有层里面的参数的所有和，当进入到每层的时候具体对应的才是这里的dict</li>
<li>当把affine_BN_relu结合在一起的时候，注意最后一层输出的地方没有BN，所以没有他的cache，需要分开讨论，不然cache的数量不对</li>
<li>注意这个fc_net的class因为需要实现多种不同的功能，所以对于是不是BN要加上条件判断</li>
<li><strong>确实非常像搭乐高了！！</strong></li>
<li>这里主要，写到这才发现最后一层的时候好像是不需要relu也不需要batchnorm</li>
</ul>
<h3 id="定义好的函数块"><a href="#定义好的函数块" class="headerlink" title="定义好的函数块"></a>定义好的函数块</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_BN_relu_forward</span><span class="params">(self, x, w, b, gamma, beta, bn_params)</span>:</span></span><br><span class="line">       a, fc_cache = affine_forward(x, w, b)</span><br><span class="line">       mid, BN_cache = batchnorm_forward(a, gamma, beta, bn_params)</span><br><span class="line">       out, relu_cache = relu_forward(mid)</span><br><span class="line">       cache = (fc_cache, BN_cache, relu_cache)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">affine_BN_relu_backward</span><span class="params">(self, dout, cache)</span>:</span></span><br><span class="line">       fc_cache, BN_cache, relu_cache = cache</span><br><span class="line">       da = relu_backward(dout, relu_cache)</span><br><span class="line">       dmin, dgamma, dbeta = batchnorm_backward_alt(da, BN_cache)</span><br><span class="line">       dx, dw, db = affine_backward(dmin, fc_cache)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> dx, dw, db, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul>
<li>可视化之后可以发现加了norm的话好像会下降的快一点<br><img src="/2019/04/15/CS231Nassignment2BN/4.jpg" alt="image_4vis"></li>
</ul>
<h2 id="Batch-normalization-and-initialization"><a href="#Batch-normalization-and-initialization" class="headerlink" title="Batch normalization and initialization"></a>Batch normalization and initialization</h2><ul>
<li>进行试验，了解BN和weight initialization的关系</li>
<li>训练一个八层的网络，包括和不包括BN，用不同的weight initialization</li>
<li>plot出来train acc, val_acc,train_loss和weight initialization的关系</li>
</ul>
<p><img src="/2019/04/15/CS231Nassignment2BN/5.jpg" alt="image_5vis"></p>
<h2 id="BN的作用"><a href="#BN的作用" class="headerlink" title="BN的作用"></a>BN的作用</h2><p>从图中可以看出来，有了BN以后，weight init对最终结果的影响明显会降低：</p>
<ul>
<li>weight的初始化对最终结果影响很严重，比如如果全是0的话，得到的所有neuron的功能都是一样的</li>
<li>BN其实就是在实际中解决weight init的办法，这样可以减少初始化参数的影响<ul>
<li>核心思想就是如果你需要更好的分布，你就加一层让他变成更好的分布</li>
<li>在计算的过程中越乘越小（或者越大），所以计算出来的结果越来越接近0</li>
<li>所以这时候如果把一些input重新分布了，就会减少这个接近0的可能性</li>
</ul>
</li>
</ul>
<h2 id="Batch-normalization-and-batch-size"><a href="#Batch-normalization-and-batch-size" class="headerlink" title="Batch normalization and batch size"></a>Batch normalization and batch size</h2><ul>
<li>试验验证BN和batch size的关系</li>
<li>训练6-layer的网络，分别with和without BN，使用不同的batch size</li>
</ul>
<p><img src="/2019/04/15/CS231Nassignment2BN/6.jpg" alt="image_6vis"></p>
<p><a href="https://datascience.stackexchange.com/questions/41873/batch-normalization-vs-batch-size" target="_blank" rel="noopener">By increasing batch size your steps can be more accurate because your sampling will be closer to the real population. If you increase the size of batch, your batch normalisation can have better results. The reason is exactly like the input layer. The samples will be closer to the population for inner activations.</a></p>
<h2 id="Layer-Normalization（LN）"><a href="#Layer-Normalization（LN）" class="headerlink" title="Layer Normalization（LN）"></a>Layer Normalization（LN）</h2><ul>
<li>前面的所有的BN已经可以让Net更好的被训练了，但是BN的大小和batch的大小有关，所以在实际应用的时候会受到一些限制<ul>
<li>在复杂的网络里面的时候，batch_size是被硬件机能限制的</li>
<li>每个minibatch的数据分布可能会比较接近，所以训练之前要shuffle，否则结果会差很多</li>
</ul>
</li>
<li><p>其中一种解决的方法就是layer normalization</p>
<ul>
<li>不是在batch上面normal</li>
<li>在layer上面normal</li>
<li>each feature vector corresponding to a single datapoint is normalized based on the sum of all terms within that feature vector.</li>
</ul>
<p><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. “Layer Normalization.” stat 1050 (2016): 21.</a></p>
</li>
</ul>
<h3 id="LN"><a href="#LN" class="headerlink" title="LN"></a>LN</h3><ul>
<li>综合一层的所有维度的输入，计算该层的平均输入和平均方差</li>
<li>然后用同一个规范化操作转换各个维度的输入</li>
<li>相当于以前我们希望可以正则到这个minibatch里面的大家都差不多，现在我们不管batch了，而是调整到一张图片里面的所有数据都是normal的</li>
</ul>
<h3 id="implement-1"><a href="#implement-1" class="headerlink" title="implement"></a>implement</h3><p><code>cs231n/layers.py</code> -&gt; <code>layernorm_backward</code></p>
<h4 id="forward-back"><a href="#forward-back" class="headerlink" title="forward + back"></a>forward + back</h4><ul>
<li>input<ul>
<li>x, (N,D)</li>
<li>gamma, scale</li>
<li>beta,shift</li>
<li>ln_params: eps</li>
</ul>
</li>
<li><p>output</p>
<ul>
<li>output,(N,D)</li>
<li>cache</li>
</ul>
</li>
<li><p>实现方法 -&gt; 实际上就是从对一列的操作变成了对一行的操作</p>
<ul>
<li>比如之前对x取mean就是求每列的mean，现在变成了取每行的mean</li>
<li>在所有normal之后并且scale之前，把这个矩阵在tranpose回来</li>
</ul>
</li>
<li>back<ul>
<li>把需要参与计算的东西都tranpose</li>
<li>然后把计算完的dx tranpose回来</li>
</ul>
</li>
</ul>
<h4 id="fc-nets"><a href="#fc-nets" class="headerlink" title="fc_nets"></a>fc_nets</h4><p>在fc_nets里面稍加改动，在normalization里面增加BN_NORM和Layer_NORM的选项就可以了，整体改动不大<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_Normal_relu_forward</span><span class="params">(self, x, w, b, gamma, beta, bn_params, mode)</span>:</span></span><br><span class="line">      a, fc_cache = affine_forward(x, w, b)</span><br><span class="line">      <span class="keyword">if</span> mode == <span class="string">"batchnorm"</span>:</span><br><span class="line">          mid, Normal_cache = batchnorm_forward(a, gamma, beta, bn_params)</span><br><span class="line">      <span class="keyword">elif</span> mode == <span class="string">"layernorm"</span>:</span><br><span class="line">          mid, Normal_cache = layernorm_forward(a, gamma, beta, bn_params)</span><br><span class="line">      out, relu_cache = relu_forward(mid)</span><br><span class="line">      cache = (fc_cache, Normal_cache, relu_cache)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">affine_Normal_relu_backward</span><span class="params">(self, dout, cache, mode)</span>:</span></span><br><span class="line">      fc_cache, Normal_cache, relu_cache = cache</span><br><span class="line">      da = relu_backward(dout, relu_cache)</span><br><span class="line">      <span class="keyword">if</span> mode == <span class="string">"batchnorm"</span>:</span><br><span class="line">          dmid, dgamma, dbeta = batchnorm_backward_alt(da, Normal_cache)</span><br><span class="line">      <span class="keyword">elif</span> mode == <span class="string">"layernorm"</span>:</span><br><span class="line">          dmid, dgamma, dbeta = layernorm_backward(da, Normal_cache)</span><br><span class="line">      dx, dw, db = affine_backward(dmid, fc_cache)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> dx, dw, db, dgamma, dbeta</span><br></pre></td></tr></table></figure></p>
<p>可以从图像看出来，layernorm中，batchsize的影响变小了<br><img src="/2019/04/15/CS231Nassignment2BN/7.jpg" alt="image_7vis"></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL/" rel="tag"># DL</a>
          
            <a href="/tags/Batch-Normalization/" rel="tag"># Batch Normalization</a>
          
            <a href="/tags/Layer-Normalization/" rel="tag"># Layer Normalization</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/15/CppPrimer笔记/" rel="next" title="CppPrimer笔记">
                <i class="fa fa-chevron-left"></i> CppPrimer笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/16/OpenGL笔记/" rel="prev" title="OpenGL笔记">
                OpenGL笔记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/avatar.jpeg" alt="RUOPENG XU">
            
              <p class="site-author-name" itemprop="name">RUOPENG XU</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">40</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">24</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">39</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://github.com/Bigphess" title="GitHub &rarr; https://github.com/Bigphess" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="mailto:xrp0308@gmail.com" title="E-Mail &rarr; mailto:xrp0308@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#target"><span class="nav-number">1.</span> <span class="nav-text">target</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-normalization-forward"><span class="nav-number">2.</span> <span class="nav-text">Batch normalization: forward</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#I-O"><span class="nav-number">2.1.</span> <span class="nav-text">I/O</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#todo"><span class="nav-number">2.2.</span> <span class="nav-text">todo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implement"><span class="nav-number">2.3.</span> <span class="nav-text">implement</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-normalization-backward"><span class="nav-number">3.</span> <span class="nav-text">Batch normalization: backward</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-normalization-alternative-backward"><span class="nav-number">4.</span> <span class="nav-text">Batch normalization: alternative backward</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#最终目标"><span class="nav-number">4.1.</span> <span class="nav-text">最终目标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#这三部分的代码如下"><span class="nav-number">5.</span> <span class="nav-text">这三部分的代码如下</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fully-Connected-Nets-with-Batch-Normalization"><span class="nav-number">6.</span> <span class="nav-text">Fully Connected Nets with Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实现中遇到的问题"><span class="nav-number">6.1.</span> <span class="nav-text">实现中遇到的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#定义好的函数块"><span class="nav-number">6.2.</span> <span class="nav-text">定义好的函数块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结论"><span class="nav-number">6.3.</span> <span class="nav-text">结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-normalization-and-initialization"><span class="nav-number">7.</span> <span class="nav-text">Batch normalization and initialization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BN的作用"><span class="nav-number">8.</span> <span class="nav-text">BN的作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-normalization-and-batch-size"><span class="nav-number">9.</span> <span class="nav-text">Batch normalization and batch size</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Layer-Normalization（LN）"><span class="nav-number">10.</span> <span class="nav-text">Layer Normalization（LN）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LN"><span class="nav-number">10.1.</span> <span class="nav-text">LN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implement-1"><span class="nav-number">10.2.</span> <span class="nav-text">implement</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#forward-back"><span class="nav-number">10.2.1.</span> <span class="nav-text">forward + back</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fc-nets"><span class="nav-number">10.2.2.</span> <span class="nav-text">fc_nets</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  
    <div id="sidebar-dimmer"></div>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">RUOPENG XU</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/affix.js?v=7.0.1"></script>

  <script src="/js/src/schemes/pisces.js?v=7.0.1"></script>




  
  <script src="/js/src/scrollspy.js?v=7.0.1"></script>
<script src="/js/src/post-details.js?v=7.0.1"></script>



  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  

  

  

  


  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
